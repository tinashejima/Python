{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17abbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (18512, 9)\n",
      "\\nFirst Few Rows:\n",
      "   gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
      "0  Female  80.0             0              1           never  25.19   \n",
      "1  Female  54.0             0              0         No Info  27.32   \n",
      "2    Male  28.0             0              0           never  27.32   \n",
      "3  Female  36.0             0              0         current  23.45   \n",
      "4    Male  76.0             1              1         current  20.14   \n",
      "\n",
      "   HbA1c_level  blood_glucose_level  diabetes  \n",
      "0          6.6                140.0         0  \n",
      "1          6.6                 80.0         0  \n",
      "2          5.7                158.0         0  \n",
      "3          5.0                155.0         0  \n",
      "4          4.8                155.0         0  \n",
      "\\nDataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18512 entries, 0 to 18511\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   gender               18512 non-null  object \n",
      " 1   age                  18511 non-null  float64\n",
      " 2   hypertension         18512 non-null  int64  \n",
      " 3   heart_disease        18512 non-null  int64  \n",
      " 4   smoking_history      18509 non-null  object \n",
      " 5   bmi                  18506 non-null  float64\n",
      " 6   HbA1c_level          18509 non-null  float64\n",
      " 7   blood_glucose_level  18511 non-null  float64\n",
      " 8   diabetes             18512 non-null  int64  \n",
      "dtypes: float64(4), int64(3), object(2)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "\\nBasic Statistics:\n",
      "                age  hypertension  heart_disease           bmi   HbA1c_level  \\\n",
      "count  18511.000000  18512.000000   18512.000000  18506.000000  18509.000000   \n",
      "mean      49.757100      0.146662       0.084162     29.240411      6.105262   \n",
      "std       21.749891      0.353778       0.277638      7.352926      1.275292   \n",
      "min        0.080000      0.000000       0.000000     10.010000      3.500000   \n",
      "25%       35.000000      0.000000       0.000000     25.650000      5.700000   \n",
      "50%       53.000000      0.000000       0.000000     27.320000      6.100000   \n",
      "75%       67.000000      0.000000       0.000000     32.500000      6.600000   \n",
      "max       80.000000      1.000000       1.000000     88.720000      9.000000   \n",
      "\n",
      "       blood_glucose_level      diabetes  \n",
      "count         18511.000000  18512.000000  \n",
      "mean            160.801361      0.459162  \n",
      "std              56.253353      0.498343  \n",
      "min              80.000000      0.000000  \n",
      "25%             130.000000      0.000000  \n",
      "50%             155.000000      0.000000  \n",
      "75%             200.000000      1.000000  \n",
      "max             300.000000      1.000000  \n",
      "\\nMissing Values:\n",
      "gender                 0\n",
      "age                    1\n",
      "hypertension           0\n",
      "heart_disease          0\n",
      "smoking_history        3\n",
      "bmi                    6\n",
      "HbA1c_level            3\n",
      "blood_glucose_level    1\n",
      "diabetes               0\n",
      "dtype: int64\n",
      "\\n================================================================================\n",
      "DATA PREPROCESSING\n",
      "================================================================================\n",
      "\\nUnique values in each column:\n",
      "gender: ['Female' 'Male' 'Other']\n",
      "age: [80.   54.   28.   36.   76.   20.   44.   40.    5.   69.   67.   50.\n",
      " 73.   53.   57.   60.   77.   47.   61.   43.   55.   64.   63.   70.\n",
      " 48.   42.   52.   71.   59.   29.   68.   79.   37.   49.   75.   66.\n",
      " 46.   38.   62.   39.   58.   78.   74.   65.   30.   51.   72.   32.\n",
      " 45.   56.     nan 25.   13.   24.   27.   18.    7.   35.   19.   22.\n",
      "  8.   34.   41.   31.   12.    6.   10.    1.24  2.   11.   15.    3.\n",
      "  9.   21.   26.   16.   23.   17.    4.   33.    0.8  14.    0.4   0.16\n",
      "  0.56  1.88  0.48  1.16  0.72  0.24  0.64  1.4   0.88  1.72  1.64  1.48\n",
      "  1.32  0.32  0.08  1.8   1.    1.56  1.08]\n",
      "hypertension: [0 1]\n",
      "heart_disease: [1 0]\n",
      "smoking_history: ['never' 'No Info' 'current' 'not current' 'former' nan 'ever']\n",
      "bmi: [25.19 27.32 23.45 ... 57.78 58.38 36.87]\n",
      "HbA1c_level: [6.6 5.7 5.  4.8 6.5 6.  6.2 9.  7.  8.8 8.2 7.5 6.8 5.8 6.1 4.  4.5 3.5\n",
      " nan]\n",
      "blood_glucose_level: [140.  80. 158. 155.  85. 200.  90. 260. 160. 159. 126. 220. 300. 280.\n",
      " 130. 145. 240.  nan 100.]\n",
      "diabetes: [0 1]\n",
      "\\nData types after conversion:\n",
      "gender                  object\n",
      "age                    float64\n",
      "hypertension             int64\n",
      "heart_disease            int64\n",
      "smoking_history         object\n",
      "bmi                    float64\n",
      "HbA1c_level            float64\n",
      "blood_glucose_level    float64\n",
      "diabetes                 int64\n",
      "dtype: object\n",
      "\\n================================================================================\n",
      "ONE-HOT ENCODING\n",
      "================================================================================\n",
      "\\nCategorical columns: ['gender', 'smoking_history']\n",
      "\\nShape after encoding: (18512, 14)\n",
      "\\nColumn names after encoding:\n",
      "['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes', 'gender_Male', 'gender_Other', 'smoking_history_current', 'smoking_history_ever', 'smoking_history_former', 'smoking_history_never', 'smoking_history_not current']\n",
      "\\n================================================================================\n",
      "DATA IMBALANCE CHECK\n",
      "================================================================================\n",
      "\\nTarget Variable Distribution:\n",
      "diabetes\n",
      "0    10012\n",
      "1     8500\n",
      "Name: count, dtype: int64\n",
      "\\nPercentage Distribution:\n",
      "diabetes\n",
      "0    54.083838\n",
      "1    45.916162\n",
      "Name: proportion, dtype: float64\n",
      "\\nImbalance Ratio: 1.18\n",
      "✓ Dataset is relatively balanced.\n",
      "\\n================================================================================\n",
      "TRAIN-TEST SPLIT & SMOTE\n",
      "================================================================================\n",
      "\\nTraining set size: (14809, 13)\n",
      "Test set size: (3703, 13)\n",
      "\\nTraining set class distribution:\n",
      "diabetes\n",
      "0    8009\n",
      "1    6800\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 131\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Apply SMOTE to training data only\u001b[39;00m\n\u001b[1;32m    130\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m X_train_balanced, y_train_balanced \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnAfter SMOTE - Training set size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_balanced\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter SMOTE - Class distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imblearn/base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imblearn/base.py:99\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m     97\u001b[0m check_classification_targets(y)\n\u001b[1;32m     98\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[0;32m---> 99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imblearn/base.py:157\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[0;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[1;32m    155\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    156\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 157\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2971\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2969\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2970\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2971\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2972\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1368\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1363\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1364\u001b[0m     )\n\u001b[1;32m   1366\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m-> 1368\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1387\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1105\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1105\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1114\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD AND EXPLORE DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"\\\\nFirst Few Rows:\")\n",
    "print(data.head())\n",
    "print(\"\\\\nDataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\\\nBasic Statistics:\")\n",
    "print(data.describe())\n",
    "print(\"\\\\nMissing Values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for missing or 'No Info' values\n",
    "print(\"\\\\nUnique values in each column:\")\n",
    "for col in data.columns:\n",
    "    print(f\"{col}: {data[col].unique()}\")\n",
    "\n",
    "# Handle 'No Info' values - replace with most frequent value or mode\n",
    "data_clean = data.copy()\n",
    "\n",
    "# For smoking_history 'No Info' - this is categorical, we'll keep it as a separate category\n",
    "# For heart_disease 'No Info' - replace with mode (0)\n",
    "if 'No Info' in data_clean['heart_disease'].values:\n",
    "    data_clean['heart_disease'] = data_clean['heart_disease'].replace('No Info', \n",
    "                                   data_clean['heart_disease'].mode()[0])\n",
    "\n",
    "# Convert data types\n",
    "data_clean['heart_disease'] = data_clean['heart_disease'].astype(int)\n",
    "data_clean['hypertension'] = data_clean['hypertension'].astype(int)\n",
    "data_clean['diabetes'] = data_clean['diabetes'].astype(int)\n",
    "\n",
    "print(\"\\\\nData types after conversion:\")\n",
    "print(data_clean.dtypes)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ONE-HOT ENCODING FOR NON-NUMERIC COLUMNS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ONE-HOT ENCODING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = data_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\\\nCategorical columns: {categorical_cols}\")\n",
    "\n",
    "# Apply one-hot encoding\n",
    "data_encoded = pd.get_dummies(data_clean, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"\\\\nShape after encoding: {data_encoded.shape}\")\n",
    "print(\"\\\\nColumn names after encoding:\")\n",
    "print(data_encoded.columns.tolist())\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CHECK DATA IMBALANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"DATA IMBALANCE CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_dist = data_encoded['diabetes'].value_counts()\n",
    "print(\"\\\\nTarget Variable Distribution:\")\n",
    "print(target_dist)\n",
    "print(f\"\\\\nPercentage Distribution:\")\n",
    "print(data_encoded['diabetes'].value_counts(normalize=True) * 100)\n",
    "\n",
    "imbalance_ratio = target_dist.max() / target_dist.min()\n",
    "print(f\"\\\\nImbalance Ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(\"⚠️  Dataset is IMBALANCED. SMOTE will be applied.\")\n",
    "else:\n",
    "    print(\"✓ Dataset is relatively balanced.\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SPLIT DATA AND APPLY SMOTE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"TRAIN-TEST SPLIT & SMOTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Separate features and target\n",
    "X = data_encoded.drop('diabetes', axis=1)\n",
    "y = data_encoded['diabetes']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                      random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\\\nTraining set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"\\\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\\\nAfter SMOTE - Training set size: {X_train_balanced.shape}\")\n",
    "print(f\"After SMOTE - Class distribution:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ============================================================================\n",
    "# 6. TRAIN MULTIPLE CLASSIFICATION MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING & EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score on training data\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train_balanced, cv=5)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_score': cv_mean,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Cross-Validation Score (mean): {cv_mean:.4f}\")\n",
    "    print(f\"\\\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"\\\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ============================================================================\n",
    "# 7. SELECT BEST 3 MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"MODEL SELECTION - TOP 3 MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort models by accuracy\n",
    "sorted_models = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "\n",
    "print(\"\\\\nAll Models Ranked by Accuracy:\")\n",
    "for i, (name, metrics) in enumerate(sorted_models, 1):\n",
    "    print(f\"{i}. {name}: Accuracy = {metrics['accuracy']:.4f}, CV Score = {metrics['cv_score']:.4f}\")\n",
    "\n",
    "# Select top 3\n",
    "top_3_names = [name for name, _ in sorted_models[:3]]\n",
    "print(f\"\\\\n✓ Selected TOP 3 Models: {top_3_names}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. CREATE VOTING ENSEMBLE FROM BEST 3 MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"VOTING ENSEMBLE USING TOP 3 MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get predictions from top 3 models\n",
    "top_3_predictions = []\n",
    "for name in top_3_names:\n",
    "    top_3_predictions.append(results[name]['predictions'])\n",
    "\n",
    "# Convert to numpy array for easier manipulation\n",
    "predictions_array = np.array(top_3_predictions)\n",
    "\n",
    "# Hard Voting - majority vote\n",
    "ensemble_predictions_hard = np.apply_along_axis(\n",
    "    lambda x: np.bincount(x).argmax(), \n",
    "    axis=0, \n",
    "    arr=predictions_array\n",
    ")\n",
    "\n",
    "# Calculate ensemble accuracy\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions_hard)\n",
    "\n",
    "print(f\"\\\\nVoting Ensemble Performance:\")\n",
    "print(f\"Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"\\\\nClassification Report:\")\n",
    "print(classification_report(y_test, ensemble_predictions_hard))\n",
    "print(f\"\\\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, ensemble_predictions_hard))\n",
    "\n",
    "# Compare with individual models\n",
    "print(f\"\\\\n{'='*60}\")\n",
    "print(\"COMPARISON: Individual Models vs Ensemble\")\n",
    "print('='*60)\n",
    "for name in top_3_names:\n",
    "    print(f\"{name}: {results[name]['accuracy']:.4f}\")\n",
    "print(f\"Voting Ensemble: {ensemble_accuracy:.4f}\")\n",
    "\n",
    "improvement = ensemble_accuracy > max([results[name]['accuracy'] for name in top_3_names])\n",
    "if improvement:\n",
    "    print(f\"\\\\n✓ Ensemble IMPROVED over individual models!\")\n",
    "else:\n",
    "    print(f\"\\\\n→ Best individual model performs as well or better than ensemble.\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Model Comparison\n",
    "ax1 = axes[0, 0]\n",
    "model_names = [name for name, _ in sorted_models]\n",
    "accuracies = [metrics['accuracy'] for _, metrics in sorted_models]\n",
    "colors = ['#2ecc71' if name in top_3_names else '#95a5a6' for name in model_names]\n",
    "ax1.barh(model_names, accuracies, color=colors)\n",
    "ax1.set_xlabel('Accuracy')\n",
    "ax1.set_title('Model Performance Comparison\\\\n(Green = Top 3 Selected)')\n",
    "ax1.set_xlim([0, 1])\n",
    "\n",
    "# 2. Class Distribution Before and After SMOTE\n",
    "ax2 = axes[0, 1]\n",
    "x_pos = np.arange(2)\n",
    "before_smote = y_train.value_counts().sort_index().values\n",
    "after_smote = pd.Series(y_train_balanced).value_counts().sort_index().values\n",
    "width = 0.35\n",
    "ax2.bar(x_pos - width/2, before_smote, width, label='Before SMOTE', color='#e74c3c')\n",
    "ax2.bar(x_pos + width/2, after_smote, width, label='After SMOTE', color='#2ecc71')\n",
    "ax2.set_xlabel('Class')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Class Distribution: Before vs After SMOTE')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(['No Diabetes (0)', 'Diabetes (1)'])\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Confusion Matrix for Ensemble\n",
    "ax3 = axes[1, 0]\n",
    "cm = confusion_matrix(y_test, ensemble_predictions_hard)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3)\n",
    "ax3.set_xlabel('Predicted')\n",
    "ax3.set_ylabel('Actual')\n",
    "ax3.set_title('Confusion Matrix - Voting Ensemble')\n",
    "\n",
    "# 4. Top 3 Models Accuracy Comparison\n",
    "ax4 = axes[1, 1]\n",
    "top_3_acc = [results[name]['accuracy'] for name in top_3_names]\n",
    "top_3_acc.append(ensemble_accuracy)\n",
    "labels = top_3_names + ['Voting\\\\nEnsemble']\n",
    "colors_comp = ['#3498db', '#3498db', '#3498db', '#e74c3c']\n",
    "ax4.bar(labels, top_3_acc, color=colors_comp)\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title('Top 3 Models vs Voting Ensemble')\n",
    "ax4.set_ylim([0, 1])\n",
    "ax4.axhline(y=max(top_3_acc), color='green', linestyle='--', alpha=0.3, label='Best Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('diabetes_classification_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\\\n✓ Visualization saved as 'diabetes_classification_results.png'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 10. SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "Dataset Information:\n",
    "- Total samples: {data.shape[0]}\n",
    "- Features: {data.shape[1] - 1}\n",
    "- Target: diabetes (binary classification)\n",
    "\n",
    "Preprocessing:\n",
    "- One-hot encoding applied to: {categorical_cols}\n",
    "- Features after encoding: {X.shape[1]}\n",
    "- SMOTE applied to balance training data\n",
    "\n",
    "Model Performance:\n",
    "- Total models trained: {len(models)}\n",
    "- Best individual model: {sorted_models[0][0]} ({sorted_models[0][1]['accuracy']:.4f})\n",
    "- Top 3 models: {', '.join(top_3_names)}\n",
    "- Voting Ensemble accuracy: {ensemble_accuracy:.4f}\n",
    "\n",
    "Conclusion:\n",
    "The voting ensemble combines predictions from the top 3 performing models\n",
    "using hard voting (majority vote) to produce the final classification.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ca14f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
