{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMRL file not found\n",
      ">>> Number of databases  1\n",
      ">>> Total size: 0.67  GB\n",
      ".........................................................................................................\n",
      ">>> 1 / 1 ./dbs/Nyamutumbu09April24.sql\n",
      "name 'df_nmrl' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'processing_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53759/202288289.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\">>> {count} / {len(facilities)} {facility}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             \u001b[0mnmrl_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:,}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_nmrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>> Starting NMRL size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmrl_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_nmrl' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53759/202288289.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1753\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: E722\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1755\u001b[0;31m             \u001b[0mcbs_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessing_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfacility\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdb_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Failed : \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"web\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1756\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processing_time' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import mysql.connector\n",
    "import configparser\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class CbsExtraction():\n",
    "    \n",
    "    def __init__(self, database_path , output_path):\n",
    "        self.database_path = database_path\n",
    "        self.output_path = output_path\n",
    "\n",
    "    # get database connection parameters from config file\n",
    "    def get_db_params(self):\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('config.ini') \n",
    "        user = config.get('Database', 'user')\n",
    "        password = config.get('Database', 'password')\n",
    "        host = config.get('Database', 'host')\n",
    "        port = config.get('Database', 'port')\n",
    "        return user, password, host, port\n",
    "    \n",
    "    # create connection to database\n",
    "    def get_connection(self):\n",
    "         user, password, host, port  = self.get_db_params()\n",
    "         connection = mysql.connector.connect(user=user, password=password, host=host, port=port)\n",
    "         return connection\n",
    "    \n",
    "     # remove database mrs from sql file \n",
    "    def trim_database(self, database_name):\n",
    "        with open(database_name, \"r\", encoding='ISO-8859-1') as f:\n",
    "            content = f.read()\n",
    "        start_text = \"USE `mrs`;\"\n",
    "        updated_text = content.split(start_text)[0]\n",
    "        directory, filename = os.path.split(database_name)\n",
    "        new_filename = f\"modified_{filename}\"\n",
    "        file_path = os.path.join(directory, new_filename)\n",
    "        with open(file_path, \"w\",encoding='utf-8') as f:\n",
    "            f.write(updated_text)\n",
    "        return file_path\n",
    "    \n",
    "    # Restore database\n",
    "    def restore_database(self, database_name):\n",
    "        database_name = database_name.replace(\"\\\\\", \"/\")\n",
    "        # Helper function for deleting existing schemas\n",
    "        def drop_database(cursor, schema):\n",
    "            schemas_to_drop = ['client', 'consultation', 'deduplication', 'facility', 'mrs', 'provider', 'report', 'terminology', 'zimepms']\n",
    "            if schema in schemas_to_drop:\n",
    "                cursor.execute(f'DROP DATABASE {schema}')\n",
    "                print(f\"   >>> DROPPED [{schema}]\")\n",
    "        # Establish connection to the server\n",
    "        connection = self.get_connection()\n",
    "        # Drop existing EHR schemas before restoring new database\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute('SET foreign_key_checks = 0')\n",
    "            cursor.execute('SELECT SCHEMA_NAME FROM information_schema.schemata;')\n",
    "            schemas = [row[0] for row in cursor.fetchall()]\n",
    "            for schema in schemas:\n",
    "                drop_database(cursor, schema)\n",
    "        # Get DB credentials \n",
    "        user, password , host, port = self.get_db_params()\n",
    "        restore_command = f\"mysql -u {user} -p{password} -h {host} -P {port} < {database_name}\"\n",
    "        try:\n",
    "            # print(\" >>> Restoring database: \"+restore_command)\n",
    "            subprocess.run(restore_command, shell=True, check=True)\n",
    "            print('   >>> DATABASE RESTORE [SUCCESSFUL>>>]')\n",
    "        except Exception as e:\n",
    "            log_file = os.path.join(os.getcwd(), 'logs.txt')\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f'{database_name} {str(e)}\\n')\n",
    "    \n",
    "    def get_mapping_file(self):\n",
    "        mapping_file = pd.read_csv(\"mapping_file.csv\")\n",
    "        mapping_file['Facility ID'] = mapping_file['Facility ID'].str.strip()\n",
    "        return mapping_file\n",
    "    \n",
    "    def get_folder_size(self):\n",
    "        total_size = 0\n",
    "        for path, dirs, files in os.walk(self.database_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(path, file)\n",
    "                total_size += os.path.getsize(file_path)\n",
    "        size_in_kb = total_size / 1024\n",
    "        size_in_mb = size_in_kb / 1024\n",
    "        size_in_gb = size_in_mb / 1024\n",
    "        return round(size_in_gb,2)\n",
    "    \n",
    "    def get_database_size(self,filename):\n",
    "        size = round(Path( filename).stat().st_size /(1024*1024*1024),2)\n",
    "        return size\n",
    "    \n",
    "    def get_facility_databases(self): \n",
    "        facility_databases = []\n",
    "        for root, dirs, files in os.walk(self.database_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".sql\") and not file.startswith(\"modified\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    facility_databases.append(file_path)\n",
    "        return facility_databases\n",
    "    \n",
    "    def extracting_data(self,sql,connection):\n",
    "        try:\n",
    "            connection = self.get_connection()\n",
    "            df = pd.read_sql(sql,connection)\n",
    "            connection.close()\n",
    "            return df\n",
    "        except:  # noqa: E722\n",
    "            return None\n",
    "        \n",
    "    def get_facility_details(self,mapping_file,facility_id):\n",
    "        facility_name = mapping_file.loc[mapping_file['Facility ID'] == facility_id] [\"Facility\"].values\n",
    "        if facility_name.size > 0:\n",
    "            facility_name = facility_name[0]\n",
    "            district_name = mapping_file.loc[mapping_file['Facility ID'] == facility_id] [\"District\"].values\n",
    "            if district_name.size > 0:\n",
    "                district_name = district_name[0]\n",
    "            else:\n",
    "                district_name = \"\"\n",
    "            province_name = mapping_file.loc[mapping_file['Facility ID'] == facility_id] [\"Province\"].values\n",
    "            if province_name.size > 0:\n",
    "                province_name = province_name[0]\n",
    "            else:\n",
    "                province_name = \"\"\n",
    "        else:\n",
    "            facility_name = \"\"\n",
    "            district_name = \"\"\n",
    "            province_name = \"\"\n",
    "        return facility_name,district_name,province_name\n",
    "    \n",
    "    # def clean_string(self,text):\n",
    "    #     if text == \"empty string\":\n",
    "    #        return np.nan\n",
    "    #     pattern = r'[^a-zA-Z0-9\\s]'  # Matches any character that is not alphanumeric or whitespace\n",
    "    #     cleaned_text = re.sub(pattern, '', text).lower().replace(' ','')\n",
    "    #     return cleaned_text\n",
    "    \n",
    "    def match_on_lab(self,df_ehr,df_nmrl_filtered, df_nmrl):\n",
    "        '''get only lab request numbers that are unique in EHR'''\n",
    "        only_one_lab_id = [item for item in list(df_ehr[\"clean_laboratory_request_number\"]) if list(df_ehr[\"clean_laboratory_request_number\"]).count(item) == 1]\n",
    "        '''matches in nmrl'''\n",
    "        df_nmrl_match = df_nmrl_filtered[df_nmrl_filtered['clean_laboratory_request_number'].isin(only_one_lab_id)]\n",
    "        matches_found = list(set(df_nmrl_match['clean_laboratory_request_number']))\n",
    "        '''matches in EHR'''\n",
    "        df_ehr_match = df_ehr[df_ehr['clean_laboratory_request_number'].isin(matches_found)]\n",
    "        '''get dataframe without matched lab numbers'''\n",
    "        df_nmrl_not_matched = df_nmrl[~df_nmrl['clean_laboratory_request_number'].isin(matches_found)]\n",
    "        # create dataframe to save , dataframe contains matched records \n",
    "        df_matched = pd.merge(df_ehr_match ,df_nmrl_match, on = 'clean_laboratory_request_number', how = 'left')\n",
    "        print(f\"\"\"    {df_ehr_match.shape[0]} matches found among {df_positives.shape[0]} records {df_ehr_match.shape[0]/df_positives.shape[0] * 100:.2f} % in EHR ART data based on lab request number\"\"\")\n",
    "        print(f\"\"\"    {df_nmrl_match.shape[0]} matches found among {df_nmrl_filtered.shape[0]} records {df_nmrl_match.shape[0]/df_nmrl_filtered.shape[0] * 100:.2f} % in NMRL data based on lab request number\"\"\")\n",
    "        df_matched = df_matched.rename(columns={\n",
    "                                               'sample_date':'Date at which Viral Load result was issued',\n",
    "                                                'Date of Dispatch': 'Date for which Viral Load was taken' ,\n",
    "                                                'Result':'Viral Load result'})\n",
    "        Viral_Load_Sample_submitted_to_lab = 'TRUE'\n",
    "        Was_Viral_Load_result_issued = 'TRUE'\n",
    "        df_matched['Viral Load Sample submitted to lab'] = Viral_Load_Sample_submitted_to_lab\n",
    "        df_matched['Was Viral Load result issued'] = Was_Viral_Load_result_issued\n",
    "        df_matched['Event_date'] = df_matched['Date at which Viral Load result was issued']\n",
    "        df_matched = df_matched[[\n",
    "                 'Event_date', 'person_id',\n",
    "                'Date at which Viral Load result was issued',\n",
    "                'Date for which Viral Load was taken',\n",
    "                'Viral Load Sample submitted to lab', 'Was Viral Load result issued',\n",
    "                'Viral Load result'\n",
    "        ]]\n",
    "        return  df_matched, df_nmrl_not_matched\n",
    "    \n",
    "    # This function finds matches using exact match in art number\n",
    "    def match_on_art_number(self,df_ehr, df_nmrl):\n",
    "        '''Get art numbers from the EHR where each art number has only one person ID'''\n",
    "        only_one_person_id = df_ehr.groupby('clean_art_number').filter(lambda x: x['person_id'].nunique() == 1)['clean_art_number']      \n",
    "        '''matches in nmrl'''\n",
    "        df_nmrl_match = df_nmrl[df_nmrl['clean_art_number'].isin(only_one_person_id)]\n",
    "        matches_found = list(set(df_nmrl_match['clean_art_number']))\n",
    "        '''matches in EHR'''\n",
    "        df_ehr_match = df_ehr[df_ehr['clean_art_number'].isin(matches_found)]\n",
    "        '''get dataframe without matched art numbers'''\n",
    "        df_nmrl_not_matched = df_nmrl[~df_nmrl['clean_art_number'].isin(matches_found)]\n",
    "        # create dataframe to save , dataframe contains matched records \n",
    "        df_matched = pd.merge(df_nmrl_match, df_ehr_match, on = 'clean_art_number', how = 'left')\n",
    "        print(f\"\"\"    {df_ehr_match.shape[0]} matches found among {df_positives.shape[0]} records {df_ehr_match.shape[0]/df_positives.shape[0] * 100:.2f} % in EHR ART data based on ART number\"\"\")\n",
    "        print(f\"\"\"    {df_nmrl_match.shape[0]} matches found among {df_nmrl.shape[0]} records {df_nmrl_match.shape[0]/df_nmrl.shape[0] * 100:.2f} % in NMRL data based on ART number\"\"\")\n",
    "        df_matched = df_matched.rename(columns={'Art Province Name': 'Viral Load Province Name', \n",
    "                                                'Art District Name': 'Viral Load District Name',\n",
    "                                                'Art Facility Name': 'Viral Load Facility Name',\n",
    "                                                'Art Facility ID': 'Viral Load Facility ID',\n",
    "                                                'Art DB Name': 'Viral Load DB Name',\n",
    "                                               'sample_date':'Date at which Viral Load result was issued',\n",
    "                                                'Date of Dispatch': 'Date for which Viral Load was taken' ,\n",
    "                                                'Result':'Viral Load result'})\n",
    "        Viral_Load_Sample_submitted_to_lab = 'TRUE'\n",
    "        Was_Viral_Load_result_issued = 'TRUE'\n",
    "        df_matched['Viral Load Sample submitted to lab'] = Viral_Load_Sample_submitted_to_lab\n",
    "        df_matched['Was Viral Load result issued'] = Was_Viral_Load_result_issued\n",
    "        df_matched['Event_date'] = df_matched['Date at which Viral Load result was issued']\n",
    "        df_matched = df_matched[[\n",
    "                'Viral Load Province Name', 'Viral Load District Name',\n",
    "                'Viral Load Facility Name', 'Viral Load Facility ID',\n",
    "                'Viral Load DB Name', 'Event_date', 'person_id',\n",
    "                'Date at which Viral Load result was issued',\n",
    "                'Date for which Viral Load was taken',\n",
    "                'Viral Load Sample submitted to lab', 'Was Viral Load result issued',\n",
    "                'Viral Load result'\n",
    "        ]]\n",
    "        return  df_matched, df_nmrl_not_matched\n",
    "    \n",
    "    def match_on_demos(self,df_ehr, df_nmrl):\n",
    "        # define the variables to use for matching\n",
    "        match_variables = ['LastName','FirstName','Sex','Birthdate_cleaned']\n",
    "        '''convert match variables to lower for comaprison'''\n",
    "        for column in match_variables[:3]:\n",
    "            df_ehr[column] = df_ehr[column].str.lower()\n",
    "            df_nmrl[column] = df_nmrl[column].str.lower() \n",
    "        '''Get person_id from the EHR where each set of match variables has only one person ID'''\n",
    "        df_only_one_person_id = df_ehr.groupby(match_variables).filter(lambda x: x['person_id'].nunique() == 1) [['person_id','LastName','FirstName','Sex','Birthdate_cleaned']]  \n",
    "        '''get list of lists for df_only_one_person_id'''\n",
    "        '''matches in nmrl'''\n",
    "        df_nmrl_match = pd.merge(df_nmrl, df_only_one_person_id , on = match_variables, how = 'inner')\n",
    "        sample_ids = df_nmrl_match['Sample ID']\n",
    "        '''matches in EHR'''\n",
    "        df_ehr_match_count = len(set(df_nmrl_match['person_id']))\n",
    "        df_nmrl_not_matched = df_nmrl[~df_nmrl['Sample ID'].isin(sample_ids)]\n",
    "        # create dataframe to save , dataframe contains matched records \n",
    "        df_nmrl_match = df_nmrl_match[['Sample ID', 'FirstName','LastName', \n",
    "                                       'Sex', 'Date of Dispatch', 'Result','sample_date','Birthdate_cleaned']]\n",
    "        \n",
    "        df_ehr = df_ehr[[\n",
    "                'Province Name', 'District Name', 'Facility Name', 'Facility Id',\n",
    "                'Facility DB Name', 'person_id', 'FirstName', 'LastName',\n",
    "                'Sex',  'Birthdate_cleaned'\n",
    "        ]]\n",
    "        df_matched = pd.merge(df_nmrl_match, df_ehr, on = match_variables, how = 'left')\n",
    "        df_nmrl_not_matched.drop('Birthdate_cleaned',axis = 1,inplace=True)\n",
    "        print(f\"\"\"   {df_ehr_match_count} matches found among {df_positives.shape[0]} records {df_ehr_match_count/df_positives.shape[0] * 100:.2f} % in EHR data based on demographics \"\"\")\n",
    "        print(f\"\"\"   {df_nmrl_match.shape[0]} matches found among {df_nmrl.shape[0]} records {df_nmrl_match.shape[0]/df_nmrl.shape[0] * 100:.2f} % in NMRL data based on ART number\"\"\")\n",
    "        df_matched = df_matched.rename(columns={'Province Name': 'Viral Load Province Name', \n",
    "                                                'District Name': 'Viral Load District Name',\n",
    "                                                'Facility Name': 'Viral Load Facility Name',\n",
    "                                                'Facility Id': 'Viral Load Facility ID',\n",
    "                                                'Facility DB Name': 'Viral Load DB Name',\n",
    "                                               'sample_date':'Date at which Viral Load result was issued',\n",
    "                                                'Date of Dispatch': 'Date for which Viral Load was taken' ,\n",
    "                                                'Result':'Viral Load result'})\n",
    "        Viral_Load_Sample_submitted_to_lab = 'TRUE'\n",
    "        Was_Viral_Load_result_issued = 'TRUE'\n",
    "        df_matched['Viral Load Sample submitted to lab'] = Viral_Load_Sample_submitted_to_lab\n",
    "        df_matched['Was Viral Load result issued'] = Was_Viral_Load_result_issued\n",
    "        df_matched['Event_date'] = df_matched['Date at which Viral Load result was issued']\n",
    "        df_matched = df_matched[[\n",
    "                'Viral Load Province Name', 'Viral Load District Name',\n",
    "                'Viral Load Facility Name', 'Viral Load Facility ID',\n",
    "                'Viral Load DB Name', 'Event_date', 'person_id',\n",
    "                'Date at which Viral Load result was issued',\n",
    "                'Date for which Viral Load was taken',\n",
    "                'Viral Load Sample submitted to lab', 'Was Viral Load result issued',\n",
    "                'Viral Load result'\n",
    "        ]]\n",
    "        return  df_matched, df_nmrl_not_matched\n",
    "    \n",
    "    ''' Functions for cleaning up dates '''\n",
    "    ''' Function for extracting the year from DOB and fixing typos/unrealistic values '''\n",
    "    def fix_years(self,df_in, date_column='DOB'):\n",
    "        # Fix any years that are not 4 digits long\n",
    "        incorrect_length = (df_in['YYYY'].apply(lambda x: len(str(x)) != 4))\n",
    "        for i in incorrect_length[incorrect_length].index:\n",
    "            x = df_in.loc[i, 'YYYY']\n",
    "            x = str(x)\n",
    "            if len(x) > 4:\n",
    "                y = x[:4]\n",
    "            elif len(x) in [2,3]:\n",
    "                if x[:2] == '19':\n",
    "                    y = '1980'\n",
    "                elif x[:2] == '20':\n",
    "                    y = '2000'\n",
    "                else:\n",
    "                    if len(x) == 3:\n",
    "                        y = '{}0'.format(x)\n",
    "                    else:\n",
    "                        y = '00{}'.format(x)\n",
    "            else:\n",
    "                y = '1980'\n",
    "            df_in.loc[i, 'YYYY'] = y\n",
    "        # Fix any years that don't start with 1 or 2\n",
    "        incorrect_first_digit = (df_in['YYYY'].apply(lambda x: str(x)[0] not in ['1','2']))\n",
    "        for i in incorrect_first_digit[incorrect_first_digit].index:\n",
    "            x = df_in.loc[i,'YYYY']\n",
    "            x = str(x)\n",
    "            if not x.isnumeric():\n",
    "                y = '1980'\n",
    "            else:\n",
    "                y = x[1:]+x[0]\n",
    "            df_in.loc[i,'YYYY'] = y        \n",
    "\n",
    "        df_in['YYXX'] = df_in['YYYY'].apply(lambda x: x[:2]) # extract first two digits of year\n",
    "        df_in['XXYY'] = df_in['YYYY'].apply(lambda x: x[-2:]) # extract last two digits of year\n",
    "        df_in['new_YYXX'] = df_in['YYXX']\n",
    "        df_in.loc[(~df_in['YYXX'].isin(['19','20'])) &\n",
    "                    (df_in['XXYY'].astype(int) > int(str(pd.to_datetime('today').year)[-2:])), 'new_YYXX'] = '19'\n",
    "        df_in.loc[(~df_in['YYXX'].isin(['19','20'])) &\n",
    "                    (df_in['XXYY'].astype(int) <= int(str(pd.to_datetime('today').year)[-2:])), 'new_YYXX'] = '20'\n",
    "        df_in.loc[(df_in['YYXX'] == '20') &\n",
    "                    (df_in['XXYY'].astype(int) > int(str(pd.to_datetime('today').year)[-2:])), 'new_YYXX'] = '19'\n",
    "        df_in['new_YYYY'] = df_in['new_YYXX'] + df_in['XXYY']\n",
    "        return df_in\n",
    "\n",
    "\n",
    "    def clean_dates(self,df_in, date_column, plot_years=True):\n",
    "        # Wrapper function for fix_years, plus some other cleaning\n",
    "        ''' Dates formatted with a slash ('/') are in the format DD/MM/YYYY or MM/DD/YYYY '''\n",
    "        ''' Dates formatted with a dash ('-') are in the format YYYY-MM-DD '''\n",
    "        ''' We need to handle these different formats separately '''\n",
    "        original_index = df_in.index\n",
    "        # df_in = df_in.reset_index(drop=True)\n",
    "    \n",
    "        dob_slash = df_in[df_in[date_column].astype(str).str.contains('/')][[date_column]]\n",
    "        dob_dash = df_in[df_in[date_column].astype(str).str.contains('-')][[date_column]]\n",
    "        # there are some patients without a documented DOB\n",
    "        dob_none = df_in[(~df_in[date_column].astype(str).str.contains('/')) &\n",
    "                        (~df_in[date_column].astype(str).str.contains('-'))][[date_column]]\n",
    "\n",
    "        ''' Clean up dates formatted like DD/MM/YYYY or MM/DD/YYYY '''\n",
    "        dob_slash['YYYY'] = dob_slash[date_column].apply(lambda x: str(x).split('/')[-1]) # extract year\n",
    "        dob_slash = self.fix_years(dob_slash, date_column)\n",
    "        dob_slash['new_{}'.format(date_column)] = pd.to_datetime(\n",
    "            dob_slash[date_column].apply(lambda x: '/'.join(x.split('/')[:2])) + '/' + dob_slash['new_YYYY'],\n",
    "            format= 'mixed',\n",
    "            dayfirst=True)\n",
    "\n",
    "        ''' Clean up dates formatted like YYYY-MM-DD '''\n",
    "        dob_dash['YYYY'] = dob_dash[date_column].apply(lambda x: str(x).split('-')[0]) # extract year\n",
    "        dob_dash = self.fix_years(dob_dash, date_column)\n",
    "        dob_dash['new_YYYY'] = dob_dash['new_YYXX'] + dob_dash['XXYY']\n",
    "        dob_dash['new_{}'.format(date_column)] = pd.to_datetime(\n",
    "            dob_dash['new_YYYY'] + '-' + dob_dash[date_column].apply(lambda x: '-'.join(x.split('-')[-2:])),\n",
    "            dayfirst=False)\n",
    "        if plot_years:\n",
    "            pass\n",
    "            # fig, ax = plt.subplots()\n",
    "            # dob_slash['new_YYYY'].astype(int).value_counts().sort_index().plot(style='.', ax=ax, label='slash')\n",
    "            # dob_dash['new_YYYY'].astype(int).value_counts().sort_index().plot(style='.', ax=ax, label='dash')\n",
    "            # plt.title(date_column)\n",
    "            # plt.legend()\n",
    "        return pd.concat([dob_dash, dob_slash, dob_none], axis=0)['new_{}'.format(date_column)].loc[original_index]\n",
    "\n",
    "    # def fix_years(self, df_in, date_column='DOB'):\n",
    "    #     # Fix any years that are not 4 digits long\n",
    "    #     incorrect_length = (df_in['YYYY'].apply(lambda x: len(str(x)) != 4))\n",
    "    #     for i in incorrect_length[incorrect_length].index:\n",
    "    #         x = df_in.loc[i, 'YYYY']\n",
    "    #         x = str(x)\n",
    "    #         if len(x) > 4:\n",
    "    #             y = x[:4]\n",
    "    #         elif len(x) in [2,3]:\n",
    "    #             if x[:2] == '19':\n",
    "    #                 y = '1980'\n",
    "    #             elif x[:2] == '20':\n",
    "    #                 y = '2000'\n",
    "    #             else:\n",
    "    #                 if len(x) == 3:\n",
    "    #                     y = '{}0'.format(x)\n",
    "    #                 else:\n",
    "    #                     y = '00{}'.format(x)\n",
    "    #         else:\n",
    "    #             y = '1980'\n",
    "    #         df_in.loc[i, 'YYYY'] = y\n",
    "    #     # Fix any years that don't start with 1 or 2\n",
    "    #     incorrect_first_digit = (df_in['YYYY'].apply(lambda x: str(x)[0] not in ['1','2']))\n",
    "    #     for i in incorrect_first_digit[incorrect_first_digit].index:\n",
    "    #         x = df_in.loc[i,'YYYY']\n",
    "    #         x = str(x)\n",
    "    #         if not x.isnumeric():\n",
    "        #         y = '1980'\n",
    "        #     else:\n",
    "        #         y = x[1:]+x[0]\n",
    "        #     df_in.loc[i,'YYYY'] = y        \n",
    "        # df_in['YYXX'] = df_in['YYYY'].apply(lambda x: x[:2]) # extract first two digits of year\n",
    "        # df_in['XXYY'] = df_in['YYYY'].apply(lambda x: x[-2:]) # extract last two digits of year\n",
    "        # df_in['new_YYXX'] = df_in['YYXX']\n",
    "        # df_in.loc[(~df_in['YYXX'].isin(['19','20'])) &\n",
    "        #             (df_in['XXYY'].astype(int) > int(str(pd.to_datetime('today').year)[-2:])), 'new_YYXX'] = '19'\n",
    "        # df_in.loc[(~df_in['YYXX'].isin(['19','20'])) &\n",
    "        #             (df_in['XXYY'].astype(int) <= int(str(pd.to_datetime('today').year)[-2:])), 'new_YYXX'] = '20'\n",
    "        # df_in.loc[(df_in['YYXX'] == '20') &\n",
    "        #             (df_in['XXYY'].astype(int) > int(str(pd.to_datetime('today').year)[-2:])), 'new_YYXX'] = '19'\n",
    "        # df_in['new_YYYY'] = df_in['new_YYXX'].astype(str)  + df_in['XXYY'].astype(str) \n",
    "        # return df_in\n",
    "    \n",
    "\n",
    "    # def clean_dates(self,df_in, date_column = 'DOB', plot_years=True):\n",
    "    #     # Wrapper function for fix_years, plus some other cleaning\n",
    "    #     ''' Dates formatted with a slash ('/') are in the format DD/MM/YYYY or MM/DD/YYYY '''\n",
    "    #     ''' Dates formatted with a dash ('-') are in the format YYYY-MM-DD '''\n",
    "    #     ''' We need to handle these different formats separately '''  \n",
    "    #     dob_slash = df_in[df_in[date_column].astype(str).str.contains('/')][[date_column]]\n",
    "    #     dob_dash = df_in[df_in[date_column].astype(str).str.contains('-')][[date_column]]\n",
    "    #     # there are some patients without a documented DOB\n",
    "    #     dob_none = df_in[(~df_in[date_column].astype(str).str.contains('/')) &\n",
    "    #                     (~df_in[date_column].astype(str).str.contains('-'))][[date_column]]\n",
    "    #     ''' Clean up dates formatted like DD/MM/YYYY or MM/DD/YYYY '''\n",
    "    #     dob_slash['YYYY'] = dob_slash[date_column].apply(lambda x: str(x).split('/')[-1]) # extract year\n",
    "    #     dob_slash = self.fix_years(dob_slash, date_column)\n",
    "    #     if (not dob_slash.empty):\n",
    "        #     dob_slash['new_{}'.format(date_column)] = pd.to_datetime(\n",
    "        #         dob_slash[date_column].apply(lambda x: '/'.join(str(x).split('/')[:2])) + '/' + dob_slash['new_YYYY'].astype(str), \n",
    "        #              format='mixed', \n",
    "        #         infer_datetime_format=True,\n",
    "        #         dayfirst=True)\n",
    "        # ''' Clean up dates formatted like YYYY-MM-DD '''\n",
    "        # dob_dash['YYYY'] = dob_dash[date_column].apply(lambda x: str(x).split('-')[0]) # extract year\n",
    "        # dob_dash = self.fix_years(dob_dash, date_column)\n",
    "        # dob_dash['new_YYYY'] = dob_dash['new_YYXX'] + dob_dash['XXYY']\n",
    "        # dob_dash['new_{}'.format(date_column)] = pd.to_datetime(\n",
    "        #     dob_dash['new_YYYY'] + '-' + dob_dash[date_column].apply(lambda x: '-'.join(str(x).split('-')[-2:])),\n",
    "        #     dayfirst=False)\n",
    "        # return pd.concat([dob_dash, dob_slash, dob_none], axis=0).sort_index()['new_{}'.format(date_column)]\n",
    "    \n",
    "\n",
    "    def load_and_clean_nmrl(self,file_path, plot_years=True):\n",
    "        def distinguish_duplicated_sample_ids(row, ind):\n",
    "            return  str(row['Sample ID']) + \"_\" + str(ind)\n",
    "\n",
    "        cols_to_use = ['Sample ID','client sample ID','FirstName','Middle_Surname','Third_Surname',\n",
    "                       'Client Patient ID',\t'DOB','Sex','Date Sampled',\t'Date of Dispatch',\t'Result','Facility Id']\n",
    "        vl_full = pd.read_csv(file_path, low_memory=False, usecols= cols_to_use)\n",
    "        # Drop duplicated rows on Sample Id and Client Patient id:\n",
    "        vl_full = vl_full.drop_duplicates(subset=['Sample ID','Client Patient ID'], keep= 'first')\n",
    "        duplicated_values = list(set(vl_full[vl_full['Sample ID'].duplicated(keep=False)]['Sample ID']))\n",
    "        # Rename duplicated Sample ID's:\n",
    "        vl_full_with_duplicates = vl_full[vl_full['Sample ID'].isin(duplicated_values)]\n",
    "        vl_full_without_duplicates = vl_full[~vl_full['Sample ID'].isin(duplicated_values)]\n",
    "        vl_full_with_duplicates['Sample ID'] = vl_full_with_duplicates.apply(lambda row: distinguish_duplicated_sample_ids(row, row.name), axis=1)\n",
    "        vl_full = pd.concat([vl_full_with_duplicates,vl_full_without_duplicates])\n",
    "        vl_full = vl_full.fillna('empty string')\n",
    "        vl_full = vl_full.replace('', 'empty string')\n",
    "        # print('NMRL data has {:,.0f} records'.format(vl_full.shape[0]))\n",
    "        # Remove any non-alphanumeric characters from names:\n",
    "        vl_full['FirstName'] = vl_full['FirstName'].astype(str).str.replace('\\W', '', regex=True).apply(lambda x: x.lower().replace(' ',''))\n",
    "        vl_full['Middle_Surname'] = vl_full['Middle_Surname'].astype(str).str.replace('\\W', '', regex=True).apply(lambda x: x.lower().replace(' ',''))\n",
    "        vl_full['Third_Surname'] = vl_full['Third_Surname'].astype(str).str.replace('\\W', '', regex=True).apply(lambda x: x.lower().replace(' ',''))\n",
    "        # Clean up Client Patient ID (ART #)\n",
    "        vl_full['Client Patient ID'] = vl_full['Client Patient ID'].astype(str).str.replace('\\W', '', regex=True).apply(lambda x: x.lower().replace(' ',''))\n",
    "        vl_full.loc[:,'clean_art_number'] = vl_full['Client Patient ID']\n",
    "        # If patient only has numeric data in their name, drop them for now\n",
    "        vl_full.loc[vl_full['FirstName'].astype(str).str.isnumeric(), 'FirstName'] = np.nan\n",
    "        vl_full.loc[vl_full['Middle_Surname'].astype(str).str.isnumeric(), 'Middle_Surname'] = np.nan\n",
    "        vl_full.loc[vl_full['Third_Surname'].astype(str).str.isnumeric(), 'Third_Surname'] = np.nan\n",
    "        for n in ['FirstName','Middle_Surname','Third_Surname']:\n",
    "            vl_full.loc[vl_full[n].isna(), n] = np.nan\n",
    "            vl_full.loc[vl_full[n]=='nan', n] = np.nan\n",
    "        ''' Add cleaned dates to dataframe '''\n",
    "        vl_full['Birthdate'] = self.clean_dates(vl_full, 'DOB')\n",
    "        vl_full['sample_date'] = self.clean_dates(vl_full, 'Date Sampled')\n",
    "        vl_full = vl_full.drop(columns=['DOB','Date Sampled'])\n",
    "        ''' Do final cleaning and return '''\n",
    "        vl_full = vl_full.replace(\n",
    "            {'FirstName':{'MISSING':np.nan, '':np.nan},\n",
    "            'Middle_Surname':{'MISSING':np.nan, '':np.nan},\n",
    "            'Third_Surname':{'MISSING':np.nan, '':np.nan}}\n",
    "        ).rename(\n",
    "            columns={\n",
    "                'Middle_Surname':'MiddleName',\n",
    "                'Third_Surname':'LastName',\n",
    "                'Client Patient ID':'Client_Patient_ID'}\n",
    "        )\n",
    "        return vl_full\n",
    "\n",
    "    def log(self,processing_time,province=\"\",district=\"\",facility=\"\",site_code=\"\",filename=\"\",first_date=\"\",last_date=\"\",\n",
    "            db_size=\"\",version=\"\",comment=\"\",ehr_type = \"\",people_registered = \"\",patients_attended = \"\"): \n",
    "         df = pd.DataFrame({\"Time File Processed\":[processing_time],\n",
    "                            \"province\": [province],\n",
    "                            \"district\": [district],\n",
    "                            \"facility\": [facility],\n",
    "                            \"Site Code\":[site_code],\n",
    "                            \"File Name\":[filename],\n",
    "                            \"Date site first used EHR\": [first_date],\n",
    "                            \"Date site last used EHR\": [last_date],\n",
    "                            \"Database Size\": [db_size],\n",
    "                            \"Version\":[version],\n",
    "                            \"Comments\":[comment],\n",
    "                            \"EHR Type\":[ehr_type],\n",
    "                            \"Number of peole registered\": [people_registered],\n",
    "                            \"Number of patients\": [patients_attended]\n",
    "                            })\n",
    "         df.to_csv('log.csv', mode='a', index = False, header=None)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    cbs_extraction = CbsExtraction('./dbs', './')\n",
    "    # 1. Reading mapping file\n",
    "    try:\n",
    "         mapping_file = cbs_extraction.get_mapping_file()\n",
    "    except:  # noqa: E722\n",
    "        print(\"Mapping file not found\")\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # 2. Load NMRL file\n",
    "    try:\n",
    "         if os.path.exists(\"nmrl_reduced.csv\"):\n",
    "              df_nmrl = cbs_extraction.load_and_clean_nmrl('nmrl_reduced.csv')\n",
    "         else:\n",
    "              df_nmrl = cbs_extraction.load_and_clean_nmrl('nmrl_with_facility_id.csv')\n",
    "    except:  # noqa: E722\n",
    "        print(\"NMRL file not found\")\n",
    "      \n",
    "\n",
    "    # 3 create log file\n",
    "    if not Path(cbs_extraction.output_path + \"log.csv\").exists():\n",
    "            log_file_header = [\n",
    "                \"Time File Processed\",\n",
    "                \"Province\",\n",
    "                \"District\",\n",
    "                \"Facility\",\n",
    "                \"Site Code\",\n",
    "                \"File Name\",\n",
    "                \"Date site first used EHR\",\n",
    "                \"Date site last used EHR\",\n",
    "                \"Database Size\",\n",
    "                \"Version\",\n",
    "                \"Comments\",\n",
    "                \"EHR Type\",\n",
    "                \"Number of peole registered in EHR\",\n",
    "                \"Number of people without date of hiv test\"\n",
    "            ]\n",
    "            with open(cbs_extraction.output_path + \"log.csv\",'w') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(log_file_header)\n",
    "    if Path(cbs_extraction.output_path + \"log.csv\").exists():\n",
    "        df_facilities_= pd.read_csv(\"log.csv\")\n",
    "        df_facilities_done = list(df_facilities_[\"File Name\"])\n",
    "        df_site_codes_done = list(df_facilities_[\"Site Code\"])\n",
    "\n",
    "    # 4 a. get list of databases to be processed\n",
    "    facilities = cbs_extraction.get_facility_databases()\n",
    "    print(\">>> Number of databases \", len(facilities))\n",
    "    # 4 b. get total size of all the databases to be processed\n",
    "    size_of_dbs = cbs_extraction.get_folder_size()\n",
    "    print(\">>> Total size:\" , round(size_of_dbs,2) , \" GB\")\n",
    "\n",
    "    # 5 create global dataframes\n",
    "    if os.path.exists(\"global_df_vl_nmrl_art.csv\"):\n",
    "        global_df_stats = pd.read_csv(\"Stats.csv\")\n",
    "        global_df_vl_nmrl_lab = pd.read_csv(\"global_vl_nmrl_lab.csv\")\n",
    "        global_df_vl_nmrl_art = pd.read_csv(\"global_df_vl_nmrl_art.csv\")\n",
    "        global_df_vl_nmrl_demos = pd.read_csv(\"global_df_vl_nmrl_demos.csv\")\n",
    "        global_df_demographics = pd.read_csv(\"Demographics.csv\")\n",
    "        global_df_positives = pd.read_csv(\"global_df_positives.csv\")\n",
    "        global_df_hts_negative = pd.read_csv(\"global_df_negative.csv\")\n",
    "        global_df_cbs = pd.read_csv(\"global_df_cbs.csv\")\n",
    "        global_df_recency = pd.read_csv(\"global_df_recency.csv\")\n",
    "        global_df_art = pd.read_csv(\"global_df_art.csv\")\n",
    "        global_df_art_visit= pd.read_csv(\"global_df_art_visit.csv\")\n",
    "        global_df_art_current_status = pd.read_csv(\"global_df_art_current_status.csv\")\n",
    "        global_df_viral_load = pd.read_csv(\"global_df_viral_load.csv\")\n",
    "        global_df_cd4 = pd.read_csv(\"global_df_cd4.csv\")\n",
    "        global_df_tb  = pd.read_csv(\"global_df_tb.csv\")\n",
    "        global_df_tb_screening = pd.read_csv(\"global_df_tb_screening.csv\")\n",
    "        global_df_transfer_out = pd.read_csv(\"global_df_transfer_out.csv\")\n",
    "        global_df_art_who_stage = pd.read_csv(\"global_df_art_who_stage.csv\")\n",
    "        global_df_laboratory_request = pd.read_csv(\"global_df_laboratory_request.csv\")\n",
    "        global_mother_to_baby = pd.read_csv(\"global_mother_to_baby.csv\")\n",
    "        global_baby_to_mother = pd.read_csv(\"global_baby_to_mother.csv\")\n",
    "        global_df_art_current_status2 = pd.read_csv(\"global_df_art_current_status2.csv\")\n",
    "    else:\n",
    "        global_df_stats = pd.DataFrame()\n",
    "        global_df_demographics = pd.DataFrame()\n",
    "        global_df_positives = pd.DataFrame()\n",
    "        global_df_hts_negative = pd.DataFrame()\n",
    "        global_df_cbs = pd.DataFrame()\n",
    "        global_df_recency = pd.DataFrame()\n",
    "        global_df_art = pd.DataFrame()\n",
    "        global_df_art_visit = pd.DataFrame()\n",
    "        global_df_art_current_status = pd.DataFrame()\n",
    "        global_df_viral_load = pd.DataFrame()\n",
    "        global_df_cd4 = pd.DataFrame()\n",
    "        global_df_tb = pd.DataFrame()\n",
    "        global_df_tb_screening = pd.DataFrame()\n",
    "        global_df_transfer_out =  pd.DataFrame()\n",
    "        global_df_art_who_stage = pd.DataFrame()\n",
    "        global_df_laboratory_request = pd.DataFrame()\n",
    "        global_df_vl_nmrl_art = pd.DataFrame()\n",
    "        global_df_vl_nmrl_demos = pd.DataFrame()\n",
    "        global_df_vl_nmrl_lab = pd.DataFrame()\n",
    "        global_baby_to_mother = pd.DataFrame()\n",
    "        global_mother_to_baby = pd.DataFrame()\n",
    "        global_df_art_current_status2 = pd.DataFrame()\n",
    "\n",
    "    db_size_left = size_of_dbs\n",
    "\n",
    "    # 6 create connection to db \n",
    "    connection = cbs_extraction.get_connection()\n",
    "    \n",
    "    for count , facility  in enumerate(facilities):\n",
    "\n",
    "        try:\n",
    "            '''Logging'''\n",
    "            count = count + 1\n",
    "            print(\".........................................................................................................\")\n",
    "            print(f\">>> {count} / {len(facilities)} {facility}\")\n",
    "            nmrl_size = \"{:,}\".format(df_nmrl.shape[0])\n",
    "            print(\">>> Starting NMRL size\", nmrl_size) \n",
    "            processing_time = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            db_size = cbs_extraction.get_database_size(facility)\n",
    "            db_size_left = db_size_left - db_size\n",
    "            print(f\"db size {db_size} GB\")\n",
    "\n",
    "            # 7 . Skip facilities already processed\n",
    "            if facility in df_facilities_done:\n",
    "                print(\"  >>> facility already done\")\n",
    "                db_size_left = db_size_left - db_size\n",
    "                # cbs_extraction.log(processing_time,\"\",\"\",\"\",\"\",facility,\"\",\"\",db_size,\"\",\"Already Processed\",\"web\",\"\",\"\")\n",
    "                continue\n",
    "\n",
    "            if db_size <= 0:\n",
    "                print(\"  >>> Corrupt database\")\n",
    "                db_size_left = db_size_left - db_size\n",
    "                cbs_extraction.log(processing_time,\"\",\"\",\"\",\"\",facility,\"\",\"\",db_size,\"\",\"Corrupt Database\",\"web\",\"\",\"\")\n",
    "                continue\n",
    "\n",
    "            # 8. Trim database\n",
    "            print(\"  >>> trimming database\")\n",
    "            trimmed_db = cbs_extraction.trim_database(facility)\n",
    "\n",
    "            # 9. Restore database\n",
    "            print(\"  >>> restoring database\")\n",
    "            cbs_extraction.restore_database(trimmed_db)\n",
    "\n",
    "            conn = sqlite3.connect(':memory:')\n",
    "\n",
    "            # 10. Get Facility id and check if database is not empty\n",
    "            df_facility = cbs_extraction.extracting_data(\"\"\"\n",
    "                                SELECT facility_id, time FROM consultation.patient\n",
    "                                where time <= now()\n",
    "                                order by time desc\n",
    "                                limit 1               \n",
    "                            \"\"\", connection)\n",
    "            if df_facility.empty:\n",
    "                print(\">>> database is empty\")\n",
    "                cbs_extraction.log(processing_time,\"\",\"\",\"\",\"\",facility,\"\",\"\",db_size,\"\",\"Database is empty\",\"web\",\"\",\"\")\n",
    "                continue\n",
    "            facility_id, latest_timestamp = df_facility.values.tolist()[0]\n",
    "\n",
    "            # 11. Get facility details (name , province name and district name)\n",
    "            facility_name , district_name, province_name =  cbs_extraction.get_facility_details(mapping_file,facility_id)\n",
    "\n",
    "             # 12. Extract data from all needed tables and populate data into dataframes\n",
    "            print(\"   >>> Extract data from all needed tables\")\n",
    "            ''' a . hts'''\n",
    "            hts_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.hts\n",
    "                                                                \"\"\", connection)\n",
    "            hts_df.to_sql('hts', conn, index=False)\n",
    "\n",
    "            '''b. patient'''\n",
    "            patient_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.patient\n",
    "                                                                \"\"\", connection)\n",
    "            patient_df.to_sql('patient', conn, index=False)\n",
    "\n",
    "            '''c. hts screening'''\n",
    "            hts_screening_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.hts_screening\n",
    "                                                                \"\"\", connection)\n",
    "            hts_screening_df.to_sql('hts_screening', conn, index=False)\n",
    "\n",
    "            '''d. person investigation'''\n",
    "            person_investigation_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.person_investigation\n",
    "                                                                \"\"\", connection)\n",
    "            person_investigation_df.to_sql('person_investigation', conn, index=False)\n",
    "\n",
    "            '''e. cbs'''\n",
    "            cbs_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.cbs\n",
    "                                                                \"\"\", connection)\n",
    "            cbs_df.to_sql('cbs', conn, index=False)\n",
    "\n",
    "            '''f. sexual history'''\n",
    "            sexual_history_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.sexual_history\n",
    "                                                                \"\"\", connection)\n",
    "            sexual_history_df.to_sql('sexual_history', conn, index=False)\n",
    "\n",
    "            '''g. sexual history question'''\n",
    "            sexual_history_question_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.sexual_history_question\n",
    "                                                                \"\"\", connection)\n",
    "            sexual_history_question_df.to_sql('sexual_history_question', conn, index=False)\n",
    "\n",
    "            '''h. patient tb screening'''\n",
    "            patient_tb_screening_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.patient_tb_screening\n",
    "                                                                \"\"\", connection)\n",
    "            patient_tb_screening_df.to_sql('patient_tb_screening', conn, index=False)\n",
    "\n",
    "            '''i. patient client profile'''\n",
    "            patient_client_profile_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.patient_client_profile\n",
    "                                                                \"\"\", connection)\n",
    "            if patient_client_profile_df is not None:\n",
    "                patient_client_profile_df.to_sql('patient_client_profile', conn, index=False)\n",
    "\n",
    "            '''j. art transfer out'''\n",
    "            art_transfer_out_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.art_transfer_out\n",
    "                                                                \"\"\", connection)\n",
    "            art_transfer_out_df.to_sql('art_transfer_out', conn, index=False)\n",
    "\n",
    "            '''k. tb'''\n",
    "            tb_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.tb\n",
    "                                                                \"\"\", connection)\n",
    "            tb_df.to_sql('tb', conn, index=False)\n",
    "\n",
    "            '''l. laboratory investigation test'''\n",
    "            laboratory_investigation_test_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.laboratory_investigation_test\n",
    "                                                                \"\"\", connection)\n",
    "            laboratory_investigation_test_df.to_sql('laboratory_investigation_test', conn, index=False)\n",
    "\n",
    "            '''m. art visit'''\n",
    "            art_visit_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.art_visit\n",
    "                                                                \"\"\", connection)\n",
    "            art_visit_df.to_sql('art_visit', conn, index=False)\n",
    "\n",
    "            '''n. art visit'''\n",
    "            art_who_stage_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.art_who_stage\n",
    "                                                                \"\"\", connection)\n",
    "            art_who_stage_df.to_sql('art_who_stage', conn, index=False)\n",
    "\n",
    "            '''o. art current status'''\n",
    "            art_current_status_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.art_current_status\n",
    "                                                                \"\"\", connection)\n",
    "            art_current_status_df.to_sql('art_current_status', conn, index=False)\n",
    "\n",
    "            '''p. art appointment'''\n",
    "            art_appointment_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.art_appointment\n",
    "                                                                \"\"\", connection)\n",
    "            art_appointment_df.to_sql('art_appointment', conn, index=False)\n",
    "\n",
    "            '''q. art'''\n",
    "            art_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.art\n",
    "                                                                \"\"\", connection)\n",
    "            art_df.to_sql('art', conn, index=False)\n",
    "\n",
    "            '''r. person diagnosis'''\n",
    "            person_diagnosis_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.person_diagnosis\n",
    "                                                                \"\"\", connection)\n",
    "            person_diagnosis_df.to_sql('person_diagnosis', conn, index=False)\n",
    "\n",
    "            '''s. laboratory_request_order'''\n",
    "            laboratory_request_order_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.laboratory_request_order\n",
    "                                                                \"\"\", connection)\n",
    "            laboratory_request_order_df.to_sql('laboratory_request_order', conn, index=False)\n",
    "\n",
    "            '''t. person'''\n",
    "            demographics_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM client.person\n",
    "                                                                \"\"\", connection)\n",
    "            demographics_df.to_sql('demographics', conn, index=False)\n",
    "\n",
    "            '''u. identification'''\n",
    "            identification_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM client.identification\n",
    "                                                                \"\"\", connection)\n",
    "            identification_df.to_sql('identification', conn, index=False)\n",
    "\n",
    "            '''v. phone'''\n",
    "            phone_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM client.phone\n",
    "                                                                \"\"\", connection)\n",
    "            phone_df.to_sql('phone', conn, index=False)\n",
    "\n",
    "            '''w. tb_screening'''\n",
    "            tb_screening_df = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.patient_tb_screening\n",
    "                                                                \"\"\", connection)\n",
    "            \n",
    "            tb_screening_df.to_sql('tb_screening', conn, index=False)\n",
    "\n",
    "\n",
    "            '''x. art current status2'''\n",
    "            art_current_status_df2 = cbs_extraction.extracting_data(\"\"\"\n",
    "                                                        SELECT * FROM consultation.art_current_status\n",
    "                                                                \"\"\", connection)\n",
    "            art_current_status_df2.to_sql('art_current_status', conn, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # 13. Extract cbs data from conctructed data frames\n",
    "            print(\"   >>> extracting cbs data from dataframes\")\n",
    "            '''a. extract positives from hts'''\n",
    "            positives_hts_query = \"\"\"\n",
    "                            WITH ranked_messages AS (\n",
    "                                SELECT\n",
    "                                    h.time AS Event_date,\n",
    "                                    h.patient_id,\n",
    "                                    h.laboratory_investigation_id,\n",
    "                                    p.test,\n",
    "                                    p.person_id,\n",
    "                                    h.hts_number AS 'HTS Number',\n",
    "                                    h.reason_for_not_initiating_art,\n",
    "                                    h.time AS 'Date of HIV Test',\n",
    "                                    h.entry_point AS 'Entry Point',\n",
    "                                    h.purpose AS 'Reason for HIV Test',\n",
    "                                    h.reason_for_not_performing_test AS 'Reason for not doing recency test',\n",
    "                                    h.entry_point_id AS 'Entry Point Id',\n",
    "                                    h.hts_type AS 'Test Method',\n",
    "                                    h.client_already_positive AS 'Already positive',\n",
    "                                    h.client_already_on_art AS 'Already on art',\n",
    "                                    p.result AS HIV_Result,\n",
    "                                    h.pregnant AS 'Pregnant during HIV Test',\n",
    "                                    RANK() OVER (PARTITION BY p.person_id ORDER BY h.time ASC) AS rnt,\n",
    "                                    'hts' AS 'Source of Positive'\n",
    "                                FROM hts h\n",
    "                                INNER JOIN person_investigation p ON h.laboratory_investigation_id = p.person_investigation_id\n",
    "                                WHERE p.result = 'POSITIVE' AND h.purpose <> 'Retesting for ART initiation'\n",
    "                            )\n",
    "                            SELECT *\n",
    "                            FROM ranked_messages\n",
    "                            WHERE rnt = 1;\n",
    "                                \"\"\"\n",
    "            df_positives_from_hts = pd.read_sql_query(positives_hts_query, conn)\n",
    "            df_positives_from_hts = df_positives_from_hts.drop_duplicates(subset=['person_id'])\n",
    "            total_positives_in_hts = df_positives_from_hts.shape[0]\n",
    "            no_date_of_hiv_test_count_hts = df_positives_from_hts[~df_positives_from_hts['Date of HIV Test'].notna()].shape[0]\n",
    "            hts_no_date = df_positives_from_hts[~df_positives_from_hts['Date of HIV Test'].notna()]\n",
    "            if no_date_of_hiv_test_count_hts > 0:\n",
    "                print(f\"   >>> {no_date_of_hiv_test_count_hts}/{total_positives_in_hts} records have no date of hiv test in hts\")\n",
    "            df_positives_from_hts = df_positives_from_hts[df_positives_from_hts['Date of HIV Test'].notna()]\n",
    "            replace_dict = {1:\"YES\",0:\"NO\"}\n",
    "            df_positives_from_hts.replace({\n",
    "                                        \"Already positive\": replace_dict,\n",
    "                                        \"Already on art\": replace_dict,\n",
    "                                        \"Pregnant during HIV Test\": replace_dict\n",
    "                                    }, inplace=True)\n",
    "            df_positives_from_hts.to_sql('hts_positives', conn, index=False)\n",
    "            \n",
    "            '''b. extract positives from cbs'''\n",
    "            positives_cbs_query = \"\"\"\n",
    "                                SELECT\n",
    "                                    c.date_of_hiv_test AS 'Event_date',\n",
    "                                    c.person_id,\n",
    "                                    c.date_of_hiv_test AS 'Date of HIV Test',\n",
    "                                    'POSITIVE' AS HIV_Result,\n",
    "                                    'cbs' AS 'Source of Positive'\n",
    "                                FROM cbs c\n",
    "                                LEFT JOIN hts_positives hp ON c.person_id = hp.person_id\n",
    "                                WHERE hp.person_id IS NULL; \n",
    "                            \"\"\"\n",
    "            df_positives_from_cbs = pd.read_sql_query(positives_cbs_query, conn)\n",
    "            df_positives_from_cbs = df_positives_from_cbs.drop_duplicates(subset=['person_id'])\n",
    "            total_positives_in_cbs = df_positives_from_cbs.shape[0]\n",
    "            no_date_of_hiv_test_count_cbs = df_positives_from_cbs[~df_positives_from_cbs['Date of HIV Test'].notna()].shape[0]\n",
    "            cbs_no_date = df_positives_from_cbs[~df_positives_from_cbs['Date of HIV Test'].notna()]\n",
    "            if no_date_of_hiv_test_count_cbs > 0:\n",
    "                print(f\"   >>> {no_date_of_hiv_test_count_cbs}/{total_positives_in_cbs} records have no date of hiv test in cbs\")\n",
    "            df_positives_from_cbs = df_positives_from_cbs[df_positives_from_cbs['Date of HIV Test'].notna()]\n",
    "            df_positives_from_cbs.to_sql('cbs_positives', conn, index=False)\n",
    "\n",
    "            '''c. extract positives from art'''\n",
    "            positives_art_query = \"\"\"\n",
    "                                SELECT\n",
    "                                    a.date_of_hiv_test AS 'Event_date',\n",
    "                                    a.person_id,\n",
    "                                    a.date_of_hiv_test AS 'Date of HIV Test',\n",
    "                                    'POSITIVE' AS HIV_Result,\n",
    "                                    'ART' AS 'Source of Positive'\n",
    "                                FROM art a\n",
    "                                LEFT JOIN hts_positives hp ON a.person_id = hp.person_id\n",
    "                                LEFT JOIN cbs_positives cp ON a.person_id = cp.person_id\n",
    "                                WHERE hp.person_id IS NULL AND cp.person_id IS NULL;\n",
    "                            \"\"\"\n",
    "            df_positives_from_art = pd.read_sql_query(positives_art_query, conn)\n",
    "            df_positives_from_art = df_positives_from_art.drop_duplicates(subset=['person_id'])\n",
    "            total_positives_in_art = df_positives_from_art.shape[0]\n",
    "            no_date_of_hiv_test_count_art = df_positives_from_art[~df_positives_from_art['Date of HIV Test'].notna()].shape[0]\n",
    "            art_no_date = df_positives_from_art[~df_positives_from_art['Date of HIV Test'].notna()]\n",
    "            if no_date_of_hiv_test_count_art > 0:\n",
    "                print(f\"   >>> {no_date_of_hiv_test_count_art}/{total_positives_in_art} records have no date of hiv test in art\")\n",
    "            df_positives_from_art = df_positives_from_art[df_positives_from_art['Date of HIV Test'].notna()]\n",
    "            \n",
    "            '''Logging . check if there are no positives in the database'''\n",
    "            if df_positives_from_hts.empty and df_positives_from_cbs.empty and df_positives_from_art.empty:\n",
    "                print(\">>> There are no positive cases in the database\")\n",
    "                cbs_extraction.log(processing_time,province_name,district_name,facility_name, facility_id, facility,\"\",\"\",db_size,\"\",\"Failed,No positives \",\"web\",\"\",\"\")\n",
    "                continue\n",
    "            \n",
    "            '''d. Merge all positives from hts , cbs and art'''\n",
    "            df_positives = pd.concat([df_positives_from_hts,df_positives_from_cbs,df_positives_from_art])\n",
    "\n",
    "            '''e. extract hiv test results for first, second and third tests'''\n",
    "            hts_investigations_query = \"\"\"\n",
    "                                WITH ranked_messages AS (\n",
    "                                    SELECT\n",
    "                                        h.laboratory_investigation_id,\n",
    "                                        h.result,\n",
    "                                        h.time,\n",
    "                                        p.person_id,\n",
    "                                        ROW_NUMBER() OVER (PARTITION BY  h.laboratory_investigation_id ORDER BY h.time ASC) AS rnt\n",
    "                                    FROM person_investigation p\n",
    "                                    LEFT JOIN laboratory_investigation_test h ON p.person_investigation_id = h.laboratory_investigation_id\n",
    "                                )\n",
    "                                SELECT *\n",
    "                                FROM ranked_messages\n",
    "                                WHERE laboratory_investigation_id IN (\n",
    "                                    SELECT laboratory_investigation_id\n",
    "                                    FROM hts_positives\n",
    "                                );\n",
    "                            \"\"\"\n",
    "            df_hts_investigations = pd.read_sql_query(hts_investigations_query, conn)    \n",
    "            df_hts_investigations = df_hts_investigations[df_hts_investigations['rnt']<=3]\n",
    "            df_hts_investigations['rnt'] = df_hts_investigations['rnt'].map(\n",
    "                {1:\"HIVtestOneResult\", 2: \"HIVtestTwoResult\",3:\"HIVtestThreeResult\"})\n",
    "            df_hts_investigations.drop_duplicates(subset=['person_id', 'rnt'], inplace=True)\n",
    "            df_hts_investigations = df_hts_investigations.pivot(index=['person_id','laboratory_investigation_id'], columns=['rnt'], values='result')\n",
    "            df_hts_investigations = df_hts_investigations.reset_index()\n",
    "            column_order = {'person_id': 1, \n",
    "                                'HIVtestOneResult':2,'HIVtestTwoResult':3,\n",
    "                                'HIVtestThreeResult':4,\n",
    "                        'laboratory_investigation_id':5\n",
    "                                }\n",
    "            df_hts_investigations = df_hts_investigations[[col for col in sorted(df_hts_investigations.columns, key=lambda x: column_order.get(x, float('inf')))]]\n",
    "\n",
    "            '''f. merge dataframe of positives with the investigations done'''\n",
    "            df_positives_with_investigations = pd.merge(df_positives,df_hts_investigations, on =[\"laboratory_investigation_id\",\"person_id\"], how =\"left\")\n",
    "\n",
    "            '''g. extract sexual status'''\n",
    "            sexual_history_query = \"\"\"\n",
    "                                SELECT\n",
    "                                    sh.sexual_history_id, sh.person_id, sh.patient_id, sh.sexually_active, sh.date\n",
    "                                FROM sexual_history sh\n",
    "                                LEFT JOIN hts_positives hp ON sh.patient_id = hp.patient_id\n",
    "                                WHERE hp.patient_id is not NULL;\n",
    "                            \"\"\"\n",
    "            df_sexual_history = pd.read_sql_query(sexual_history_query, conn)  \n",
    "            replace_dict = {1:\"YES\",0:\"NO\"}\n",
    "            df_sexual_history.replace({\"sexually_active\": replace_dict},inplace=True)\n",
    "            df_sexual_history.to_sql('sexual_history_table', conn, index=False)\n",
    "            \n",
    "            '''h. extract sexual history questions'''\n",
    "            sexual_history_question_query = \"\"\"\n",
    "                                SELECT shq.sexual_history_id, shq.question, shq.response_type\n",
    "                                FROM sexual_history_question shq\n",
    "                                INNER JOIN sexual_history_table sh ON shq.sexual_history_id = sh.sexual_history_id;\n",
    "                            \"\"\"\n",
    "            df_sexual_history_question = pd.read_sql_query(sexual_history_question_query, conn)   \n",
    "            df_sexual_history_question.drop_duplicates(subset=[\"sexual_history_id\",\"question\"],inplace = True)\n",
    "            df_sexual_history_question = df_sexual_history_question.pivot(index='sexual_history_id', columns='question')\n",
    "            df_sexual_history_question.columns = df_sexual_history_question.columns.droplevel()\n",
    "            df_sexual_history_question.reset_index(inplace=True)\n",
    "            df_sexual_history_question = pd.merge(df_sexual_history_question, df_sexual_history , how=\"left\", on=[\"sexual_history_id\"])\n",
    "            df_sexual_history_question.rename(columns = {'Exchanged sex for  money/material goods':'Exchanged sex for moneymaterial goods',\n",
    "                                                            'Victim/ Suspected victim of sexual abuse':'Victim Suspected victim of sexual abuse',\n",
    "                                                            'Unprotected sex without a condom':'Unprotected sex without a condom',\n",
    "                                                            'sexually_active': 'Sexually Active' }, inplace = True)\n",
    "            column_order = {'Event_date': 1, \n",
    "                                'person_id':2,'Been incarcerated into jail':3,\n",
    "                        'Exchanged sex for moneymaterial goods':4,\n",
    "                        'Had Anal Sex':5,'Had sex with male':6,'Had sex with female':7,'Had sex with a sex worker':8,\n",
    "                        'Victim Suspected victim of sexual abuse':9,'Tattooed with unsterilized instruments':10,\n",
    "                        'Received medical injections':11, 'Unprotected sex without a condom':12, 'Injected recreational drugs':13,\n",
    "                        'History of an STI':14, 'Had Oral Sex':15,  'Received blood transfusions':16, 'Sexually Active':17,\n",
    "                        'patient_id':18\n",
    "                                }\n",
    "            df_sexual_history_question = df_sexual_history_question[[col for col in sorted(df_sexual_history_question.columns, key=lambda x: column_order.get(x, float('inf')))]]\n",
    "            df_sexual_history_question.drop_duplicates(subset=['patient_id'], keep='last',inplace=True)\n",
    "            \n",
    "            '''i. merge dataframe df_positives_with_investigations (13.h) with sexual history'''\n",
    "            df_positives_complete = pd.merge(df_positives_with_investigations,df_sexual_history_question, on =[\"patient_id\",\"person_id\"] , how = \"left\")\n",
    "            df_positives_complete.insert(0,\"HIV Positive Province Name\",province_name)\n",
    "            df_positives_complete.insert(1,\"HIV Positive District Name\",district_name)\n",
    "            df_positives_complete.insert(2,\"HIV Positive Facility Name\",facility_name)\n",
    "            df_positives_complete.insert(3,\"HIV Positive Facility Id\",facility_id)\n",
    "            df_positives_complete.insert(4,\"HIV Positive DB Name\",facility)\n",
    "            \n",
    "            df_positives_complete.rename(columns = {\"HIV_Result\":\"Final HIV Result\" ,\n",
    "                                            \"reason_for_not_initiating_art\":\"Reason for not initiating on ART\"  },inplace = True)\n",
    "            \n",
    "            df_positives_complete.replace(replace_dict, inplace=True)\n",
    "            df_positives_complete.drop_duplicates(subset=['person_id','Date of HIV Test'], keep='first',inplace=True)\n",
    "            df_positives_complete.to_sql('positives_complete', conn, index=False)\n",
    "            ''' j. add df_positives_complete to the global dataframe'''\n",
    "            global_df_positives = pd.concat([global_df_positives,df_positives_complete])\n",
    "\n",
    "            '''k. extract data from recency '''\n",
    "            recency_query = \"\"\"\n",
    "                                SELECT \n",
    "                                    pi.date as 'Event_date',\n",
    "                                    pi.person_id, pi.date as 'Date Recency Done',\n",
    "                                    pi.person_investigation_id,\n",
    "                                    pi.result as 'Recency Result'\n",
    "                                FROM person_investigation pi\n",
    "                                WHERE pi.test = 'HIV-1 Rapid Recency'\n",
    "                            \"\"\"            \n",
    "            df_recency = pd.read_sql_query(recency_query, conn)  \n",
    "            df_recency.insert(0,\"Recency Province Name\",province_name)\n",
    "            df_recency.insert(1,\"Recency District Name\",district_name)\n",
    "            df_recency.insert(2,\"Recency Facility Name\",facility_name)\n",
    "            df_recency.insert(3,\"Recency Facility ID\",facility_id)\n",
    "            df_recency.insert(4,\"Recency DB Name\",facility)\n",
    "            df_recency = df_recency.sort_values(by=['Recency Result'])\n",
    "            df_recency.drop_duplicates(subset=['Event_date','person_id'], keep='first',inplace = True)\n",
    "            global_df_recency = pd.concat([global_df_recency,df_recency])\n",
    "\n",
    "            '''l. extract data for demographics'''\n",
    "            demographics_query = \"\"\"\n",
    "                        SELECT \n",
    "                            d.person_id,d.firstname as FirstName,d.lastname as LastName, d.birthdate as Birthdate, d.sex as Sex, \n",
    "                            d.self_identified_gender as 'Self Identified Gender',\n",
    "                            d.marital as 'Marital Status',\n",
    "                            d.education as 'Education',d.occupation as 'Occupation',\n",
    "                            d.religion as 'Religion',d.nationality_id as 'Nationality Id',\n",
    "                            d.nationality as Nationality, d.street as 'Street',d.city as City, \n",
    "                            d.town as 'Residential Town',i.type as 'Identifier Type' , i.number as 'Identifier Number',p.number as 'Phone Number'\n",
    "                        FROM demographics d\n",
    "                        LEFT JOIN identification as i\n",
    "                            ON d.person_id = i.person_id \n",
    "                        LEFT JOIN phone p\n",
    "                            ON d.person_id = p.person_id\n",
    "                    \"\"\"\n",
    "            df_demographics_extract = pd.read_sql_query(demographics_query, conn) \n",
    "            df_demographics_extract.sort_values(['person_id','Identifier Number'],inplace=True)\n",
    "            df_demographics_extract.drop_duplicates(subset=['person_id'],keep='first',inplace=True)\n",
    "\n",
    "            ''' m. extract data from patient client profile'''\n",
    "            if patient_client_profile_df is not None:\n",
    "                patient_client_profile_query = \"\"\"\n",
    "                        SELECT \n",
    "                            person_id, client_profile as 'Client Profile', date\n",
    "                        FROM patient_client_profile\n",
    "                \"\"\"\n",
    "                df_patient_client_profile = pd.read_sql_query(patient_client_profile_query, conn) \n",
    "                df_patient_client_profile.sort_values(by=['person_id','date'], inplace=True)\n",
    "                df_patient_client_profile.drop_duplicates(subset=['person_id'], keep='last',inplace = True)\n",
    "                df_demographics = pd.merge(df_demographics_extract,df_patient_client_profile,on =[\"person_id\"],how=\"left\")\n",
    "            else:\n",
    "                df_demographics = df_demographics_extract\n",
    "        \n",
    "            df_demographics.insert(0,\"Province Name\",province_name)\n",
    "            df_demographics.insert(1,\"District Name\",district_name)\n",
    "            df_demographics.insert(2,\"Facility Name\",facility_name)\n",
    "            df_demographics.insert(3,\"Facility Id\",facility_id)\n",
    "            df_demographics.insert(4,\"Facility DB Name\",facility)\n",
    "            ''' n. add demographics to the global dataframe'''\n",
    "            global_df_demographics = pd.concat([global_df_demographics,df_demographics])\n",
    "\n",
    "            ''' o. extract hiv negative data from hts'''\n",
    "            hts_negative_query =  \"\"\"\n",
    "                    SELECT \n",
    "                        'hts' as 'Source of Last Negative',\n",
    "                        h.time as Event_date,\n",
    "                        p.person_id,\n",
    "                        h.time as 'Last HIV negative date'\n",
    "                    FROM hts h  \n",
    "                    LEFT JOIN person_investigation p\n",
    "                        ON h.laboratory_investigation_id = p.person_investigation_id\n",
    "                    WHERE  p.test = 'HIV' AND p.result in ('Negative' ,'NEGATIVE' ,'NEG','Neg')\n",
    "                \"\"\"               \n",
    "            df_hts_negative = pd.read_sql_query(hts_negative_query, conn) \n",
    "            df_hts_negative.to_sql('df_hts_negative', conn, index=False)\n",
    "\n",
    "            '''p. extract hiv negative data from hts screening'''\n",
    "            hts_screening_negative_query = \"\"\"\n",
    "                    SELECT \n",
    "                        'HTS Screening' as 'Source of Last Negative',\n",
    "                        h.date_last_tested as Event_date,\n",
    "                        p.person_id ,\n",
    "                        h.date_last_tested as 'Last HIV negative date'\n",
    "                    FROM hts_screening h\n",
    "                    LEFT JOIN patient p\n",
    "                        ON h.patient_id = p.patient_id\n",
    "                    WHERE result IN ('Negative' ,'NEGATIVE' ,'NEG','Neg')\n",
    "                \"\"\"   \n",
    "            df_hts_screening_negative = pd.read_sql_query(hts_screening_negative_query, conn) \n",
    "            df_hts_screening_negative.to_sql('df_hts_screening_negative', conn, index=False) \n",
    "               \n",
    "            '''q. extract hiv negative data from investigation'''\n",
    "            investigation_negative_query = \"\"\"\n",
    "                    SELECT \n",
    "                                'Investigation' as 'Source of Last Negative',\n",
    "                                p.date as Event_date,\n",
    "                                p.person_id,\n",
    "                                p.date as 'Last HIV negative date'\n",
    "                    FROM person_investigation p\n",
    "                    LEFT JOIN df_hts_negative\n",
    "                        ON p.person_id = df_hts_negative.person_id\n",
    "                    LEFT JOIN df_hts_screening_negative\n",
    "                        ON p.person_id = df_hts_screening_negative.person_id\n",
    "                    WHERE  test = 'HIV' and result in ('Negative' ,'NEGATIVE' ,'NEG','Neg')  \n",
    "                        AND df_hts_negative.person_id IS NULL\n",
    "                        AND df_hts_screening_negative.person_id IS NULL                                \n",
    "                            \n",
    "                \"\"\"\n",
    "            df_investigation_negative = pd.read_sql_query(investigation_negative_query, conn) \n",
    "\n",
    "            '''r. create one dataframe on negatives '''\n",
    "            df_negatives = pd.concat([df_hts_negative,df_investigation_negative,df_hts_screening_negative])\n",
    "            df_negatives.insert(0,\"Last Negative Province Name\",province_name)\n",
    "            df_negatives.insert(1,\"Last Negative District Name\",district_name)\n",
    "            df_negatives.insert(2,\"Last Negative Facility Name\",facility_name)\n",
    "            df_negatives.insert(3,\"Last Negative Facility ID\",facility_id)\n",
    "            df_negatives.insert(4,\"last negative DB Name\",facility)\n",
    "            ''' s. add negatives to the global dataframe'''\n",
    "            global_df_hts_negative = pd.concat([global_df_hts_negative,df_negatives])\n",
    "\n",
    "            '''t. extract data from cbs '''\n",
    "            cbs_query = \"\"\"\n",
    "                    SELECT date as 'Event_date' ,\n",
    "                        person_id,'Yes' as 'Notified', date as 'Date of Notification' , been_on_prep as 'Been On Prep' \n",
    "                    FROM cbs \n",
    "            \"\"\"\n",
    "            df_cbs = pd.read_sql_query(cbs_query, conn) \n",
    "            df_cbs.insert(0,\"Facility Id\",facility_id)\n",
    "            df_cbs.drop_duplicates(subset=['Event_date','person_id'], keep='last',inplace = True)\n",
    "            global_df_cbs = pd.concat([global_df_cbs,df_cbs])\n",
    "\n",
    "\n",
    "            '''u. extract data from art '''\n",
    "            art_query = \"\"\"\n",
    "                    SELECT  date as 'Event_date',\n",
    "                        art_id,person_id,art_number as \"Art Number\",art_cohort_number as \"Art Cohort Number\",\n",
    "                        date as \"Art_Initiation_Date\",date_enrolled as \"Date Enrolled into ART\"\n",
    "                    FROM art \n",
    "                \"\"\"\n",
    "\n",
    "            df_art = pd.read_sql_query(art_query, conn) \n",
    "            df_art.drop_duplicates(subset=['Event_date','person_id'], keep='last',inplace = True)\n",
    "            df_art.insert(0,\"Art Province Name\",province_name)\n",
    "            df_art.insert(1,\"Art District Name\",district_name)\n",
    "            df_art.insert(2,\"Art Facility Name\",facility_name)\n",
    "            df_art.insert(3,\"Art Facility ID\",facility_id)\n",
    "            df_art.insert(4,\"Art DB Name\",facility)\n",
    "            global_df_art = pd.concat([global_df_art,df_art])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            '''u. extract data from art current status '''\n",
    "            art_query = \"\"\"\n",
    "                    SELECT  date as 'Event_date',\n",
    "                        art_id,person_id,art_number as \"Art Number\",art_cohort_number as \"Art Cohort Number\",\n",
    "                        date as \"Art_Initiation_Date2\",date_enrolled as \"Date Enrolled into ART\"\n",
    "                    FROM art \n",
    "                    order by art_id, date\n",
    "                \"\"\"\n",
    "            df_art_current_status2 = pd.read_sql_query(art_current_status_query, conn)\n",
    "            df_art_current_status2.drop_duplicates(subset=['art_id'], keep='first',inplace = True)\n",
    "            df_art_current_status2.insert(0,\"Art Current Status Province Name\",province_name)\n",
    "            df_art_current_status2.insert(1,\"Art Current Status District Name\",district_name)\n",
    "            df_art_current_status2.insert(2,\"Art Current Status Facility Name\",facility_name)\n",
    "            df_art_current_status2.insert(3,\"Art Current Status Facility ID\",facility_id)\n",
    "            df_art_current_status2.insert(4,\"Art Current Status Stage DB Name\",facility)\n",
    "            global_df_art_current_status2 = pd.concat([global_df_art_current_status2,df_art_current_status2])\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            '''v. extract data from art_visit '''\n",
    "            art_visit_query = \"\"\"\n",
    "                    SELECT p.time as 'Event_date', \n",
    "                        p.time as \"Art visit date\", p.person_id, a.visit_type as 'Art visit type',\n",
    "                        a.tb_status as 'TB Status', a.family_planning_status, a.lactating_status\n",
    "                    FROM art_visit a left join patient p\n",
    "                        ON a.patient_id = p.patient_id\n",
    "                    \n",
    "                \"\"\"  \n",
    "\n",
    "            df_art_visit = pd.read_sql_query(art_visit_query, conn)\n",
    "            df_art_visit.drop_duplicates(subset=['Event_date','person_id'], keep='last',inplace = True)\n",
    "            df_art_visit = df_art_visit.replace('Presumptive-if there are signs', 'Suspect or Presumptive')\n",
    "            df_art_visit = df_art_visit.replace('Screened and has no signs', 'Screened with no signs')\n",
    "            df_art_visit = df_art_visit.replace('Tb status not assesssed', '')\n",
    "            df_art_visit.insert(0,\"Art Visit Province Name\",province_name)\n",
    "            df_art_visit.insert(1,\"Art Visit District Name\",district_name)\n",
    "            df_art_visit.insert(2,\"Art Visit Facility Name\",facility_name)\n",
    "            df_art_visit.insert(3,\"Art Visit Facility ID\",facility_id)\n",
    "            df_art_visit.insert(4,\"Art Visit DB Name\",facility)\n",
    "            global_df_art_visit = pd.concat([global_df_art_visit,df_art_visit])\n",
    "\n",
    "\n",
    "            '''w. extract data from art_current_status '''\n",
    "            art_current_status_query = \"\"\"\n",
    "                    SELECT a.date as 'Event_date', \n",
    "                        a.date as 'Art Current Status Date',a.art_id ,a.regimen as regimen , a.state as 'ARV Status', a.art_initiation_category as 'Art Initiation Category',\n",
    "                        art.person_id,a.regimen_id as 'Regimen Id'\n",
    "                    FROM art_current_status a \n",
    "                    LEFT JOIN art \n",
    "                        ON a.art_id = art.art_id \n",
    "                \"\"\"     \n",
    "\n",
    "            df_art_current_status = pd.read_sql_query(art_current_status_query, conn)\n",
    "            df_art_current_status.drop_duplicates(subset=['Event_date','person_id'], keep='last',inplace = True)\n",
    "            df_art_current_status.insert(0,\"Art Current Status Province Name\",province_name)\n",
    "            df_art_current_status.insert(1,\"Art Current Status District Name\",district_name)\n",
    "            df_art_current_status.insert(2,\"Art Current Status Facility Name\",facility_name)\n",
    "            df_art_current_status.insert(3,\"Art Current Status Facility ID\",facility_id)\n",
    "            df_art_current_status.insert(4,\"Art Current Status Stage DB Name\",facility)\n",
    "            global_df_art_current_status = pd.concat([global_df_art_current_status,df_art_current_status])\n",
    "\n",
    "            '''x. extract data from art who stage '''\n",
    "            art_who_stage_query = \"\"\"\n",
    "                        SELECT  a.date as 'Event_date',\n",
    "                            p.person_id , a.art_id, a.date as 'Who Stage Date',\n",
    "                            a.date as 'Outcome Date',\n",
    "                            a.stage as 'Who Stage', a.follow_up_status as 'Art Outcome'\n",
    "                        FROM art_who_stage a left join art p\n",
    "                            on a.art_id  = p.art_id\n",
    "                        \n",
    "                    \"\"\"\n",
    "\n",
    "            df_art_who_stage = pd.read_sql_query(art_who_stage_query, conn)\n",
    "            df_art_who_stage.drop_duplicates(subset=['Event_date','person_id'], keep='last',inplace = True)\n",
    "            df_art_who_stage.insert(0,\"Who stage Province Name\",province_name)\n",
    "            df_art_who_stage.insert(1,\"Who stage District Name\",district_name)\n",
    "            df_art_who_stage.insert(2,\"Who stage Facility Name\",facility_name)\n",
    "            df_art_who_stage.insert(3,\"Who stage Facility ID\",facility_id)\n",
    "            df_art_who_stage.insert(4,\"Who Stage DB Name\",facility)\n",
    "            global_df_art_who_stage = pd.concat([global_df_art_who_stage,df_art_who_stage])\n",
    "\n",
    "            '''y. extract data from viral load '''\n",
    "            viral_load_query = \"\"\"\n",
    "                    SELECT date as \"Event_date\",\n",
    "                        person_id,date as \"Date at which Viral Load result was issued\",\n",
    "                        date as \"Date for which Viral Load was taken\",\n",
    "                        'TRUE' as \"Viral Load Sample submitted to lab\",\n",
    "                        'TRUE' as \"Was Viral Load result issued\",\n",
    "                        result as \"Viral Load result\"\n",
    "                    FROM person_investigation \n",
    "                    WHERE test = 'Viral Load'  \n",
    "                    \n",
    "                \"\"\"\n",
    "            df_viral_load = pd.read_sql_query(viral_load_query, conn) \n",
    "            df_viral_load.drop_duplicates(subset=['Event_date','person_id'], keep='last', inplace = True)\n",
    "            df_viral_load.insert(0,\"Viral Load Province Name\",province_name)\n",
    "            df_viral_load.insert(1,\"Viral Load District Name\",district_name)\n",
    "            df_viral_load.insert(2,\"Viral Load Facility Name\",facility_name)\n",
    "            df_viral_load.insert(3,\"Viral Load Facility ID\",facility_id)\n",
    "            df_viral_load.insert(4,\"Viral Load DB Name\",facility)\n",
    "            global_df_viral_load = pd.concat([global_df_viral_load,df_viral_load])\n",
    "\n",
    "            '''z. extract data from cd4 '''\n",
    "            cd4_query = \"\"\"\n",
    "                    SELECT date as \"Event_date\",\n",
    "                        person_id, date as 'Date at which cd4 sample was taken',\n",
    "                        date as 'Date at which cd4 result was issued',\n",
    "                        'TRUE' as \"CD4 Sample submitted to lab\",\n",
    "                        'TRUE' as \"Was cd4 result issued\",\n",
    "                        result as 'CD4 Count'\n",
    "                    FROM person_investigation where test = 'CD4 Count' \n",
    "                \"\"\"       \n",
    "\n",
    "            df_cd4 = pd.read_sql_query(cd4_query, conn)\n",
    "\n",
    "            df_cd4.drop_duplicates(subset=['Event_date','person_id'], keep='last',inplace = True)\n",
    "            df_cd4.insert(0,\"cd4 Province Name\",province_name)\n",
    "            df_cd4.insert(1,\"cd4 District Name\",district_name)\n",
    "            df_cd4.insert(2,\"cd4 Facility Name\",facility_name)\n",
    "            df_cd4.insert(3,\"cd4 Facility ID\",facility_id)\n",
    "            df_cd4.insert(4,\"cd4 DB Name\",facility)\n",
    "            global_df_cd4 = pd.concat([global_df_cd4,df_cd4])\n",
    "\n",
    "            '''aa. extract data from cd4 '''\n",
    "            TB_query = \"\"\"\n",
    "                    SELECT date as 'Event_date',\n",
    "                        person_id,date as 'TB Treatment Start Date',type_of_tb as 'Type Of TB',\n",
    "                        tb_disease_site as 'TB Disease Site',\n",
    "                        tb_disease_type as 'TB Disease Type',outcome as 'TB Outcome'\n",
    "                    FROM tb \n",
    "                \"\"\"\n",
    "\n",
    "            df_TB = pd.read_sql_query(TB_query, conn)\n",
    "            df_TB.drop_duplicates(subset=['Event_date','person_id'], keep='last',inplace =  True)\n",
    "            df_TB.insert(0,\"TB Province Name\",province_name)\n",
    "            df_TB.insert(1,\"TB District Name\",district_name)\n",
    "            df_TB.insert(2,\"TB Facility Name\",facility_name)\n",
    "            df_TB.insert(3,\"TB Facility Id\",facility_id)\n",
    "            df_TB.insert(4,\"TB DB Name\",facility)\n",
    "            global_df_tb = pd.concat([global_df_tb,df_TB])\n",
    "\n",
    "            '''ab. extract data from TB screening '''\n",
    "            TB_Screening_query = \"\"\"\n",
    "                    SELECT t.time as 'Event_date',\n",
    "                        t.person_id , p.presumptive as 'TB Screened'\n",
    "                    FROM tb_screening p\n",
    "                    LEFT JOIN patient t \n",
    "                        on p.patient_id = t.patient_id\n",
    "                \"\"\"  \n",
    "\n",
    "            df_TB_Screening = pd.read_sql_query(TB_Screening_query, conn)      \n",
    "        \n",
    "            df_TB_Screening = df_TB_Screening.replace(1, 'Suspect or Presumptive')\n",
    "            df_TB_Screening = df_TB_Screening.replace(0, 'Screened with no signs')\n",
    "            \n",
    "            df_TB_Screening.drop_duplicates(subset=['Event_date','person_id'], keep='last',inplace =  True)\n",
    "            df_TB_Screening.insert(0,\"TB Screening Province Name\",province_name)\n",
    "            df_TB_Screening.insert(1,\"TB Screening District Name\",district_name)\n",
    "            df_TB_Screening.insert(2,\"TB Screening Facility Name\",facility_name)\n",
    "            df_TB_Screening.insert(3,\"TB Screening Facility Id\",facility_id)\n",
    "            df_TB_Screening.insert(4,\"TB Screening DB Name\",facility)\n",
    "            global_df_tb_screening = pd.concat([global_df_tb_screening,df_TB_Screening])\n",
    "\n",
    "            '''ac. extract data from transfer out '''\n",
    "            transfer_out_query = \"\"\"\n",
    "                    SELECT \n",
    "                        a.person_id , t.art_id , t.transfer_out_date as 'Event_date',\n",
    "                        t.transfer_out_date as 'Transfer Out date',\n",
    "                        t.transfer_reason as 'Transfer Reason', t.transfer_facility_id\n",
    "                    FROM art_transfer_out t \n",
    "                    LEFT JOIN art a\n",
    "                        ON t.art_id = a.art_id\n",
    "                \"\"\"\n",
    "\n",
    "            df_transfer_out = pd.read_sql_query(transfer_out_query, conn)\n",
    "            df_transfer_out.drop_duplicates(subset=['Event_date','person_id'], keep='last',inplace = True)\n",
    "            df_transfer_out.insert(0,\"Transfer Out Province Name\",province_name)\n",
    "            df_transfer_out.insert(1,\"Transfer Out District Name\",district_name)\n",
    "            df_transfer_out.insert(2,\"Transfer Out Facility Name\",facility_name)\n",
    "            df_transfer_out.insert(3,\"Transfer Out Facility Id\",facility_id)\n",
    "            global_df_transfer_out = pd.concat([global_df_transfer_out,df_transfer_out])\n",
    "        \n",
    "            '''ad. Extract data to use for merging with NMRL viral load data '''\n",
    "            laboratory_request_order_query = \"\"\"\n",
    "                                            SELECT p.person_id, \n",
    "                                                d.firstname,d.lastname,d.birthdate,d.sex,\n",
    "                                                l.laboratory_request_order_id,\n",
    "                                                l.laboratory_request_number,\n",
    "                                                l.laboratory_investigation_id,\n",
    "                                                l.facility_id,\n",
    "                                                l.request_reason,\n",
    "                                                l.laboratory,\n",
    "                                                l.date_sample_taken,\n",
    "                                                l.result,\n",
    "                                                l.result_date,\n",
    "                                                l.result_issued,\n",
    "                                                a.art_id,\n",
    "                                                a.art_number\n",
    "                                            FROM laboratory_request_order l\n",
    "                                            LEFT JOIN person_investigation p\n",
    "                                                on l.laboratory_investigation_id = p.person_investigation_id\n",
    "                                            LEFT JOIN art a\n",
    "                                                on p.person_id = a.person_id\n",
    "                                            LEFT JOIN demographics d\n",
    "                                                on p.person_id = d.person_id\n",
    "                                                \"\"\"\n",
    "            df_laboratory_request_order = pd.read_sql_query(laboratory_request_order_query, conn)\n",
    "            global_df_laboratory_request = pd.concat([global_df_laboratory_request,df_laboratory_request_order])\n",
    "\n",
    "            '''ae. clean lab request number'''\n",
    "            print(\"   >>> clean lab request number\")\n",
    "            df_laboratory_request_order.loc[:,'clean_laboratory_request_number'] = df_laboratory_request_order[\"laboratory_request_number\"].astype(str).str.replace('\\W', '', regex=True).apply(lambda x: x.lower().replace(' ',''))\n",
    "            df_nmrl.loc[:, 'clean_laboratory_request_number'] = df_nmrl[\"client sample ID\"].astype(str).str.replace('\\W', '', regex=True).apply(lambda x: x.lower().replace(' ',''))\n",
    "            \n",
    "            '''af. merging nmrl data on lab request number'''\n",
    "            print(\"   >>> merging on lab request number\")\n",
    "            df_nmrl_on_lab_id = pd.DataFrame()\n",
    "            df_nmrl_on_lab_id_facility =  df_nmrl[df_nmrl['Facility Id'] == facility_id]\n",
    "            if df_nmrl_on_lab_id_facility.shape[0] != 0:\n",
    "                df_nmrl_on_lab_id , df_nmrl = cbs_extraction.match_on_lab(\n",
    "                    df_laboratory_request_order,\n",
    "                    df_nmrl_on_lab_id_facility, df_nmrl\n",
    "                )\n",
    "                df_nmrl_on_lab_id.insert(0,\"Viral Load Province Name\",province_name)\n",
    "                df_nmrl_on_lab_id.insert(1,\"Viral Load District Name\",district_name)\n",
    "                df_nmrl_on_lab_id.insert(2,\"Viral Load Facility Name\",facility_name)\n",
    "                df_nmrl_on_lab_id.insert(3,\"Viral Load Facility ID\",facility_id)\n",
    "                df_nmrl_on_lab_id.insert(4,\"Viral Load DB Name\",facility)\n",
    "                global_df_vl_nmrl_lab = pd.concat([global_df_vl_nmrl_lab,df_nmrl_on_lab_id])\n",
    "                print(\"nmrl new size \", df_nmrl.shape)\n",
    "\n",
    "            '''ag. clean art numbers'''\n",
    "            # Clean art numbers...............................................................................\n",
    "            print(\"    >>> cleaning art number\")\n",
    "            df_art = df_art.fillna('empty string')\n",
    "            df_art = df_art.replace('', 'empty string')\n",
    "            df_art.loc[:,'clean_art_number'] = df_art[\"Art Number\"].astype(str).str.replace('\\W', '', regex=True).apply(lambda x: x.lower().replace(' ',''))\n",
    "\n",
    "            '''ah. merging nmrl data on art number'''\n",
    "            exact_matches = {}\n",
    "            print(\"   Total positive people >>>\", df_positives.shape[0])\n",
    "            print(\"   >>> matching on art\")\n",
    "            df_match_art, df_nmrl = cbs_extraction.match_on_art_number( df_art , df_nmrl)\n",
    "            global_df_vl_nmrl_art = pd.concat([global_df_vl_nmrl_art,df_match_art])\n",
    "\n",
    "            print(\"    >>> NMRL new size \", df_nmrl.shape)\n",
    "            \n",
    "            '''ah. merging nmrl data on demographics'''\n",
    "            print(\"    >>> exact matches between NMRL Demographics and EHR Demographics\")\n",
    "            ''' Find exact matches between NMRL Facility/Lab/Demographics and EHR Laboratory Investigation ID '''\n",
    "            \n",
    "            # df_match_demos['Birthdate'] = cbs_extraction.clean_dates(df_nmrl_on_lab_id, 'DOB')\n",
    "            df_demographics['Birthdate_cleaned'] = cbs_extraction.clean_dates(df_demographics, 'Birthdate')\n",
    "            df_nmrl['Birthdate_cleaned'] = df_nmrl['Birthdate']\n",
    "            df_match_demos, df_nmrl  = cbs_extraction.match_on_demos(\n",
    "                                    df_demographics, \n",
    "                                    df_nmrl)\n",
    "            \n",
    "            print(\"    >>> NMRL new size \", df_nmrl.shape)\n",
    "            \n",
    "            global_df_vl_nmrl_demos = pd.concat([global_df_vl_nmrl_demos ,df_match_demos])  \n",
    "\n",
    "            # Mother baby pair.......................................................................................\n",
    "            print(\"   >>> Extracting babies who tested positive\")\n",
    "            positive_babies_mother_sql = \"\"\"\n",
    "                                    WITH ranked_messages AS (\n",
    "                                        SELECT ps.person_id as 'Baby Person id',\n",
    "                                            ps.firstname as 'Baby FirstName',\n",
    "                                            ps.lastname as 'Baby LastName',\n",
    "                                            ps.birthdate as 'Baby Birthdate',\n",
    "                                            ps.sex as 'Baby Sex',\n",
    "                                            pi.date as 'Baby Date of HIV Test', \n",
    "                                            pi.result as 'HIV Result of Baby',\n",
    "                                            pi.test as 'HIV test for Baby',\n",
    "                                            pm.person_id as 'Mother Person id' ,\n",
    "                                            pm.firstname as 'Mother FirstName',\n",
    "                                            pm.lastname as 'Mother LastName',\n",
    "                                            pm.birthdate as 'Mother Birthdate',\n",
    "                                            pm.sex as 'Mother Sex',\n",
    "                                            ROW_NUMBER() OVER (PARTITION BY ps.person_id ORDER BY pi.date ASC) as ranking\n",
    "                                            FROM client.person ps\n",
    "                                            left join consultation.person_investigation pi                                        \n",
    "                                            on ps.person_id = pi.person_id\n",
    "                                            left join consultation.pregnancy_live_birth pb                                       \n",
    "                                            on pi.person_id = pb.person_id\n",
    "                                            left join consultation.pregnancy pg                                      \n",
    "                                            on pb.pregnancy_id = pg.pregnancy_id\n",
    "                                            left join client.person pm                                   \n",
    "                                            on pg.person_id = pm.person_id\n",
    "                                            where pi.result = 'positive'\n",
    "                                            and (pi.test = 'hiv' or pi.test = 'HIV DNA PCR')\n",
    "                                            AND DATEDIFF(pi.date, ps.birthdate) <= 730\n",
    "                                                )\n",
    "                                                SELECT * FROM ranked_messages where ranking = 1\n",
    "                                    \"\"\"\n",
    "            positive_babies_mother = cbs_extraction.extracting_data(positive_babies_mother_sql, connection)\n",
    "            status_of_mother = cbs_extraction.extracting_data(\"\"\"\n",
    "                                            select person_id as 'Mother Person id', date as 'Mother Date of HIV Test ',\n",
    "                                            result as 'HIV Result of Mother' from consultation.person_investigation\n",
    "                                            where test in ('HIV','HIV DNA PCR')\n",
    "                                            \"\"\",connection)\n",
    "            status_of_mother_art = cbs_extraction.extracting_data(\"\"\"\n",
    "                                            select person_id as 'Mother Person id', date_of_hiv_test as 'Mother Date of HIV Test (ART)',\n",
    "                                            'Positive' as 'HIV Result of Mother (ART)' from consultation.art\n",
    "                                            \"\"\",connection)\n",
    "            merged_df = positive_babies_mother.merge(status_of_mother, on='Mother Person id', how='left')\n",
    "            merged_df = merged_df.merge(status_of_mother_art, on='Mother Person id', how='left')\n",
    "            merged_df[\"Facility id\"] = facility_id\n",
    "            merged_df[\"Facility Name\"] = facility\n",
    "            merged_df.drop('ranking', axis=1, inplace=True)\n",
    "            global_baby_to_mother = pd.concat([global_baby_to_mother,merged_df])\n",
    "\n",
    "            # a. Method 1: Link babies who are less than or equal to 24 months old and HIV positive with their mothers.\n",
    "            # and Method 1 b: Link the remaining babies to their mothers based on their relationship.\n",
    "            positive_babies_mother_relation_sql = \"\"\"\n",
    "                                    WITH ranked_messages AS (\n",
    "                                        SELECT ps.person_id as 'Baby Person id',\n",
    "                                            ps.firstname as 'Baby FirstName',\n",
    "                                            ps.lastname as 'Baby LastName',\n",
    "                                            ps.birthdate as 'Baby Birthdate',\n",
    "                                            ps.sex as 'Baby Sex',\n",
    "                                            pi.date as 'Baby Date of HIV Test', \n",
    "                                            pi.result as 'HIV Result of Baby',\n",
    "                                            pi.test as 'HIV test for Baby',\n",
    "                                            pm.person_id as 'Mother Person id' ,\n",
    "                                            pm.firstname as 'Mother FirstName',\n",
    "                                            pm.lastname as 'Mother LastName',\n",
    "                                            pm.birthdate as 'Mother Birthdate',\n",
    "                                            pm.sex as 'Mother Sex',\n",
    "                                            ROW_NUMBER() OVER (PARTITION BY ps.person_id ORDER BY pi.date ASC) as ranking\n",
    "                                            FROM client.person ps\n",
    "                                            left join consultation.person_investigation pi                                        \n",
    "                                            on ps.person_id = pi.person_id\n",
    "                                            left join client.relationship rs                                      \n",
    "                                            on pi.person_id = rs.member_id\n",
    "                                            left join client.person pm                                   \n",
    "                                            on rs.person_id = pm.person_id\n",
    "                                            where pi.result = 'positive' and rs.relation = 'CHILD'\n",
    "                                            and (pi.test = 'hiv' or pi.test = 'HIV DNA PCR')\n",
    "                                            AND DATEDIFF(pi.date, ps.birthdate) <= 730\n",
    "                                                )\n",
    "                                                SELECT * FROM ranked_messages where ranking = 1\n",
    "                                    \"\"\"\n",
    "            positive_babies_mother_relation = cbs_extraction.extracting_data(positive_babies_mother_relation_sql, connection)\n",
    "            positive_babies_mother_relation[\"Facility id\"] = facility_id\n",
    "            positive_babies_mother_relation[\"Facility Name\"] = facility\n",
    "            positive_babies_mother_relation.drop('ranking', axis=1, inplace=True)\n",
    "            # global_babies_mother_relation = pd.concat([global_babies_mother_relation,positive_babies_mother_relation])\n",
    "            \n",
    "            #Method 2:\n",
    "            # 1. Extract demographics of all females who test positive for HIV and (pregnant or breast feeding).\n",
    "            # 2a. Link the mothers to their babies if the mothers delivered at the same facility where they tested positive.\n",
    "            positive_mothers_preg_or_lactating_sql = \"\"\"\n",
    "                                    WITH ranked_messages AS (\n",
    "                                        SELECT ps.person_id as 'Mother Person id',\n",
    "                                            ps.firstname as 'Mother FirstName',\n",
    "                                            ps.lastname as 'Mother LastName',\n",
    "                                            ps.birthdate as 'Mother Birthdate',\n",
    "                                            ps.sex as 'Mother Sex',\n",
    "                                            h.time as 'Mother Date of HIV Test', \n",
    "                                            pi.result as 'HIV Result of Mother',\n",
    "                                            ps2.firstname as 'Baby FirstName',\n",
    "                                            ps2.lastname as 'Baby LastName',\n",
    "                                            ps2.birthdate as 'Baby Birthdate',\n",
    "                                            ps2.sex as 'Baby Sex', \n",
    "                                            'Alive' as 'Delivery Status',\n",
    "                                            plb.delivery_mode as 'Delivery mode',\n",
    "                                            plb.weight as 'Weight of Baby',\n",
    "                                            ROW_NUMBER() OVER (PARTITION BY ps.person_id ORDER BY pi.date ASC) as ranking\n",
    "                                            FROM consultation.hts h\n",
    "                                            left join consultation.person_investigation pi\n",
    "                                            on h.laboratory_investigation_id = pi.person_investigation_id\n",
    "                                            left join client.person ps\n",
    "                                            on pi.person_id = ps.person_id\n",
    "                                            left join consultation.pregnancy pgg\n",
    "                                            on pi.person_id = pgg.person_id\n",
    "                                            left join consultation.pregnancy_live_birth plb\n",
    "                                            on pgg.pregnancy_id = plb.pregnancy_id\n",
    "                                            left join client.person ps2\n",
    "                                            on plb.person_id = ps2.person_id\n",
    "                                            where pi.result = 'positive'\n",
    "                                            and (pi.test = 'hiv' or pi.test = 'HIV DNA PCR')\n",
    "                                            AND (pregnant = 1 or lactating = 1)\n",
    "                                            )\n",
    "                                                SELECT * FROM ranked_messages where ranking = 1\n",
    "                                    \"\"\"\n",
    "            positive_mothers_live_birth = cbs_extraction.extracting_data(positive_mothers_preg_or_lactating_sql, connection)\n",
    "            positive_mothers_live_birth.drop('ranking', axis=1, inplace=True)\n",
    "            positive_mothers_live_birth['Delivery Status'] = positive_mothers_live_birth.apply(lambda row: '' if pd.isnull(row['Baby Sex']) else row['Delivery Status'], axis=1)\n",
    "\n",
    "            # Still Birth\n",
    "            positive_mothers_preg_or_lactating_sql = \"\"\"\n",
    "                                    WITH ranked_messages AS (\n",
    "                                        SELECT ps.person_id as 'Mother Person id',\n",
    "                                            ps.firstname as 'Mother FirstName',\n",
    "                                            ps.lastname as 'Mother LastName',\n",
    "                                            ps.birthdate as 'Mother Birthdate',\n",
    "                                            ps.sex as 'Mother Sex',\n",
    "                                            h.time as 'Mother Date of HIV Test', \n",
    "                                            pi.result as 'HIV Result of Mother',\n",
    "                                            plb.time as 'Still Birth date',\n",
    "                                            plb.sex as 'Baby Sex', \n",
    "                                            'Still Birth' as 'Delivery Status',\n",
    "                                            plb.delivery_mode as 'Delivery mode',\n",
    "                                            plb.weight as 'Weight of Baby',\n",
    "                                            ROW_NUMBER() OVER (PARTITION BY ps.person_id ORDER BY pi.date ASC) as ranking\n",
    "                                            FROM consultation.hts h\n",
    "                                            left join consultation.person_investigation pi\n",
    "                                            on h.laboratory_investigation_id = pi.person_investigation_id\n",
    "                                            left join client.person ps\n",
    "                                            on pi.person_id = ps.person_id\n",
    "                                            left join consultation.pregnancy pgg\n",
    "                                            on pi.person_id = pgg.person_id\n",
    "                                            left join consultation.pregnancy_still_birth plb\n",
    "                                            on pgg.pregnancy_id = plb.pregnancy_id\n",
    "                                            where pi.result = 'positive'\n",
    "                                            and (pi.test = 'hiv' or pi.test = 'HIV DNA PCR')\n",
    "                                            AND (pregnant = 1 or lactating = 1)\n",
    "                                            )\n",
    "                                            SELECT * FROM ranked_messages where ranking = 1\n",
    "                                    \"\"\"\n",
    "            positive_mothers_still_birth = cbs_extraction.extracting_data(positive_mothers_preg_or_lactating_sql, connection)\n",
    "            positive_mothers_still_birth['Delivery Status'] = positive_mothers_still_birth.apply(lambda row: '' if pd.isnull(row['Baby Sex']) else row['Delivery Status'], axis=1)\n",
    "\n",
    "            df_positive_mother =  pd.concat([positive_mothers_live_birth,positive_mothers_still_birth])\n",
    "            df_positive_mother[\"Facility id\"] = facility_id\n",
    "            df_positive_mother[\"Facility Name\"] = facility\n",
    "            \n",
    "            df_positive_mother.drop_duplicates(keep='first',inplace=True)\n",
    "            df_positive_mother.drop('ranking', axis=1, inplace=True)\n",
    "            # global_positive_mother = pd.concat([global_positive_mother,df_positive_mother])\n",
    "\n",
    "            #Method 2:\n",
    "            # 2b. Link the mothers to their children through their relationship.\n",
    "            #    - Link the \"person\" table with the \"relationship\" table based on the \"person_id\" field.\n",
    "            positive_mothers_relationship_sql = \"\"\"\n",
    "                                    WITH ranked_messages AS (\n",
    "                                        SELECT ps.person_id as 'Mother Person id',\n",
    "                                            ps.firstname as 'Mother FirstName',\n",
    "                                            ps.lastname as 'Mother LastName',\n",
    "                                            ps.birthdate as 'Mother Birthdate',\n",
    "                                            ps.sex as 'Mother Sex',\n",
    "                                            h.time as 'Mother Date of HIV Test', \n",
    "                                            pi.result as 'HIV Result of Mother',\n",
    "                                            ps2.person_id as 'Baby Person id',\n",
    "                                            ps2.firstname as 'Baby FirstName',\n",
    "                                            ps2.lastname as 'Baby LastName',\n",
    "                                            ps2.birthdate as 'Baby Birthdate',\n",
    "                                            ps2.sex as 'Baby Sex',\n",
    "                                            'MOTHER' as relation,\n",
    "                                            ROW_NUMBER() OVER (PARTITION BY ps.person_id ORDER BY pi.date ASC) as ranking\n",
    "                                            FROM consultation.hts h\n",
    "                                            left join consultation.person_investigation pi\n",
    "                                            on h.laboratory_investigation_id = pi.person_investigation_id\n",
    "                                            left join client.person ps\n",
    "                                            on pi.person_id = ps.person_id\n",
    "                                            left join client.relationship rs\n",
    "                                            on pi.person_id = rs.person_id\n",
    "                                            left join client.person ps2\n",
    "                                            on rs.member_id = ps2.person_id\n",
    "                                            where pi.result = 'positive'\n",
    "                                            and (pi.test = 'hiv' or pi.test = 'HIV DNA PCR')\n",
    "                                            AND (pregnant = 1 or lactating = 1)\n",
    "                                            and rs.relation = 'CHILD'\n",
    "                                            )\n",
    "                                                SELECT * FROM ranked_messages where ranking = 1\n",
    "                                    \"\"\"\n",
    "            positive_mothers_relation = cbs_extraction.extracting_data(positive_mothers_relationship_sql, connection)\n",
    "            \n",
    "            positive_mothers_relation.drop('ranking', axis=1, inplace=True)\n",
    "\n",
    "            babies_relationship_sql = \"\"\"\n",
    "                                    WITH ranked_messages AS (\n",
    "                                        SELECT ps.person_id as 'Baby Person id',\n",
    "                                            pi.date as 'Baby Date of HIV Test',\n",
    "                                            pi.result as 'HIV Result of Baby',\n",
    "                                            ROW_NUMBER() OVER (PARTITION BY ps.person_id ORDER BY pi.date ASC) as ranking\n",
    "                                            FROM consultation.person_investigation pi\n",
    "                                            left join client.person ps\n",
    "                                            on pi.person_id = ps.person_id\n",
    "                                            where test in ('hiv','HIV DNA PCR')\n",
    "                                            )\n",
    "                                                SELECT * FROM ranked_messages where ranking = 1\n",
    "                                    \"\"\"\n",
    "            baby_relation = cbs_extraction.extracting_data(babies_relationship_sql, connection)\n",
    "\n",
    "            positive_mothers_relation = pd.merge(positive_mothers_relation,baby_relation,on = ['Baby Person id'], how = 'left')\n",
    "            positive_mothers_relation[\"Facility id\"] = facility_id\n",
    "            positive_mothers_relation[\"Facility Name\"] = facility\n",
    "\n",
    "            # global_mothers_relation = pd.concat([global_mothers_relation,positive_mothers_relation])\n",
    "\n",
    "            mother_to_baby = pd.concat([df_positive_mother,positive_mothers_relation])\n",
    "\n",
    "            global_mother_to_baby = pd.concat([global_mother_to_baby,mother_to_baby])\n",
    "\n",
    "            if not df_match_art.empty:\n",
    "                df_match_art_len = len(set(df_match_art['person_id']))\n",
    "            else:\n",
    "                df_match_art_len = 0\n",
    "            \n",
    "            if not df_match_demos.empty:\n",
    "                df_match_demos_len = len(set(df_match_demos['person_id']))\n",
    "            else:\n",
    "                df_match_demos_len = 0\n",
    "            \n",
    "            if not df_nmrl_on_lab_id.empty:\n",
    "                df_nmrl_on_lab_id_len = len(set(df_nmrl_on_lab_id['person_id']))\n",
    "            else:\n",
    "                df_nmrl_on_lab_id_len = 0\n",
    "\n",
    "\n",
    "            df_stats = pd.DataFrame({\"Province\": [province_name],\n",
    "                                    \"District\": [district_name],\n",
    "                                    \"Facility\": [facility_name],\n",
    "                                    \"Facility id\": [facility_id],\n",
    "                                    \"Number of positive people\": df_positives.shape[0],\n",
    "                                    \"Number of matches found using ART\": df_match_art_len,\n",
    "                                    \"Number of matches found using demos\": df_match_demos_len,\n",
    "                                    \"Number of matches found using lab number\": df_nmrl_on_lab_id_len\n",
    "                                    })\n",
    "            global_df_stats = pd.concat([global_df_stats, df_stats])\n",
    "            \n",
    "            # save reduced nmrl file\n",
    "            df_nmrl.to_feather(\"nmrl_reduced.feather\")\n",
    "            df_nmrl.to_csv(\"nmrl_reduced.csv\")\n",
    "            # Save globals in case there is an error in extraction \n",
    "            # saved to csv for any validation to be done \n",
    "            print(\"  >>> Saving global dataframes to csv files\")\n",
    "            # if os.path.exists(\"global_df_vl_nmrl_art.csv\"):\n",
    "            #     df_stats.to_csv(\"Stats.csv\", mode = 'a', header = False, index= False)\n",
    "            #      global_df_vl_nmrl_lab.to_csv(\"global_vl_nmrl_lab.csv\", mode = 'a', header = False, index = False)\n",
    "            #     global_df_vl_nmrl_art.to_csv(\"global_df_vl_nmrl_art.csv\", mode = 'a', header = False, index = False)\n",
    "            #     global_df_vl_nmrl_demos.to_csv(\"global_df_vl_nmrl_demos.csv\", mode = 'a', header = False, index = False)\n",
    "            #     global_df_demographics.to_csv(\"Demographics.csv\",mode='a', header = False, index = False)\n",
    "            #     global_df_positives.to_csv(\"global_df_positives.csv\",mode='a', header = False, index = False)\n",
    "            #     global_df_hts_negative.to_csv(\"global_df_negative.csv\",mode='a', header = False, index = False)\n",
    "            #     global_df_cbs.to_csv(\"global_df_cbs.csv\",mode='a', header = False, index = False)\n",
    "            #     global_df_recency.to_csv(\"global_df_recency.csv\",mode='a', header = False, index = False)\n",
    "            #     global_df_art.to_csv(\"global_df_art.csv\",mode='a', header = False, index = False)\n",
    "            #     global_df_art_visit.to_csv(\"global_df_art_visit.csv\",mode='a', header = False, index = False)\n",
    "            #     global_df_art_current_status.to_csv(\"global_df_art_current_status.csv\",mode='a', header = False, index = False)\n",
    "            #     global_df_viral_load.to_csv(\"global_df_viral_load.csv\",mode='a', header = False, index = False)\n",
    "            #     global_df_cd4.to_csv(\"global_df_cd4.csv\",mode='a',  header = False, index = False)\n",
    "            #     global_df_tb.to_csv(\"global_df_tb.csv\",mode='a',  header = False, index = False)\n",
    "            #     global_df_tb_screening.to_csv(\"global_df_tb_screening.csv\",mode='a',  header = False, index = False)\n",
    "            #     global_df_transfer_out.to_csv(\"global_df_transfer_out.csv\",mode='a', header = False, index = False)\n",
    "            #     global_df_art_who_stage.to_csv(\"global_df_art_who_stage.csv\",mode='a', header = False, index = False)\n",
    "            #     global_df_laboratory_request.to_csv(\"global_df_laboratory_request.csv\",mode='a',  header = False, index = False)\n",
    "            #     global_mother_to_baby.to_csv(\"global_mother_to_baby.csv\",mode='a',  header = False, index = False)\n",
    "            #     global_baby_to_mother.to_csv(\"global_baby_to_mother.csv\",mode='a',  header = False, index = False)\n",
    "            \n",
    "            # else:\n",
    "            #     # Write data to the CSV file with headers\n",
    "            global_df_stats.to_csv(\"Stats.csv\", index= False)\n",
    "            global_df_vl_nmrl_lab.to_csv(\"global_vl_nmrl_lab.csv\", index = False)\n",
    "            global_df_vl_nmrl_art.to_csv(\"global_df_vl_nmrl_art.csv\", index = False)\n",
    "            global_df_vl_nmrl_demos.to_csv(\"global_df_vl_nmrl_demos.csv\", index = False)\n",
    "            global_df_demographics.to_csv(\"Demographics.csv\", index = False)\n",
    "            global_df_positives.to_csv(\"global_df_positives.csv\", index = False)\n",
    "            global_df_hts_negative.to_csv(\"global_df_negative.csv\", index = False)\n",
    "            global_df_cbs.to_csv(\"global_df_cbs.csv\",index = False)\n",
    "            global_df_recency.to_csv(\"global_df_recency.csv\", index = False)\n",
    "            global_df_art.to_csv(\"global_df_art.csv\", index = False)\n",
    "            global_df_art_visit.to_csv(\"global_df_art_visit.csv\", index = False)\n",
    "            global_df_art_current_status.to_csv(\"global_df_art_current_status.csv\", index = False)\n",
    "            global_df_viral_load.to_csv(\"global_df_viral_load.csv\", index = False)\n",
    "            global_df_cd4.to_csv(\"global_df_cd4.csv\" , index = False)\n",
    "            global_df_tb.to_csv(\"global_df_tb.csv\" , index = False)\n",
    "            global_df_tb_screening.to_csv(\"global_df_tb_screening.csv\", index = False)\n",
    "            global_df_transfer_out.to_csv(\"global_df_transfer_out.csv\", index = False)\n",
    "            global_df_art_who_stage.to_csv(\"global_df_art_who_stage.csv\", index = False)\n",
    "            global_df_laboratory_request.to_csv(\"global_df_laboratory_request.csv\", index = False)\n",
    "            global_mother_to_baby.to_csv(\"global_mother_to_baby.csv\", index = False)\n",
    "            global_baby_to_mother.to_csv(\"global_baby_to_mother.csv\", index = False)\n",
    "            global_df_art_current_status2.to_csv(\"global_df_art_current_status2.csv\", index = False)\n",
    "    \n",
    "            # Delete trimmed database\n",
    "            os.remove(trimmed_db)\n",
    "\n",
    "            if db_size_left <= 0:\n",
    "                db_size_left = 0 \n",
    "            print(f\"size left {round(db_size_left,2)} GB\")\n",
    "            cbs_extraction.log(processing_time,province_name,district_name,facility_name,facility_id,facility,\"\",\"\",db_size,\"\",\"Completed\",\"web\",\"\",\"\")\n",
    "         \n",
    "        except Exception as e:  # noqa: E722\n",
    "            print (e)\n",
    "            cbs_extraction.log(processing_time,\"\",\"\",\"\",\"\",facility,\"\",\"\",db_size,\"\",\"Failed : \"+ str(e),\"web\",\"\",\"\")\n",
    "            continue\n",
    "\n",
    "    print(\"  >>> Saving global dataframes to feather files\")\n",
    "  \n",
    "\n",
    "\n",
    "    # global_mother_to_baby = pd.read_csv(\"global_mother_to_baby.csv\", index = False)\n",
    "    # global_mother_to_baby.to_feather(\"global_mother_to_baby.feather\")\n",
    "\n",
    "    # global_baby_to_mother = pd.read_csv(\"global_baby_to_mother.csv\")\n",
    "    # global_df_vl_nmrl_lab.to_feather(\"global_baby_to_mother.feather\")\n",
    "\n",
    "    # global_df_vl_nmrl_lab = pd.read_csv('global_vl_nmrl_lab.csv')\n",
    "    # global_df_vl_nmrl_lab.to_feather(\"global_vl_nmrl_lab.feather\")\n",
    "\n",
    "    # global_df_vl_nmrl_art = pd.read_csv('global_df_vl_nmrl_art.csv')\n",
    "    # global_df_vl_nmrl_art.to_feather(\"global_df_vl_nmrl_art.feather\")\n",
    "\n",
    "    # global_df_vl_nmrl_demos = pd.read_csv('global_df_vl_nmrl_demos.csv')\n",
    "    # global_df_vl_nmrl_demos.to_feather(\"global_df_vl_nmrl_demos.feather\")\n",
    "\n",
    "    # global_df_demographics = pd.read_csv('Demographics.csv')\n",
    "    # global_df_demographics.to_feather(\"Demographics.feather\")\n",
    "\n",
    "    # global_df_positives = pd.read_csv('global_df_positives.csv')\n",
    "    # global_df_positives.to_feather(\"global_df_positives.feather\")\n",
    "\n",
    "    # global_df_hts_negative = pd.read_csv('global_df_negative.csv')\n",
    "    # global_df_hts_negative.to_feather(\"global_df_negative.feather\")\n",
    "\n",
    "    # global_df_cbs = pd.read_csv('global_df_cbs.csv')\n",
    "    # global_df_cbs.to_feather(\"global_df_cbs.feather\")\n",
    "\n",
    "    # global_df_recency = pd.read_csv('global_df_recency.csv')\n",
    "    # global_df_recency.to_feather(\"global_df_recency.feather\")\n",
    "\n",
    "    # global_df_art = pd.read_csv('global_df_art.csv')\n",
    "    # global_df_art.to_feather(\"global_df_art.feather\")\n",
    "\n",
    "    # global_df_art_visit = pd.read_csv('global_df_art_visit.csv')\n",
    "    # global_df_art_visit.to_feather(\"global_df_art_visit.feather\")\n",
    "\n",
    "    # global_df_art_current_status = pd.read_csv('global_df_art_current_status.csv')\n",
    "    # global_df_art_current_status.to_feather(\"global_df_art_current_status.feather\")\n",
    "\n",
    "    # global_df_viral_load = pd.read_csv('global_df_viral_load.csv')\n",
    "    # global_df_viral_load.to_feather(\"global_df_viral_load.feather\")\n",
    "\n",
    "    # global_df_cd4 = pd.read_csv('global_df_cd4.csv')\n",
    "    # global_df_cd4.to_feather(\"global_df_cd4.feather\")\n",
    "\n",
    "    # global_df_tb = pd.read_csv('global_df_tb.csv')\n",
    "    # global_df_tb.to_feather(\"global_df_tb.feather\")\n",
    "\n",
    "    # global_df_tb_screening = pd.read_csv('global_df_tb_screening.csv')\n",
    "    # global_df_tb_screening.to_feather(\"global_df_tb_screening.feather\")\n",
    "\n",
    "    # global_df_transfer_out = pd.read_csv('global_df_transfer_out.csv')\n",
    "    # global_df_transfer_out.to_feather(\"global_df_transfer_out.feather\")\n",
    "\n",
    "    # global_df_art_who_stage = pd.read_csv('global_df_art_who_stage.csv')\n",
    "    # global_df_art_who_stage.to_feather(\"global_df_art_who_stage.feather\")\n",
    "\n",
    "    # global_df_laboratory_request = pd.read_csv('global_df_laboratory_request.csv')\n",
    "    # global_df_laboratory_request.to_feather(\"global_df_laboratory_request.feather\")\n",
    "\n",
    "    # df_ehr = pd.read_feather(\"Demographics.feather\")\n",
    "    # df_nmrl = pd.read_feather('nmrl_reduced.feather')\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fomartting "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
