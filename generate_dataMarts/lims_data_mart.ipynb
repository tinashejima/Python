{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d89ebc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Hive and Local databases...\n",
      "Error creating Local Database connection: connection to server at \"localhost\" (127.0.0.1), port 3307 failed: Connection refused\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "Failed to establish database connections.\n",
      "Please check your connection parameters and network connectivity.\n",
      "Closing database connections...\n"
     ]
    }
   ],
   "source": [
    "from pyhive import hive\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import time\n",
    "import hashlib\n",
    "import thrift_sasl\n",
    "import math\n",
    "\n",
    "class HiveConnection:\n",
    "    def __init__(self, host, port, username, password, database, auth_mode):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.database = database\n",
    "        self.auth_mode = auth_mode\n",
    "        self.connection = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Creates a connection to the Hive database.\"\"\"\n",
    "        try:\n",
    "            self.connection = hive.Connection(\n",
    "                host=self.host,\n",
    "                port=self.port,\n",
    "                username=self.username,\n",
    "                password=self.password,\n",
    "                database=self.database,\n",
    "                auth=self.auth_mode\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating connection: {e}\")\n",
    "            return False\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the connection to the Hive database.\"\"\"\n",
    "        if self.connection:\n",
    "            try:\n",
    "                self.connection.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing connection: {e}\")\n",
    "\n",
    "    def execute_query(self, query):\n",
    "        \"\"\"Executes the given query and returns the results.\"\"\"\n",
    "        try:\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            results = cursor.fetchall()\n",
    "            return results, columns\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing query: {e}\")\n",
    "            return None, None\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "\n",
    "class LocalConnection:\n",
    "    def __init__(self, dbname, user, password, host, port):\n",
    "        self.dbname = dbname\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.connection = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Creates a connection to the Local database.\"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(\n",
    "                dbname=self.dbname,\n",
    "                user=self.user,\n",
    "                password=self.password,\n",
    "                host=self.host,\n",
    "                port=self.port\n",
    "            )\n",
    "            self.connection.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "            self.cursor = self.connection.cursor()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Local Database connection: {e}\")\n",
    "            return False\n",
    "\n",
    "    def execute_query(self, query, params=None):\n",
    "        \"\"\"Executes a query with optional parameters for SELECT, INSERT, or UPDATE operations.\"\"\"\n",
    "        try:\n",
    "            # Execute the query with parameters\n",
    "            self.cursor.execute(query, params)\n",
    "\n",
    "            # Check if the query is a SELECT statement\n",
    "            if query.strip().upper().startswith(\"SELECT\"):\n",
    "                # Fetch and return results for SELECT queries\n",
    "                return self.cursor.fetchall()\n",
    "            else:\n",
    "                # Commit transaction for non-SELECT queries (e.g., INSERT, UPDATE)\n",
    "                self.connection.commit()\n",
    "                return True  # Return True to indicate success for INSERT/UPDATE queries\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing LocalDB query: {e}\")\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the Local database connection.\"\"\"\n",
    "        if self.connection:\n",
    "            try:\n",
    "                self.connection.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing Local database connection: {e}\")\n",
    "\n",
    "    def create_lab_datamart_table(self):\n",
    "        \"\"\"Creates the lab data mart table if it doesn't exist.\"\"\"\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS marts.dm_lab (\n",
    "            lab_request_number VARCHAR(100),\n",
    "            patient_id VARCHAR(100),\n",
    "            task_id VARCHAR(100),\n",
    "            encounter_id VARCHAR(100),\n",
    "            servicerequest_id VARCHAR(100),\n",
    "            specimen_id VARCHAR(100),\n",
    "            task_status VARCHAR(50),\n",
    "            status_reason_display TEXT,\n",
    "            task_execution_start_date TIMESTAMP,\n",
    "            encounter_period_start TIMESTAMP,\n",
    "            sample_code VARCHAR(50),\n",
    "            specimen_last_updated TIMESTAMP,\n",
    "            diagnosticreport_last_updated TIMESTAMP,\n",
    "            encounter_last_updated TIMESTAMP,\n",
    "            observations_last_updated TIMESTAMP,\n",
    "            servicerequest_last_updated TIMESTAMP,\n",
    "            task_last_updated TIMESTAMP,\n",
    "            task_output_reference TEXT,\n",
    "            task_output_type VARCHAR(100),\n",
    "            test_type VARCHAR(100),\n",
    "            test_code VARCHAR(50),\n",
    "            service_request_category VARCHAR(100),\n",
    "            service_request_authored_on TIMESTAMP,\n",
    "            task_authored_on TIMESTAMP,\n",
    "            service_request_reason TEXT,\n",
    "            sample_type VARCHAR(100),\n",
    "            date_sample_taken TIMESTAMP,\n",
    "            diagnostic_report_date_issued TIMESTAMP,\n",
    "            diagnostic_report_effective_date TIMESTAMP,\n",
    "            result TEXT,\n",
    "            result_interpretation TEXT,\n",
    "            dedupe_id VARCHAR(100),\n",
    "            gender VARCHAR(10),\n",
    "            birth_date DATE,\n",
    "            patient_managing_organization_id VARCHAR(100),\n",
    "            patient_managing_organization VARCHAR(200),\n",
    "            lab_id VARCHAR(100),\n",
    "            encounter_facility VARCHAR(200),\n",
    "            encounter_facility_id VARCHAR(100),\n",
    "            dw_date_created TIMESTAMP,\n",
    "            dm_date_created TIMESTAMP DEFAULT NOW(),\n",
    "            record_hash VARCHAR(64) UNIQUE,\n",
    "            PRIMARY KEY (record_hash)\n",
    "        );\n",
    "        \n",
    "        -- Create indexes for better performance\n",
    "        CREATE INDEX IF NOT EXISTS idx_dm_lab_patient_id ON marts.dm_lab(patient_id);\n",
    "        CREATE INDEX IF NOT EXISTS idx_dm_lab_encounter_id ON marts.dm_lab(encounter_id);\n",
    "        CREATE INDEX IF NOT EXISTS idx_dm_lab_task_id ON marts.dm_lab(task_id);\n",
    "        CREATE INDEX IF NOT EXISTS idx_dm_lab_last_updated ON marts.dm_lab(dw_date_created);\n",
    "        CREATE INDEX IF NOT EXISTS idx_dm_lab_lab_request_number ON marts.dm_lab(lab_request_number);\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.cursor.execute(create_table_query)\n",
    "            self.connection.commit()\n",
    "            print(\"Lab data mart table created successfully.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating lab data mart table: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "class LabDataFetcher:\n",
    "    def __init__(self, hive_conn, pg_conn, batch_size=50000, polling_interval=600):\n",
    "        self.hive_conn = hive_conn\n",
    "        self.pg_conn = pg_conn\n",
    "        self.batch_size = batch_size\n",
    "        self.polling_interval = polling_interval\n",
    "        self.last_processed_value = '1900-01-01 00:00:00'\n",
    "\n",
    "    def check_if_snapshot_done(self):\n",
    "        \"\"\"Check the latest processed timestamp from the data mart.\"\"\"\n",
    "        query = \"SELECT MAX(dw_date_created) FROM marts.dm_lab\"\n",
    "        result = self.pg_conn.execute_query(query)\n",
    "        if result and result[0][0]:\n",
    "            return result[0][0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def hash_record(self, record):\n",
    "        \"\"\"\n",
    "        Hash the combined values of a record using SHA-256.\n",
    "        If a value is None or NaN, replace it with an empty string before hashing.\n",
    "        \"\"\"\n",
    "        combined = ''.join([str(value) if not pd.isna(value) and value is not None else '' for value in record])\n",
    "        return hashlib.sha256(combined.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def fetch_data_in_batches(self):\n",
    "        \"\"\"Fetch data from Hive in batches and process them.\"\"\"\n",
    "        snapshot_date = self.check_if_snapshot_done()\n",
    "        if snapshot_date:\n",
    "            self.last_processed_value = snapshot_date\n",
    "            print(f\">>> Detected Snapshot done: {self.last_processed_value}\")\n",
    "        else:\n",
    "            print(\">>> Initial snapshot\")\n",
    "\n",
    "        while True:\n",
    "            print(f\"Fetching batch data where last_updated > {self.last_processed_value}\")\n",
    "            \n",
    "            # Update this query with your actual view name\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                lab_request_number, patient_id, task_id, encounter_id, last_updated,\n",
    "                serviceRequest_id, specimen_id, task_status, status_reason_display,\n",
    "                task_execution_start_date, encounter_period_start, sample_code,\n",
    "                specimen_last_updated, diagnosticreport_last_updated, encounter_last_updated,\n",
    "                observations_last_updated, servicerequest_last_updated, task_last_updated,\n",
    "                task_output_reference, task_output_type, test_type, test_code,\n",
    "                service_request_category, service_request_authored_on, task_authored_on,\n",
    "                service_request_reason, sample_type, date_sample_taken,\n",
    "                diagnostic_report_date_issued, diagnostic_report_effective_date,\n",
    "                result, result_interpretation, dedupe_id, gender, birth_date,\n",
    "                patient_managing_organization_id, patient_managing_organization,\n",
    "                lab_id, encounter_facility, encounter_facility_id\n",
    "            FROM your_lab_view_name \n",
    "            WHERE last_updated > '{self.last_processed_value}' \n",
    "            ORDER BY last_updated ASC \n",
    "            LIMIT {self.batch_size}\n",
    "            \"\"\"\n",
    "            \n",
    "            batch_data, columns = self.hive_conn.execute_query(query)\n",
    "\n",
    "            if not batch_data:  # No more data\n",
    "                print(\"No more data to fetch. Ending batch fetching.\")\n",
    "                break\n",
    "\n",
    "            df = pd.DataFrame(batch_data, columns=columns)\n",
    "            print(f\"Fetched batch data: {df.shape}\")\n",
    "            \n",
    "            # Add hash column to the DataFrame\n",
    "            df['record_hash'] = df.apply(lambda row: self.hash_record(row), axis=1)\n",
    "            self.process_data(df)\n",
    "\n",
    "            # Update the last_processed_value to the latest timestamp in the batch\n",
    "            self.last_processed_value = df['last_updated'].max()\n",
    "            print(f\"Updated last_processed_value to: {self.last_processed_value}\")\n",
    "\n",
    "    def process_data(self, df):\n",
    "        \"\"\"Process each record - either insert new or update existing.\"\"\"\n",
    "        for _, row in df.iterrows():\n",
    "            record_hash = row['record_hash']\n",
    "            \n",
    "            # Check if record already exists\n",
    "            check_query = \"SELECT 1 FROM marts.dm_lab WHERE record_hash = %s\"\n",
    "            existing_record = self.pg_conn.execute_query(check_query, (record_hash,))\n",
    "            \n",
    "            if existing_record:\n",
    "                # Update existing record\n",
    "                print(f\"Updating record for record_hash: {record_hash}\")\n",
    "                update_query = \"\"\"\n",
    "                UPDATE marts.dm_lab\n",
    "                SET \n",
    "                    lab_request_number = %s, patient_id = %s, task_id = %s, encounter_id = %s,\n",
    "                    servicerequest_id = %s, specimen_id = %s, task_status = %s, \n",
    "                    status_reason_display = %s, task_execution_start_date = %s, \n",
    "                    encounter_period_start = %s, sample_code = %s, specimen_last_updated = %s,\n",
    "                    diagnosticreport_last_updated = %s, encounter_last_updated = %s,\n",
    "                    observations_last_updated = %s, servicerequest_last_updated = %s,\n",
    "                    task_last_updated = %s, task_output_reference = %s, task_output_type = %s,\n",
    "                    test_type = %s, test_code = %s, service_request_category = %s,\n",
    "                    service_request_authored_on = %s, task_authored_on = %s,\n",
    "                    service_request_reason = %s, sample_type = %s, date_sample_taken = %s,\n",
    "                    diagnostic_report_date_issued = %s, diagnostic_report_effective_date = %s,\n",
    "                    result = %s, result_interpretation = %s, dedupe_id = %s, gender = %s,\n",
    "                    birth_date = %s, patient_managing_organization_id = %s,\n",
    "                    patient_managing_organization = %s, lab_id = %s, encounter_facility = %s,\n",
    "                    encounter_facility_id = %s, dw_date_created = %s, dm_date_created = NOW()\n",
    "                WHERE record_hash = %s\n",
    "                \"\"\"\n",
    "                \n",
    "                self.pg_conn.execute_query(update_query, (\n",
    "                    row['lab_request_number'], row['patient_id'], row['task_id'], row['encounter_id'],\n",
    "                    row['serviceRequest_id'], row['specimen_id'], row['task_status'], \n",
    "                    row['status_reason_display'], row['task_execution_start_date'],\n",
    "                    row['encounter_period_start'], row['sample_code'], row['specimen_last_updated'],\n",
    "                    row['diagnosticreport_last_updated'], row['encounter_last_updated'],\n",
    "                    row['observations_last_updated'], row['servicerequest_last_updated'],\n",
    "                    row['task_last_updated'], row['task_output_reference'], row['task_output_type'],\n",
    "                    row['test_type'], row['test_code'], row['service_request_category'],\n",
    "                    row['service_request_authored_on'], row['task_authored_on'],\n",
    "                    row['service_request_reason'], row['sample_type'], row['date_sample_taken'],\n",
    "                    row['diagnostic_report_date_issued'], row['diagnostic_report_effective_date'],\n",
    "                    row['result'], row['result_interpretation'], row['dedupe_id'], row['gender'],\n",
    "                    row['birth_date'], row['patient_managing_organization_id'],\n",
    "                    row['patient_managing_organization'], row['lab_id'], row['encounter_facility'],\n",
    "                    row['encounter_facility_id'], row['last_updated'], record_hash\n",
    "                ))\n",
    "            else:\n",
    "                # Insert new record\n",
    "                print(f\"Inserting new record for record_hash: {record_hash}\")\n",
    "                insert_query = \"\"\"\n",
    "                INSERT INTO marts.dm_lab (\n",
    "                    lab_request_number, patient_id, task_id, encounter_id, servicerequest_id,\n",
    "                    specimen_id, task_status, status_reason_display, task_execution_start_date,\n",
    "                    encounter_period_start, sample_code, specimen_last_updated,\n",
    "                    diagnosticreport_last_updated, encounter_last_updated, observations_last_updated,\n",
    "                    servicerequest_last_updated, task_last_updated, task_output_reference,\n",
    "                    task_output_type, test_type, test_code, service_request_category,\n",
    "                    service_request_authored_on, task_authored_on, service_request_reason,\n",
    "                    sample_type, date_sample_taken, diagnostic_report_date_issued,\n",
    "                    diagnostic_report_effective_date, result, result_interpretation,\n",
    "                    dedupe_id, gender, birth_date, patient_managing_organization_id,\n",
    "                    patient_managing_organization, lab_id, encounter_facility,\n",
    "                    encounter_facility_id, dw_date_created, dm_date_created, record_hash\n",
    "                ) VALUES (\n",
    "                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), %s\n",
    "                )\n",
    "                \"\"\"\n",
    "                \n",
    "                self.pg_conn.execute_query(insert_query, (\n",
    "                    row['lab_request_number'], row['patient_id'], row['task_id'], row['encounter_id'],\n",
    "                    row['serviceRequest_id'], row['specimen_id'], row['task_status'], \n",
    "                    row['status_reason_display'], row['task_execution_start_date'],\n",
    "                    row['encounter_period_start'], row['sample_code'], row['specimen_last_updated'],\n",
    "                    row['diagnosticreport_last_updated'], row['encounter_last_updated'],\n",
    "                    row['observations_last_updated'], row['servicerequest_last_updated'],\n",
    "                    row['task_last_updated'], row['task_output_reference'], row['task_output_type'],\n",
    "                    row['test_type'], row['test_code'], row['service_request_category'],\n",
    "                    row['service_request_authored_on'], row['task_authored_on'],\n",
    "                    row['service_request_reason'], row['sample_type'], row['date_sample_taken'],\n",
    "                    row['diagnostic_report_date_issued'], row['diagnostic_report_effective_date'],\n",
    "                    row['result'], row['result_interpretation'], row['dedupe_id'], row['gender'],\n",
    "                    row['birth_date'], row['patient_managing_organization_id'],\n",
    "                    row['patient_managing_organization'], row['lab_id'], row['encounter_facility'],\n",
    "                    row['encounter_facility_id'], row['last_updated'], record_hash\n",
    "                ))\n",
    "\n",
    "    def start_polling(self):\n",
    "        \"\"\"Starts polling to fetch data every polling_interval seconds.\"\"\"\n",
    "        while True:\n",
    "            print(\"Starting lab data fetch cycle.\")\n",
    "            self.fetch_data_in_batches()\n",
    "            print(f\"Waiting for {self.polling_interval} seconds before next fetch.\")\n",
    "            time.sleep(self.polling_interval)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize Hive connection\n",
    "    hive_conn = HiveConnection(\n",
    "        host=\"197.221.242.150\",\n",
    "        port=17251,\n",
    "        username=\"tjima\",\n",
    "        password=\"vHYWzTVyygV4Q8tq\",\n",
    "        database=\"default\",\n",
    "        auth_mode=\"LDAP\"\n",
    "    )\n",
    "\n",
    "    # Initialize LocalDB connection\n",
    "    pg_conn = LocalConnection(\n",
    "        dbname=\"health_db\",\n",
    "        user=\"tynash\",\n",
    "        password=\"password\",\n",
    "        host=\"localhost\",\n",
    "        port=3307\n",
    "    )\n",
    "\n",
    "    print(\"Connecting to Hive and Local databases...\")\n",
    "    \n",
    "    if hive_conn.connect() and pg_conn.connect():\n",
    "        print(\"Successfully connected to both databases.\")\n",
    "        \n",
    "        # Create the lab data mart table\n",
    "        print(\"Creating lab data mart table...\")\n",
    "        pg_conn.create_lab_datamart_table()\n",
    "        \n",
    "        # Initialize and start the data fetcher\n",
    "        print(\"Starting lab data ETL process...\")\n",
    "        lab_data_fetcher = LabDataFetcher(hive_conn, pg_conn, batch_size=10000, polling_interval=300)\n",
    "        lab_data_fetcher.start_polling()\n",
    "    else:\n",
    "        print(\"Failed to establish database connections.\")\n",
    "        print(\"Please check your connection parameters and network connectivity.\")\n",
    "\n",
    "    # Close connections\n",
    "    print(\"Closing database connections...\")\n",
    "    hive_conn.close()\n",
    "    pg_conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "005d18d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting thrift_sasl\n",
      "  Using cached thrift_sasl-0.4.3-py2.py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: thrift>=0.10.0 in /home/tynash/.local/lib/python3.10/site-packages (from thrift_sasl) (0.22.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from thrift_sasl) (1.16.0)\n",
      "Collecting pure-sasl>=0.6.2\n",
      "  Using cached pure-sasl-0.6.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pure-sasl\n",
      "  Building wheel for pure-sasl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pure-sasl: filename=pure_sasl-0.6.2-py3-none-any.whl size=11443 sha256=7c0c416a5812532f4fb0cfc5de59b49d747688dd22d34652ae3037ccfb253a09\n",
      "  Stored in directory: /home/tynash/.cache/pip/wheels/57/7c/93/062238b0a68efe214024ca178233f248971045db1033c96a52\n",
      "Successfully built pure-sasl\n",
      "Installing collected packages: pure-sasl, thrift_sasl\n",
      "Successfully installed pure-sasl-0.6.2 thrift_sasl-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install thrift_sasl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5d318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
