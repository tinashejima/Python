{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data fetch cycle.\n",
      ">>> Detected Snapshot done: 2024-12-17T09:14:47.497+00:00\n",
      "Fetching batch data where last_updated > 2024-12-17T09:14:47.497+00:00\n",
      "Fetched batch data: (1, 28)\n",
      "Inserting new record for record_hash: e1f5aa2d805c653042afd713aa336fa66048518ce111091b80c535eb8c7bbe77\n",
      "Updated last_processed_value to: 2024-12-17T18:41:23.838+00:00\n",
      "Fetching batch data where last_updated > 2024-12-17T18:41:23.838+00:00\n",
      "Error executing query: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: Observation; line 13 pos 11:36:35', 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:325', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:216', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:43', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.GeneratedMethodAccessor172:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:568', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'jdk.proxy2.$Proxy37:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:280', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:456', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', '*org.apache.spark.sql.AnalysisException:Table or view not found: Observation; line 13 pos 11:438:403', 'org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt:failAnalysis:package.scala:42', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis$1:CheckAnalysis.scala:131', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis$1$adapted:CheckAnalysis.scala:102', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:367', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:366', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:366', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:366', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:366', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:366', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:366', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:366', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:366', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:366', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:366', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:366', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:366', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis:CheckAnalysis.scala:102', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis$:CheckAnalysis.scala:97', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews:Analyzer.scala:1008', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews:Analyzer.scala:1011', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:$anonfun$applyOrElse$46:Analyzer.scala:1059', 'scala.Option:map:Option.scala:230', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:applyOrElse:Analyzer.scala:1059', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:applyOrElse:Analyzer.scala:1023', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$3:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:TreeNode.scala:176', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.BinaryLike:mapChildren:TreeNode.scala:1254', 'org.apache.spark.sql.catalyst.trees.BinaryLike:mapChildren$:TreeNode.scala:1253', 'org.apache.spark.sql.catalyst.plans.logical.Join:mapChildren:basicLogicalOperators.scala:390', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1023', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:982', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$2:RuleExecutor.scala:211', 'scala.collection.LinearSeqOptimized:foldLeft:LinearSeqOptimized.scala:126', 'scala.collection.LinearSeqOptimized:foldLeft$:LinearSeqOptimized.scala:122', 'scala.collection.immutable.List:foldLeft:List.scala:91', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1:RuleExecutor.scala:208', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1$adapted:RuleExecutor.scala:200', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:execute:RuleExecutor.scala:200', 'org.apache.spark.sql.catalyst.analysis.Analyzer:org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext:Analyzer.scala:231', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$resolveViews$2:Analyzer.scala:1003', 'org.apache.spark.sql.internal.SQLConf$:withExistingConf:SQLConf.scala:158', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$resolveViews$1:Analyzer.scala:1003', 'org.apache.spark.sql.catalyst.analysis.AnalysisContext$:withAnalysisContext:Analyzer.scala:167', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews:Analyzer.scala:995', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews:Analyzer.scala:1011', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:$anonfun$applyOrElse$46:Analyzer.scala:1059', 'scala.Option:map:Option.scala:230', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:applyOrElse:Analyzer.scala:1059', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:applyOrElse:Analyzer.scala:1023', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$3:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:TreeNode.scala:176', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.BinaryLike:mapChildren:TreeNode.scala:1256', 'org.apache.spark.sql.catalyst.trees.BinaryLike:mapChildren$:TreeNode.scala:1253', 'org.apache.spark.sql.catalyst.plans.logical.Join:mapChildren:basicLogicalOperators.scala:390', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.BinaryLike:mapChildren:TreeNode.scala:1254', 'org.apache.spark.sql.catalyst.trees.BinaryLike:mapChildren$:TreeNode.scala:1253', 'org.apache.spark.sql.catalyst.plans.logical.Join:mapChildren:basicLogicalOperators.scala:390', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.Aggregate:mapChildren:basicLogicalOperators.scala:977', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1023', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:982', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$2:RuleExecutor.scala:211', 'scala.collection.LinearSeqOptimized:foldLeft:LinearSeqOptimized.scala:126', 'scala.collection.LinearSeqOptimized:foldLeft$:LinearSeqOptimized.scala:122', 'scala.collection.immutable.List:foldLeft:List.scala:91', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1:RuleExecutor.scala:208', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1$adapted:RuleExecutor.scala:200', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:execute:RuleExecutor.scala:200', 'org.apache.spark.sql.catalyst.analysis.Analyzer:org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext:Analyzer.scala:231', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$resolveViews$2:Analyzer.scala:1003', 'org.apache.spark.sql.internal.SQLConf$:withExistingConf:SQLConf.scala:158', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$resolveViews$1:Analyzer.scala:1003', 'org.apache.spark.sql.catalyst.analysis.AnalysisContext$:withAnalysisContext:Analyzer.scala:167', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews:Analyzer.scala:995', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews:Analyzer.scala:1011', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:$anonfun$applyOrElse$46:Analyzer.scala:1059', 'scala.Option:map:Option.scala:230', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:applyOrElse:Analyzer.scala:1059', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:applyOrElse:Analyzer.scala:1023', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$3:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:TreeNode.scala:176', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1023', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:982', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$2:RuleExecutor.scala:211', 'scala.collection.LinearSeqOptimized:foldLeft:LinearSeqOptimized.scala:126', 'scala.collection.LinearSeqOptimized:foldLeft$:LinearSeqOptimized.scala:122', 'scala.collection.immutable.List:foldLeft:List.scala:91', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1:RuleExecutor.scala:208', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1$adapted:RuleExecutor.scala:200', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:execute:RuleExecutor.scala:200', 'org.apache.spark.sql.catalyst.analysis.Analyzer:org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext:Analyzer.scala:231', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$resolveViews$2:Analyzer.scala:1003', 'org.apache.spark.sql.internal.SQLConf$:withExistingConf:SQLConf.scala:158', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$resolveViews$1:Analyzer.scala:1003', 'org.apache.spark.sql.catalyst.analysis.AnalysisContext$:withAnalysisContext:Analyzer.scala:167', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews:Analyzer.scala:995', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews:Analyzer.scala:1011', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:$anonfun$applyOrElse$46:Analyzer.scala:1059', 'scala.Option:map:Option.scala:230', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:applyOrElse:Analyzer.scala:1059', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13:applyOrElse:Analyzer.scala:1023', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$3:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:TreeNode.scala:176', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.Sort:mapChildren:basicLogicalOperators.scala:756', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode:mapChildren:LogicalPlan.scala:208', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1228', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1227', 'org.apache.spark.sql.catalyst.plans.logical.GlobalLimit:mapChildren:basicLogicalOperators.scala:1258', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:30', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1023', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:982', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$2:RuleExecutor.scala:211', 'scala.collection.LinearSeqOptimized:foldLeft:LinearSeqOptimized.scala:126', 'scala.collection.LinearSeqOptimized:foldLeft$:LinearSeqOptimized.scala:122', 'scala.collection.immutable.List:foldLeft:List.scala:91', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1:RuleExecutor.scala:208', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1$adapted:RuleExecutor.scala:200', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:execute:RuleExecutor.scala:200', 'org.apache.spark.sql.catalyst.analysis.Analyzer:org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext:Analyzer.scala:231', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$execute$1:Analyzer.scala:227', 'org.apache.spark.sql.catalyst.analysis.AnalysisContext$:withNewAnalysisContext:Analyzer.scala:173', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:227', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$executeAndTrack$1:RuleExecutor.scala:179', 'org.apache.spark.sql.catalyst.QueryPlanningTracker$:withTracker:QueryPlanningTracker.scala:88', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:executeAndTrack:RuleExecutor.scala:179', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:212', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:330', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:211', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:76', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:111', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$2:QueryExecution.scala:185', 'org.apache.spark.sql.execution.QueryExecution$:withInternalError:QueryExecution.scala:510', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:185', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:779', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:184', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:76', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:74', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:66', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:98', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:779', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:96', 'org.apache.spark.sql.SparkSession:$anonfun$sql$1:SparkSession.scala:622', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:779', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:617', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:651', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:291'], sqlState=None, errorCode=0, errorMessage='Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: Observation; line 13 pos 11'), operationHandle=None)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 306\u001b[0m\n\u001b[0;32m    303\u001b[0m     pg_conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 300\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hive_conn\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;129;01mand\u001b[39;00m pg_conn\u001b[38;5;241m.\u001b[39mconnect():\n\u001b[0;32m    299\u001b[0m     data_fetcher \u001b[38;5;241m=\u001b[39m DataFetcher(hive_conn, pg_conn)\n\u001b[1;32m--> 300\u001b[0m     \u001b[43mdata_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_polling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m hive_conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    303\u001b[0m pg_conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[1], line 273\u001b[0m, in \u001b[0;36mDataFetcher.start_polling\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting data fetch cycle.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_data_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolling_interval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds before next fetch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    275\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolling_interval)\n",
      "Cell \u001b[1;32mIn[1], line 159\u001b[0m, in \u001b[0;36mDataFetcher.fetch_data_in_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching batch data where last_updated > \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_processed_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    154\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124mSELECT * FROM fact_anc \u001b[39m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124mWHERE last_updated_date> \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_processed_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124mORDER BY last_updated_date ASC LIMIT \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 159\u001b[0m batch_data, columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhive_conn\u001b[38;5;241m.\u001b[39mexecute_query(query)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_data:  \u001b[38;5;66;03m# No more data\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo more data to fetch. Ending batch fetching.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from pyhive import hive\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import time\n",
    "import hashlib\n",
    "import thrift_sasl\n",
    "import math\n",
    "\n",
    "class HiveConnection:\n",
    "    def __init__(self, host, port, username, password, database, auth_mode):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.database = database\n",
    "        self.auth_mode = auth_mode\n",
    "        self.connection = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Creates a connection to the Hive database.\"\"\"\n",
    "        try:\n",
    "            self.connection = hive.Connection(\n",
    "                host=self.host,\n",
    "                port=self.port,\n",
    "                username=self.username,\n",
    "                password=self.password,\n",
    "                database=self.database,\n",
    "                auth=self.auth_mode\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating connection: {e}\")\n",
    "            return False\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the connection to the Hive database.\"\"\"\n",
    "        if self.connection:\n",
    "            try:\n",
    "                self.connection.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing connection: {e}\")\n",
    "\n",
    "    def execute_query(self, query):\n",
    "        \"\"\"Executes the given query and returns the results.\"\"\"\n",
    "        try:\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            results = cursor.fetchall()\n",
    "            return results, columns\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing query: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "\n",
    "class PostgresConnection:\n",
    "    def __init__(self, dbname, user, password, host, port):\n",
    "        self.dbname = dbname\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.connection = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Creates a connection to the PostgreSQL database.\"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(\n",
    "                dbname=self.dbname,\n",
    "                user=self.user,\n",
    "                password=self.password,\n",
    "                host=self.host,\n",
    "                port=self.port\n",
    "            )\n",
    "            self.connection.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "            self.cursor = self.connection.cursor()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating PostgreSQL connection: {e}\")\n",
    "            return False\n",
    "\n",
    "    def execute_query(self, query, params=None):\n",
    "        \"\"\"Executes a query with optional parameters for SELECT, INSERT, or UPDATE operations.\"\"\"\n",
    "        try:\n",
    "            # Execute the query with parameters\n",
    "            self.cursor.execute(query, params)\n",
    "\n",
    "            # Check if the query is a SELECT statement\n",
    "            if query.strip().upper().startswith(\"SELECT\"):\n",
    "                # Fetch and return results for SELECT queries\n",
    "                return self.cursor.fetchall()\n",
    "            else:\n",
    "                # Commit transaction for non-SELECT queries (e.g., INSERT, UPDATE)\n",
    "                self.connection.commit()\n",
    "                return True  # Return True to indicate success for INSERT/UPDATE queries\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing PostgreSQL query: {e}\")\n",
    "            return None\n",
    "\n",
    "    # def execute_query(self, query, params=None):\n",
    "    #     \"\"\"Executes a query with optional parameters.\"\"\"\n",
    "    #     try:\n",
    "    #         self.cursor.execute(query, params)\n",
    "    #         return self.cursor.fetchall()\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error executing PostgreSQL query: {e}\")\n",
    "    #         return None\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the PostgreSQL database connection.\"\"\"\n",
    "        if self.connection:\n",
    "            try:\n",
    "                self.connection.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing PostgreSQL connection: {e}\")\n",
    "\n",
    "\n",
    "class DataFetcher:\n",
    "    def __init__(self, hive_conn, pg_conn, batch_size=50000, polling_interval=600):\n",
    "        self.hive_conn = hive_conn\n",
    "        self.pg_conn = pg_conn\n",
    "        self.batch_size = batch_size\n",
    "        self.polling_interval = polling_interval\n",
    "        self.last_processed_value = '1900-01-01 00:00:00'\n",
    "\n",
    "    def check_if_snapshot_done(self):\n",
    "        query = \"SELECT dw_date_created FROM marts.dm_anc ORDER BY dm_date_created DESC LIMIT 1\"\n",
    "        result = self.pg_conn.execute_query(query)\n",
    "        if result:\n",
    "            return result[0][0]\n",
    "        else:\n",
    "            return None\n",
    "    def hash_record(self, record):\n",
    "        \"\"\"\n",
    "        Hash the combined values of a record using SHA-256.\n",
    "        If a value is None or NaN, replace it with an empty string before hashing.\n",
    "        \"\"\"\n",
    "        combined = ''.join([str(value) if not pd.isna(value) else '' for value in record])\n",
    "        return hashlib.sha256(combined.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def fetch_data_in_batches(self):\n",
    "        snapshot_date = self.check_if_snapshot_done()\n",
    "        if snapshot_date:\n",
    "            self.last_processed_value = snapshot_date\n",
    "            print(\">>> Detected Snapshot done:\", self.last_processed_value)\n",
    "        else:\n",
    "            print(\">>> Initial snapshot\")\n",
    "\n",
    "        while True:\n",
    "            print(f\"Fetching batch data where last_updated > {self.last_processed_value}\")\n",
    "            query = f\"\"\"\n",
    "            SELECT * FROM fact_anc \n",
    "            WHERE last_updated_date> '{self.last_processed_value}' \n",
    "            ORDER BY last_updated_date ASC LIMIT {self.batch_size}\n",
    "            \"\"\"\n",
    "            batch_data, columns = self.hive_conn.execute_query(query)\n",
    "\n",
    "            if not batch_data:  # No more data\n",
    "                print(\"No more data to fetch. Ending batch fetching.\")\n",
    "                break\n",
    "\n",
    "            df = pd.DataFrame(batch_data, columns=columns)\n",
    "            print(\"Fetched batch data:\", df.shape)\n",
    "            #  print (df.columns)\n",
    "            # Add hash column to the DataFrame\n",
    "            df['record_hash'] = df.apply(lambda row: self.hash_record(row), axis=1)\n",
    "            self.process_data(df)\n",
    "\n",
    "            # Update the last_processed_value to the latest timestamp in the batch\n",
    "            self.last_processed_value = df['last_updated_date'].max()\n",
    "            print(\"Updated last_processed_value to:\", self.last_processed_value)\n",
    "\n",
    "    def process_data(self, df):\n",
    "\n",
    "        ''' df.rename(columns={'patient_id': 'person_id',\n",
    "                           'organization_id': 'facility_id_code',\n",
    "                           'last_updated':'dw_date_created'\n",
    "                           }, inplace=True)'''\n",
    "  \n",
    "        for _, row in df.iterrows():\n",
    "           record_hash = row['record_hash']\n",
    "           encounter_id = row['encounter_id']\n",
    "           check_query = \"SELECT 1 FROM marts.dm_anc WHERE record_hash = %s\"\n",
    "        if self.pg_conn.execute_query(check_query, (record_hash,)):\n",
    "                # Update existing record\n",
    "                print(f\"Updating record for record_hash: {record_hash}\")\n",
    "                update_query = \"\"\"\n",
    "                UPDATE marts.dm_anc\n",
    "                SET anc_number = %s, registration_date = %s, first_time_booking= %s, sex = %s, \n",
    "                    age= %s, lnmp= %s, gravida = %s,\n",
    "                    parity_at_booking= %s, weight = %s, height = %s,\n",
    "                    pulse = %s, bp = %s, assess_for_polor = %s,\n",
    "                    presentation = %s, ga_at_visit = %s, hiv_status_prior_booking = %s, art_number = %s, edd = %s,\n",
    "                    sex = %s, dw_date_created = %s,\n",
    "                    dm_date_created = NOW(), event_date = %s, encounter_id= %s, facility_id_code = %s, \n",
    "                     \n",
    "                WHERE redord_hash = %s\n",
    "                \"\"\"\n",
    "                self.pg_conn.execute_query(update_query, (\n",
    "                    row['anc_number'], row['registration_date'], row['first_time_booking'],\n",
    "                    row['age'], row['lnmp'],\n",
    "                    \"\", row['gravida'], row['parity_at_booking'],\n",
    "                    row['weight'], \"\", \"\",\n",
    "                    row['height'], row['pulse'], \"\",\n",
    "                    row['bp'], row['assess_for_polor'], row['presentation'], row['ga_at_visit'],\n",
    "                    row['hiv_status_at_booking'], row['art_number'], row['edd'],\n",
    "                    row['sex'], row['last_updated_date'], row['anc_visit_date'], row['encounter_id'], row['facility_id_code'], record_hash\n",
    "                ))\n",
    "        else:\n",
    "                # Insert new record\n",
    "                print(f\"Inserting new record for record_hash: {record_hash}\")\n",
    "                insert_query = \"\"\"\n",
    "                INSERT INTO marts.dm_anc (\n",
    "                    anc_number, \n",
    "                    registration_date, \n",
    "                    first_time_booking, \n",
    "                    age, \n",
    "                    lnmp,\n",
    "                    gravida, \n",
    "                    parity_at_booking, \n",
    "                    weight, \n",
    "                    height,\n",
    "                    pulse, \n",
    "                    bp, \n",
    "                    assess_for_polor, \n",
    "                    presentation, \n",
    "                    ga_at_visit, \n",
    "                    hiv_status_prior_booking, \n",
    "                    art_number,\n",
    "                    edd, \n",
    "                    sex,\n",
    "                    dw_date_created, \n",
    "                    dm_date_created,\n",
    "                    event_date, \n",
    "                    encounter_id, \n",
    "                    facility_id_code, \n",
    "                    record_hash\n",
    "                ) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),%s,%s,%s,%s)\n",
    "                \"\"\"\n",
    "                self.pg_conn.execute_query(insert_query, (\n",
    "                    row['anc_id'], \n",
    "                    row['reg_date'], \n",
    "                    row['first_anc_booking'],\n",
    "                    row['age_at_encounter'], \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    row['parity_at_booking'], \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    \"\",\n",
    "                    \"\", \n",
    "                    \"\", \n",
    "                    row['estimated_date_of_delivery'], \n",
    "                    \"\", \n",
    "                    row['last_updated_date'], \n",
    "                    row['anc_visit_date'], \n",
    "                    row['encounter_id'], \n",
    "                    row['organization_id'], \n",
    "                    record_hash\n",
    "                ))\n",
    "\n",
    "    def start_polling(self):\n",
    "        \"\"\"Starts polling to fetch data every 5 minutes.\"\"\"\n",
    "        while True:\n",
    "            print(\"Starting data fetch cycle.\")\n",
    "            self.fetch_data_in_batches()\n",
    "            print(f\"Waiting for {self.polling_interval} seconds before next fetch.\")\n",
    "            time.sleep(self.polling_interval)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize Hive connection\n",
    "    hive_conn = HiveConnection(\n",
    "        host=\"197.221.242.150\",\n",
    "        port=17251,\n",
    "        username=\"schaputsira\",\n",
    "        password=\"brbvKgRhESRa9R4u\",\n",
    "        database=\"default\",\n",
    "        auth_mode=\"LDAP\"\n",
    "    )\n",
    "\n",
    "    # Initialize PostgreSQL connection\n",
    "    pg_conn = PostgresConnection(\n",
    "        dbname=\"HTSDATA\",\n",
    "        user=\"postgres\",\n",
    "        password=\"wGMCAE6zFHcyrBmXtus97JPanxvkY4fb\",\n",
    "        host=\"127.0.0.1\",\n",
    "        port=5431\n",
    "    )\n",
    "\n",
    "    if hive_conn.connect() and pg_conn.connect():\n",
    "        data_fetcher = DataFetcher(hive_conn, pg_conn)\n",
    "        data_fetcher.start_polling()\n",
    "\n",
    "    hive_conn.close()\n",
    "    pg_conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
