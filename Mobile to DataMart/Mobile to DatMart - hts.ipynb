{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data fetch cycle.\n",
      ">>> Detected Snapshot done: 2024-11-05 17:32:48\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      ">>> Detected Snapshot done: 2024-11-05 17:32:48\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      ">>> Detected Snapshot done: 2024-11-05 17:32:48\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      ">>> Detected Snapshot done: 2024-11-05 17:32:48\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      ">>> Detected Snapshot done: 2024-11-05 17:32:48\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      ">>> Detected Snapshot done: 2024-11-05 17:32:48\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      ">>> Detected Snapshot done: 2024-11-05 17:32:48\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      ">>> Detected Snapshot done: 2024-11-05 17:32:48\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: server closed the connection unexpectedly\n",
      "\tThis probably means the server terminated abnormally\n",
      "\tbefore or while processing the request.\n",
      "\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "No more data to fetch. Ending batch fetching.\n",
      "Waiting for 600 seconds before next fetch.\n",
      "Starting data fetch cycle.\n",
      "Error executing PostgreSQL query: cursor already closed\n",
      ">>> Initial snapshot\n",
      "Fetching batch data where event_date > 2024-11-05 17:32:48\n",
      "Error executing query: could not receive data from server: Software caused connection abort (0x00002745/10053)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 284\u001b[0m\n\u001b[0;32m    280\u001b[0m     pg_conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 284\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[2], line 277\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mob_conn\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;129;01mand\u001b[39;00m pg_conn\u001b[38;5;241m.\u001b[39mconnect():\n\u001b[0;32m    276\u001b[0m     data_fetcher \u001b[38;5;241m=\u001b[39m DataFetcher(mob_conn, pg_conn)\n\u001b[1;32m--> 277\u001b[0m     data_fetcher\u001b[38;5;241m.\u001b[39mstart_polling()\n\u001b[0;32m    279\u001b[0m mob_conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    280\u001b[0m pg_conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[2], line 251\u001b[0m, in \u001b[0;36mDataFetcher.start_polling\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting data fetch cycle.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_data_in_batches()\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolling_interval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds before next fetch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    253\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolling_interval)\n",
      "Cell \u001b[1;32mIn[2], line 153\u001b[0m, in \u001b[0;36mDataFetcher.fetch_data_in_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching batch data where event_date > \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_processed_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    146\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124mSELECT * FROM report.hts\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124mWHERE event_date > \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_processed_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124mORDER BY event_date ASC LIMIT \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 153\u001b[0m batch_data, columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmob_conn\u001b[38;5;241m.\u001b[39mexecute_query(query)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_data:  \u001b[38;5;66;03m# No more data\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo more data to fetch. Ending batch fetching.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from pyhive import hive\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import time\n",
    "import math\n",
    "\n",
    "class MobileConnection:\n",
    "    def __init__(self, dbname, user, password, host, port):\n",
    "        self.dbname = dbname\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.connection = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Creates a connection to the Postgres Mobile database.\"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(\n",
    "                dbname=self.dbname,\n",
    "                user=self.user,\n",
    "                password=self.password,\n",
    "                host=self.host,\n",
    "                port=self.port\n",
    "            )\n",
    "            self.connection.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "            self.cursor = self.connection.cursor()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating PostgreSQL Mobile connection: {e}\")\n",
    "            return False\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the connection to the Mobile database.\"\"\"\n",
    "        if self.connection:\n",
    "            try:\n",
    "                self.connection.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing connection: {e}\")\n",
    "\n",
    "    def execute_query(self, query):\n",
    "        \"\"\"Executes the given query and returns the results.\"\"\"\n",
    "        try:\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            results = cursor.fetchall()\n",
    "            return results, columns\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing query: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "\n",
    "class PostgresConnection:\n",
    "    def __init__(self, dbname, user, password, host, port):\n",
    "        self.dbname = dbname\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.connection = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Creates a connection to the PostgreSQL database.\"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(\n",
    "                dbname=self.dbname,\n",
    "                user=self.user,\n",
    "                password=self.password,\n",
    "                host=self.host,\n",
    "                port=self.port\n",
    "            )\n",
    "            self.connection.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "            self.cursor = self.connection.cursor()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating PostgreSQL connection: {e}\")\n",
    "            return False\n",
    "\n",
    "    def execute_query(self, query, params=None):\n",
    "        \"\"\"Executes a query with optional parameters for SELECT, INSERT, or UPDATE operations.\"\"\"\n",
    "        try:\n",
    "            # Execute the query with parameters\n",
    "            self.cursor.execute(query, params)\n",
    "\n",
    "            # Check if the query is a SELECT statement\n",
    "            if query.strip().upper().startswith(\"SELECT\"):\n",
    "                # Fetch and return results for SELECT queries\n",
    "                return self.cursor.fetchall()\n",
    "            else:\n",
    "                # Commit transaction for non-SELECT queries (e.g., INSERT, UPDATE)\n",
    "                self.connection.commit()\n",
    "                return True  # Return True to indicate success for INSERT/UPDATE queries\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing PostgreSQL query: {e}\")\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the PostgreSQL database connection.\"\"\"\n",
    "        if self.connection:\n",
    "            try:\n",
    "                self.connection.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing PostgreSQL connection: {e}\")\n",
    "\n",
    "\n",
    "class DataFetcher:\n",
    "    def __init__(self, mob_conn, pg_conn, batch_size=50000, polling_interval=600):\n",
    "        self.mob_conn = mob_conn\n",
    "        self.pg_conn = pg_conn\n",
    "        self.batch_size = batch_size\n",
    "        self.polling_interval = polling_interval\n",
    "        self.last_processed_value = '1900-01-01 00:00:00'\n",
    "\n",
    "    def check_if_snapshot_done(self):\n",
    "        query = \"SELECT event_date FROM marts.dm_hts_test ORDER BY dm_date_created DESC LIMIT 1\"\n",
    "        result = self.pg_conn.execute_query(query)\n",
    "        if result:\n",
    "            return result[0][0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # def hash_record(self, record):\n",
    "    #     \"\"\"\n",
    "    #     Hash the combined values of a record using SHA-256.\n",
    "    #     If a value is None or NaN, replace it with an empty string before hashing.\n",
    "    #     \"\"\"\n",
    "    #     combined = ''.join([str(value) if not pd.isna(value) else '' for value in record])\n",
    "    #     return hashlib.sha256(combined.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def fetch_data_in_batches(self):\n",
    "        snapshot_date = self.check_if_snapshot_done()\n",
    "        if snapshot_date:\n",
    "            self.last_processed_value = snapshot_date\n",
    "            print(\">>> Detected Snapshot done:\", self.last_processed_value)\n",
    "        else:\n",
    "            print(\">>> Initial snapshot\")\n",
    "\n",
    "        while True:\n",
    "            print(f\"Fetching batch data where event_date > {self.last_processed_value}\")\n",
    "            query = f\"\"\"\n",
    "            SELECT * FROM report.hts\n",
    "            WHERE event_date > '{self.last_processed_value}' \n",
    "            ORDER BY event_date ASC LIMIT {self.batch_size}\n",
    "            \"\"\"\n",
    "\n",
    "            \n",
    "            batch_data, columns = self.mob_conn.execute_query(query)\n",
    "\n",
    "            if not batch_data:  # No more data\n",
    "                print(\"No more data to fetch. Ending batch fetching.\")\n",
    "                break\n",
    "            \n",
    "            df = pd.DataFrame(batch_data, columns=columns)\n",
    "            print(\"Fetched batch data:\", df.shape)\n",
    "\n",
    "            # Add hash column to the DataFrame\n",
    "            # df['record_hash'] = df.apply(lambda row: self.hash_record(row), axis=1)\n",
    "\n",
    "            df.to_csv(\"outputwithdps.csv\", index=False)\n",
    "            \n",
    "            # Process and update the last_processed_value\n",
    "            self.process_data(df)\n",
    "\n",
    "            # Update the last_processed_value to the latest timestamp in the batch\n",
    "\n",
    "            self.last_processed_value = df['event_date'].max()\n",
    "            print(\"Updated last_processed_value to:\", self.last_processed_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def process_data(self, df):\n",
    "        \"\"\"Processes the data batch by cleaning and then inserting/updating it in PostgreSQL.\"\"\"\n",
    "      \n",
    "        for _, row in df.iterrows():\n",
    "            encounter_id = row['encounter_id']\n",
    "            check_query = \"SELECT 1 FROM marts.dm_hts_test WHERE encounter_id = %s\"\n",
    "            if self.pg_conn.execute_query(check_query, (encounter_id,)):\n",
    "                # Update existing record\n",
    "                print(f\"Updating record for encounter_id: {encounter_id}\")\n",
    "                update_query = \"\"\"\n",
    "                UPDATE marts.dm_hts_test\n",
    "                SET hts_model= %s, person_id = %s, birthdate = %s, sex = %s, date_of_hiv_test = %s, \n",
    "                reason_for_hiv_testing =%s, hts_test_result = %s, hts_approach = %s,\n",
    "                mobile_client = %s, hts_number = %s, hts_type = %s, age_at_visit = %s,\n",
    "                event_date = %s, consent_to_index_testing = %s, client_profile = %s, index_case_hts_number = %s,\n",
    "                contact_of_index_case = %s, dedupe_id = %s, rtri_result_testhts = %s,\n",
    "                entry_point = %s, first_test_ever_in_life = %s, first_test_for_this_pregnancy = %s, hts_sub_model= %s,\n",
    "                reasonfor_not_performingrecency_test = %s, timing_of_hiv_diagnosis = %s, \"hts_test_A1\" = %s, \"hts_test_A2\" = %s, \n",
    "                \"hts_test_A3\" = %s, opted_out_of_hiv_testing =%s, pre_test_information_given = %s, rtri_test_done = %s,\n",
    "                reason_for_not_issuing_result = %s, received_hiv_test_results = %s, received_post_test_counselling = %s,\n",
    "                \"retesting_before_art_Initiation\" = %s, self_identified_gender = %s, verification_test_done = %s, hts_hiv_positive = %s,\n",
    "                number_of_hiv_tests_done = %s, facility_id_code = %s, dm_date_created = NOW()\n",
    "                WHERE encounter_id = %s\n",
    "                \"\"\"\n",
    "                self.pg_conn.execute_query(update_query, (\n",
    "                    row['htc_model'], row['person_id'], row['birthdate'], row['sex'], row['date'], row['test_purpose'], row['result'], row['approach'],\n",
    "                    '', row['hts_number'], row['test_type'], row['age_at_visit'], row['event_date'], row['consent_to_index_testing'], row['client_profile'],\n",
    "                    row['index_case_hts_number'], '', '', row['rtri_test_results'], row['refered_service'], row['first_test_ever'], row['pregnancytest'], row['hts_model_sub_type'], row['reason_for_not_performing_test'], \n",
    "                    row['care_giver_result_date'], row['hts_test_a1'],row['hts_test_a2'], row['hts_test_a3'], row['opt'], row['pre_test_counselling'], row['rtri_test_done'], \n",
    "                    row['reason_for_not_issuing_result'],\n",
    "                    row['results_issued'], row['post_test_counselling'], row['retest_before_art_initiation'], row['self_identified_gender'], row['verification_test_done'], row['hts_hiv_positive'], row['test_count'], row['tenant_id'], \n",
    "                    encounter_id\n",
    "                ))\n",
    "            else:\n",
    "                # Insert new record\n",
    "                print(f\"Inserting new record for encounter_id: {encounter_id}\")\n",
    "                insert_query = \"\"\"\n",
    "                INSERT INTO marts.dm_hts_test (hts_model, person_id, birthdate, sex, date_of_hiv_test,\n",
    "                reason_for_hiv_testing, hts_test_result, hts_approach, mobile_client, hts_number, hts_type, age_at_visit,\n",
    "                event_date, consent_to_index_testing, client_profile, index_case_hts_number,\n",
    "                contact_of_index_case, dedupe_id, rtri_result_testhts,\n",
    "                entry_point, first_test_ever_in_life, first_test_for_this_pregnancy, hts_sub_model,\n",
    "                reasonfor_not_performingrecency_test, timing_of_hiv_diagnosis, \"hts_test_A1\", \"hts_test_A2\",\n",
    "                \"hts_test_A3\", opted_out_of_hiv_testing, pre_test_information_given, rtri_test_done,\n",
    "                reason_for_not_issuing_result, received_hiv_test_results, received_post_test_counselling,\n",
    "                \"retesting_before_art_Initiation\", self_identified_gender, verification_test_done, hts_hiv_positive,\n",
    "                number_of_hiv_tests_done, facility_id_code, dm_date_created, encounter_id \n",
    "                ) VALUES (%s, %s, %s, %s, \n",
    "                            %s, %s, %s,%s,\n",
    "                            %s, %s, %s, %s,\n",
    "                            %s, %s, %s, %s,\n",
    "                            %s, %s, %s, %s,\n",
    "                            %s, %s, %s, %s,\n",
    "                            %s, %s, %s, %s,\n",
    "                            %s, %s, %s, %s,\n",
    "                            %s, %s, %s, %s,\n",
    "                            %s, %s, %s, %s,\n",
    "                            NOW(), %s) \n",
    "                \"\"\"\n",
    "                self.pg_conn.execute_query(insert_query, (\n",
    "                    row['htc_model'], row['person_id'], row['birthdate'], row['sex'], row['date'], row['test_purpose'], row['result'], row['approach'],\n",
    "                    '', row['hts_number'], row['test_type'], row['age_at_visit'], row['event_date'], row['consent_to_index_testing'], row['client_profile'],\n",
    "                    row['index_case_hts_number'], '', '', row['rtri_test_results'], row['refered_service'], row['first_test_ever'], row['pregnancytest'], row['hts_model_sub_type'], row['reason_for_not_performing_test'], \n",
    "                    row['care_giver_result_date'], row['hts_test_a1'],row['hts_test_a2'], row['hts_test_a3'], row['opt'], row['pre_test_counselling'], row['rtri_test_done'], \n",
    "                    row['reason_for_not_issuing_result'],\n",
    "                    row['results_issued'], row['post_test_counselling'], row['retest_before_art_initiation'], row['self_identified_gender'], row['verification_test_done'], row['hts_hiv_positive'],row['test_count'], row['tenant_id'], \n",
    "                    encounter_id\n",
    "                ))\n",
    "\n",
    "    def start_polling(self):\n",
    "        \"\"\"Starts polling to fetch data every 5 minutes.\"\"\"\n",
    "        while True:\n",
    "            print(\"Starting data fetch cycle.\")\n",
    "            self.fetch_data_in_batches()\n",
    "            print(f\"Waiting for {self.polling_interval} seconds before next fetch.\")\n",
    "            time.sleep(self.polling_interval)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize Mobile connection\n",
    "    mob_conn = MobileConnection(\n",
    "        dbname=\"master\",\n",
    "        user=\"postgres\",\n",
    "        password=\"wGMCAE6zFHcyrBmXtus97JPanxvkY4fb\",\n",
    "        host=\"127.0.0.1\",\n",
    "        port=5434\n",
    "    )\n",
    "\n",
    "    # Initialize PostgreSQL connection\n",
    "    pg_conn = PostgresConnection(\n",
    "        dbname=\"HTSDATA\",\n",
    "        user=\"postgres\",\n",
    "        password=\"wGMCAE6zFHcyrBmXtus97JPanxvkY4fb\",\n",
    "        host=\"127.0.0.1\",\n",
    "        port=5431\n",
    "    )\n",
    "\n",
    "    if mob_conn.connect() and pg_conn.connect():\n",
    "        data_fetcher = DataFetcher(mob_conn, pg_conn)\n",
    "        data_fetcher.start_polling()\n",
    "\n",
    "    mob_conn.close()\n",
    "    pg_conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
