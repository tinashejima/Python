import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
import shap
import matplotlib.pyplot as plt

df = pd.read_csv('diabetes_dataset.csv')

# Data Preprocessing
# Handle 'No Info' in smoking_history by treating it as a separate category
df['smoking_history'] = df['smoking_history'].replace('No Info', 'unknown')

# Separate features and target variable
X = df.drop('diabetes', axis=1)
y = df['diabetes']

# Identify categorical and numerical features
categorical_features = ['gender', 'smoking_history']
numerical_features = ['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', 'blood_glucose_level']

# Create preprocessing pipelines for numerical and categorical features
numerical_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Training
# Define individual models
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
lr_clf = LogisticRegression(random_state=42, max_iter=1000)
svm_clf = SVC(probability=True, random_state=42)

# Create the ensemble model with a VotingClassifier
ensemble_clf = VotingClassifier(
    estimators=[('rf', rf_clf), ('lr', lr_clf), ('svm', svm_clf)],
    voting='soft'
)

# Create the full pipeline
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                            ('classifier', ensemble_clf)])

# Train the model
pipeline.fit(X_train, y_train)

# Model Evaluation
y_pred = pipeline.predict(X_test)

print('Classification Report:')
print(classification_report(y_test, y_pred))

print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))

# Model Interpretation with SHAP
# SHAP requires a function that takes a numpy array and returns predictions
# We'll create a wrapper for our pipeline's prediction function
def predict_fn(x):
    return pipeline.predict_proba(pd.DataFrame(x, columns=X_test.columns))

# Create a SHAP explainer
explainer = shap.KernelExplainer(predict_fn, shap.sample(X_train, 100))

# Calculate SHAP values for the test set (or a sample for speed)
shap_values = explainer.shap_values(shap.sample(X_test, 100))

# Get the feature names after one-hot encoding
ohe_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)
feature_names = numerical_features + list(ohe_feature_names)

# Update the SHAP values with the correct feature names
shap_values_df = pd.DataFrame(shap_values[1], columns=feature_names)

# SHAP Summary Plot
shap.summary_plot(shap_values[1], shap.sample(X_test, 100), feature_names=feature_names, show=False)
plt.title('SHAP Summary Plot for Diabetes Prediction')
plt.show()

# SHAP Dependence Plots
for feature in numerical_features:
    shap.dependence_plot(feature, shap_values[1], shap.sample(X_test, 100), feature_names=feature_names, interaction_index=None, show=False)
    plt.title(f'SHAP Dependence Plot for {feature}')
    plt.show()