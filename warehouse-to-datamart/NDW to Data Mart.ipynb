{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect (Attempt 1)...\n",
      "Initial load: Fetching all data from the fact_hts_dedup table in batches of 50000.\n",
      ">>> Initial snapshot\n",
      "...................................................\n",
      "Total Number of Records to be fetched: 9300\n",
      "Total Number of Facilities: 1\n",
      "Number of Distinct People: 4137\n",
      "Number of Distinct Encounters: 5039\n",
      "Last time update: 2025-09-24T09:58:00.022+00:00\n",
      "Total Number of Batches  1\n",
      "..................................................\n",
      "working on Batch  1\n"
     ]
    }
   ],
   "source": [
    "from pyhive import hive\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Connection to HTS Data Mart\n",
    "# conn_mart = psycopg2.connect(dbname=\"HTSDATA\", user=\"postgres\", password=\"wGMCAE6zFHcyrBmXtus97JPanxvkY4fb\", host=\"127.0.0.1\",port= 5431)\n",
    "conn_mart = psycopg2.connect(dbname=\"LIMSDATA\", user=\"postgres\", password=\"wGMCAE6zFHcyrBmXtus97JPanxvkY4fb\", host=\"127.0.0.1\",port= 5431)\n",
    "\n",
    "# conn_mart = psycopg2.connect(dbname=\"HTS_DB\", user=\"postgres\", password=\"root\", host=\"127.0.0.1\",port= 5432)\n",
    "conn_mart.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "cur_mart = conn_mart.cursor()\n",
    "\n",
    "# Function to establish a Hive connection\n",
    "def create_hive_connection(host, port, username, password, database, auth_mode):\n",
    "    \"\"\"Creates a connection to the Hive database.\"\"\"\n",
    "    try:\n",
    "        conn = hive.Connection(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            username=username,\n",
    "            password=password,\n",
    "            database=database,\n",
    "            auth=auth_mode\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating connection: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to execute a query and fetch results\n",
    "def execute_hive_query(conn, query):\n",
    "    \"\"\"Executes the given query and returns the results.\"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        results = cursor.fetchall()\n",
    "        return results, columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "\n",
    "# Function to get all data from the fact_lab_request_orders\n",
    "def get_all_patient_data(conn):\n",
    " \n",
    "    \"\"\"Fetches all data from the act_lab_request_orders view.\"\"\"\n",
    "    query = 'SELECT * FROM fact_lab_request_orders where cast(task_authored_on as date) >=2025-02-18 AND encounter_facility_id = ZW090A17'\n",
    "    return execute_hive_query(conn, query)\n",
    "\n",
    "# Function to get new or updated patient records (incremental pull)\n",
    "def get_new_patient_data(conn, last_timestamp):\n",
    "    \"\"\"Fetches newly inserted/updated patient data based on last timestamp.\"\"\"\n",
    "    # Assuming there is a timestamp column 'last_modified' in the patient table\n",
    "    query = f\"SELECT * FROM fact_lab_request_orders WHERE last_updated > '{last_timestamp}' order by last_updated asc\"\n",
    "    return execute_hive_query(conn, query)\n",
    "\n",
    "# Function to close the Hive connection\n",
    "def close_hive_connection(conn):\n",
    "    \"\"\"Closes the connection to the Hive database.\"\"\"\n",
    "    try:\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error closing connection: {e}\")\n",
    "\n",
    "# Polling logic to listen for new or updated records\n",
    "def listen_for_changes(conn, last_timestamp, polling_interval=300):\n",
    "    \"\"\"Polls the Hive database for changes at a regular interval.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Polling for changes after {last_timestamp}...\")\n",
    "            print(\"start here\")\n",
    "            new_data = get_new_patient_data(conn, last_timestamp)\n",
    "            print(\"new data\", new_data[0])\n",
    "            \n",
    "            if new_data[0]:\n",
    "                print(\"New/Updated records found:\")\n",
    "                for row in new_data:\n",
    "                    print(row)\n",
    "                    # Update the last_timestamp to the latest record's timestamp\n",
    "                    # Assuming the 'last_modified' column exists and is at index -1\n",
    "                    last_timestamp = row[-1]  # Update the last timestamp\n",
    "                    \n",
    "            else:\n",
    "                print(\"No new records found.\")\n",
    "            \n",
    "            # Wait for the polling interval before checking again\n",
    "            time.sleep(polling_interval)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during polling: {e}\")\n",
    "            break  # Exit polling loop if an error occurs\n",
    "\n",
    "def check_if_snapshot_done():\n",
    "    cur_mart.execute(\"SELECT dw_date_created FROM marts.dm_lab_request_orders order by dm_date_created desc limit 1\") \n",
    "    result = cur_mart.fetchone()\n",
    "    if result:\n",
    "        return  result[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_all_patient_data_in_batches(conn, batch_size=50000):\n",
    "    batch_number = 1\n",
    "    snapshot_date = check_if_snapshot_done()\n",
    "\n",
    "    if snapshot_date:\n",
    "        last_processed_value = snapshot_date #get last date for the snapshot and start from there \n",
    "        print(\">>> Detected Snapshot done\", last_processed_value)\n",
    "    else:\n",
    "        last_processed_value = '1900-01-01 00:00:00'  #Initialize last_processed_value to start fetching from\n",
    "        print(\">>> Initial snapshot\")\n",
    "\n",
    "    print(\"...................................................\")\n",
    "\n",
    "    # Combine all queries into one\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) AS total_records, \n",
    "        COUNT(DISTINCT encounter_facility_id) AS total_facilities, \n",
    "        COUNT(DISTINCT patient_id) AS distinct_people, \n",
    "        COUNT(DISTINCT encounter_id) AS distinct_encounters, \n",
    "        MAX(last_updated) AS last_update_time\n",
    "    FROM fact_lab_request_orders where last_updated > '{last_processed_value}' and encounter_facility_id = 'ZW090A17'\n",
    "    \"\"\"\n",
    "    # Execute the single query\n",
    "    result = execute_hive_query(conn, query)[0][0]\n",
    "\n",
    "    # Output the results\n",
    "    print(\"Total Number of Records to be fetched:\", result[0])\n",
    "    print(\"Total Number of Facilities:\", result[1])\n",
    "    print(\"Number of Distinct People:\", result[2])\n",
    "    print(\"Number of Distinct Encounters:\", result[3])\n",
    "    print(\"Last time update:\", result[4])\n",
    "\n",
    "    print(\"Total Number of Batches \", str(math.ceil(int(result[0])/ 50000)))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        print(\"..................................................\")\n",
    "        print(\"working on Batch \", batch_number)\n",
    "        # Query to fetch data in batches\n",
    "\n",
    "        query = f\"SELECT * FROM fact_lab_request_orders where last_updated > '{last_processed_value}' and encounter_facility_id = 'ZW090A17'  order by last_updated asc LIMIT {batch_size}\"\n",
    "\n",
    "        batch_data, columns = execute_hive_query(conn, query)\n",
    "        \n",
    "        \n",
    "        if not batch_data:  # No more data\n",
    "            print(f\"All data fetched. Total batches: {batch_number - 1}\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Batch {batch_number}: Fetched {len(batch_data)} rows.\")\n",
    "        \n",
    "        # Append the batch data to the all_data list\n",
    "        df = pd.DataFrame(batch_data, columns=columns)\n",
    "        print(\"Shape of dataframe created \", df.shape)\n",
    "\n",
    "        # Drop duplicate rows \n",
    "        # df['temp_last_updated'] = pd.to_datetime(df['last_updated'], errors='coerce')\n",
    "        # df.sort_values(by=['encounter_id', 'temp_last_updated'], inplace=True)\n",
    "        # df.drop_duplicates(subset=['encounter_id'], keep='last', inplace=True)\n",
    "        # print(\"Shape of dataframe after dropping duplicates \", df.shape)\n",
    "\n",
    "        df['test_type'] =  df['test_type'].str.replace(')','')\n",
    "        df.rename(columns={'patient_id':'person_id', \n",
    "                           'encounter_facility_id':'facility_id_code',\n",
    "                           'birth_date':'birthdate',\n",
    "                           'last_updated':'date_created',\n",
    "                           'result':'test_results',\n",
    "                           'task_authored_on':'shr_date',\n",
    "                           'task_execution_start_date':'impilo_registration_date',\n",
    "                           'task_status':'lab_order_status'}, inplace= True)\n",
    "        \n",
    "        df['event_date'] = df['shr_date']\n",
    "        #df.drop(columns=['has_hts_results'], inplace=True)\n",
    "        print(df.shape)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            encounter_id = row['task_id']\n",
    "            # Check if a record with the given hts_number exists\n",
    "            cur_mart.execute(\"SELECT 1 FROM marts.dm_lab_request_orders WHERE encounter_id = %s\", (encounter_id,))\n",
    "            if cur_mart.fetchone():\n",
    "                # Update existing record\n",
    "                print(f\"Updating record for encounter_id: {encounter_id}\")\n",
    "                update_query = \"\"\"\n",
    "                UPDATE marts.dm_lab_request_orders\n",
    "                SET event_date = %s, dedupe_id = %s, lab_request_number = %s, birthdate = %s, gender = %s, \n",
    "                    shr_date = %s, impilo_registration_date = %s, date_sample_taken = %s , lab_order_status = %s, status_reason = %s, note = %s, sample_code = %s, sample_type = %s,\n",
    "                    test_type = %s, facility_id_code = %s, lab = %s, dw_date_created = %s,\n",
    "                     test_results = %s, dm_date_created = NOW(),person_id = %s\n",
    "                WHERE encounter_id = %s\n",
    "                \"\"\"\n",
    "                cur_mart.execute(update_query, (\n",
    "                    row['event_date'], row['dedupe_id'], row['lab_request_number'], row['birthdate'], row['gender'], row['shr_date'],\n",
    "                    row['impilo_registration_date'], row['date_sample_taken'], row['lab_order_status'],\n",
    "                    row['status_reason'],  row['note'], row['sample_code'],  row['sample_type'],  row['test_type'],\n",
    "                    row['facility_id_code'], row['lab'], row['date_created'],  row['test_results'],\n",
    "                     row['person_id'], encounter_id\n",
    "                ))\n",
    "            else:\n",
    "                # Insert new record\n",
    "                print(f\"Inserting new record for encounter_id: {encounter_id}\")\n",
    "                # insert_query = \"\"\"\n",
    "                # INSERT INTO marts.dm_hts (\n",
    "                #     event_date, dedupe_id, birthdate, sex, date_of_hiv_test,\n",
    "                #     reason_for_hiv_testing, hts_test_result, hts_type, age_at_visit,\n",
    "                #     first_test_ever_in_life, client_profile, self_identified_gender, \n",
    "                #     dw_date_created, dm_date_created, person_id, facility_id_code, received_hiv_test_results,\n",
    "                #     encounter_id\n",
    "                # ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), %s, %s, %s,%s)\n",
    "                # \"\"\"\n",
    "                insert_query = \"\"\"\n",
    "                    INSERT INTO marts.dm_lab_request_orders (\n",
    "                    event_date, dedupe_id, lab_request_number, birthdate,  gender, shr_date, impilo_registration_date,\n",
    "                    date_sample_taken,lab_order_status, status_reason, note, sample_code, sample_type , test_type,\n",
    "                    facility_id_code, lab, test_results,\n",
    "                    dw_date_created, dm_date_created, person_id,\n",
    "                    encounter_id\n",
    "                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,  %s, %s, %s, %s, %s, NOW(),%s,%s)\n",
    "                \"\"\"\n",
    "                cur_mart.execute(insert_query, (\n",
    "                    row['event_date'], row['dedupe_id'], row['lab_request_number'], row['birthdate'], row['gender'], row['shr_date'],\n",
    "                    row['impilo_registration_date'], row['date_sample_taken'], row['lab_order_status'],\n",
    "                    row['status_reason'],  row['note'], row['sample_code'],  row['sample_type'],  row['test_type'],\n",
    "                    row['facility_id_code'], row['lab'], row['date_created'],  row['test_results'],\n",
    "                     row['person_id'], encounter_id\n",
    "                ))\n",
    "\n",
    "        # Update the last_processed_value to the latest timestamp in the batch\n",
    "        last_processed_value = df['date_created'].max()\n",
    "        print(\"max\",last_processed_value)\n",
    "        print(\"min\", df['date_created'].min())\n",
    "        batch_number += 1\n",
    "    return last_processed_value\n",
    "\n",
    "\n",
    "# Main function \n",
    "def main():\n",
    "    # Connection details..............................................\n",
    "    # Test server.....................................................\n",
    "    # host = \"57.151.95.136\"\n",
    "    # port = 10001\n",
    "    # username = \"tnhema\"\n",
    "    # password = \"*nhm@4865!\"\n",
    "    # database = \"default\"\n",
    "    # auth_mode = \"LDAP\"\n",
    "\n",
    "    # production server...............................................\n",
    "    host = \"197.221.242.150\"\n",
    "    port = 17251\n",
    "    username = \"tnhema\"\n",
    "    password = \"ZFCG9ZSGksEMpSpA\"\n",
    "    database = \"default\"\n",
    "    auth_mode = \"LDAP\"\n",
    "    \n",
    "    # Variable to control initial full load or change listening\n",
    "    start = \"on\"  # Set to \"on\" for initial load or \"off\" to only listen for changes\n",
    "    \n",
    "    # Retry mechanism variables\n",
    "    retry_attempts = 0\n",
    "    max_retries = 5000000  # Maximum number of retries before stopping \n",
    "    retry_delay = 10  # Delay between retries in seconds\n",
    "    last_processed_value = '1900-01-01 00:00:00'\n",
    "    \n",
    "    while max_retries is None or retry_attempts < max_retries:\n",
    "        try:\n",
    "            print(f\"Attempting to connect (Attempt {retry_attempts + 1})...\")\n",
    "            \n",
    "            # Create a connection to Hive\n",
    "            conn = create_hive_connection(host, port, username, password, database, auth_mode)\n",
    "            \n",
    "            if conn:\n",
    "                if start == \"on\":\n",
    "                    # Fetch all data from the patient table during initial load in batches\n",
    "                    print(\"Initial load: Fetching all data from the fact_hts_dedup table in batches of 50000.\")\n",
    "                    last_processed_value = get_all_patient_data_in_batches(conn, batch_size=50000)\n",
    "                \n",
    "                # After initial load, listen for changes (or if start is \"off\")\n",
    "                print(\"Listening for changes in the hts table...\")\n",
    "                listen_for_changes(conn, last_processed_value, polling_interval=300)\n",
    "            \n",
    "            # Close the connection after completing tasks\n",
    "            if conn:\n",
    "                close_hive_connection(conn)\n",
    "            \n",
    "            # Reset retry_attempts if everything succeeds\n",
    "            retry_attempts = 0\n",
    "            break  # Exit the loop if everything worked successfully\n",
    "\n",
    "        except Exception as e:\n",
    "            retry_attempts += 1\n",
    "            print(f\"An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
    "\n",
    "            # Wait before retrying\n",
    "            time.sleep(retry_delay)\n",
    "            \n",
    "            if max_retries is not None and retry_attempts >= max_retries:\n",
    "                print(f\"Max retries reached ({max_retries}). Exiting.\")\n",
    "                break\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 15:16:43,089 - INFO - Attempting to connect (Attempt 1)...\n",
      "2025-09-24 15:16:43,371 - INFO - USE `default`\n",
      "2025-09-24 15:16:43,544 - INFO - Hive connection established successfully\n",
      "2025-09-24 15:16:43,547 - INFO - Initial load: Fetching all data from the fact_lab_request_orders table in batches of 50000.\n",
      "2025-09-24 15:16:43,554 - INFO - No previous snapshot found\n",
      "2025-09-24 15:16:43,555 - INFO - >>> Initial snapshot\n",
      "2025-09-24 15:16:43,557 - INFO - ==================================================\n",
      "2025-09-24 15:16:43,560 - INFO - SELECT 1 as test_value\n",
      "2025-09-24 15:16:43,887 - INFO - ✓ Hive connection test successful\n",
      "2025-09-24 15:16:43,889 - INFO - ✓ Data mart connection test successful\n",
      "2025-09-24 15:16:43,891 - INFO - DEBUG: Executing count query: \n",
      "    SELECT \n",
      "        COUNT(*) AS total_records, \n",
      "        COUNT(DISTINCT encounter_facility_id) AS total_facilities, \n",
      "        COUNT(DISTINCT patient_id) AS distinct_people, \n",
      "        COUNT(DISTINCT encounter_id) AS distinct_encounters, \n",
      "        MAX(last_updated) AS last_update_time\n",
      "    FROM fact_lab_request_orders \n",
      "    WHERE last_updated > '1900-01-01 00:00:00' \n",
      "    AND encounter_facility_id = 'ZW090A17'\n",
      "    \n",
      "2025-09-24 15:16:43,893 - INFO - \n",
      "    SELECT \n",
      "        COUNT(*) AS total_records, \n",
      "        COUNT(DISTINCT encounter_facility_id) AS total_facilities, \n",
      "        COUNT(DISTINCT patient_id) AS distinct_people, \n",
      "        COUNT(DISTINCT encounter_id) AS distinct_encounters, \n",
      "        MAX(last_updated) AS last_update_time\n",
      "    FROM fact_lab_request_orders \n",
      "    WHERE last_updated > '1900-01-01 00:00:00' \n",
      "    AND encounter_facility_id = 'ZW090A17'\n",
      "    \n",
      "2025-09-24 15:17:14,789 - INFO - ✓ Count query successful\n",
      "2025-09-24 15:17:14,790 - INFO - Total Number of Records to be fetched: 9300\n",
      "2025-09-24 15:17:14,793 - INFO - Total Number of Facilities: 1\n",
      "2025-09-24 15:17:14,795 - INFO - Number of Distinct People: 4137\n",
      "2025-09-24 15:17:14,797 - INFO - Number of Distinct Encounters: 5039\n",
      "2025-09-24 15:17:14,799 - INFO - Last time update: 2025-09-24T12:55:00.085+00:00\n",
      "2025-09-24 15:17:14,802 - INFO - Total Number of Batches: 1\n",
      "2025-09-24 15:17:14,804 - INFO - ==================================================\n",
      "2025-09-24 15:17:14,807 - INFO - Working on Batch 1\n",
      "2025-09-24 15:17:14,809 - INFO - DEBUG: Executing batch query: \n",
      "        SELECT * FROM fact_lab_request_orders \n",
      "        WHERE last_updated > '1900-01-01 00:00:00' \n",
      "        AND encounter_facility_id = 'ZW090A17'\n",
      "        ORDER BY last_updated ASC \n",
      "        LIMIT 5000...\n",
      "2025-09-24 15:17:14,811 - INFO - \n",
      "        SELECT * FROM fact_lab_request_orders \n",
      "        WHERE last_updated > '1900-01-01 00:00:00' \n",
      "        AND encounter_facility_id = 'ZW090A17'\n",
      "        ORDER BY last_updated ASC \n",
      "        LIMIT 50000\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from pyhive import hive\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def safe_convert_value(value):\n",
    "    \"\"\"Convert pandas values to Python native types safely\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    elif isinstance(value, (pd.Series, pd.Index)):\n",
    "        # If it's a Series/Index, get the first value\n",
    "        return str(value.iloc[0]) if len(value) > 0 else None\n",
    "    elif isinstance(value, (pd.Timestamp, datetime)):\n",
    "        return value.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        return str(value)\n",
    "\n",
    "# Connection to HTS Data Mart\n",
    "conn_mart = psycopg2.connect(\n",
    "    dbname=\"LIMSDATA\", \n",
    "    user=\"postgres\", \n",
    "    password=\"wGMCAE6zFHcyrBmXtus97JPanxvkY4fb\", \n",
    "    host=\"127.0.0.1\",\n",
    "    port=5431\n",
    ")\n",
    "conn_mart.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "cur_mart = conn_mart.cursor()\n",
    "\n",
    "def create_hive_connection(host, port, username, password, database, auth_mode):\n",
    "    \"\"\"Creates a connection to the Hive database.\"\"\"\n",
    "    try:\n",
    "        conn = hive.Connection(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            username=username,\n",
    "            password=password,\n",
    "            database=database,\n",
    "            auth=auth_mode\n",
    "        )\n",
    "        logger.info(\"Hive connection established successfully\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating Hive connection: {e}\")\n",
    "        return None\n",
    "\n",
    "def execute_hive_query(conn, query):\n",
    "    \"\"\"Executes the given query and returns the results.\"\"\"\n",
    "    cursor = None\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        results = cursor.fetchall()\n",
    "        return results, columns\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error executing query: {e}\")\n",
    "        logger.error(f\"Query: {query}\")\n",
    "        return None, None\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "\n",
    "def get_new_patient_data(conn, last_timestamp):\n",
    "    \"\"\"Fetches newly inserted/updated patient data based on last timestamp.\"\"\"\n",
    "    # Fixed: Added facility filter and proper parameter handling\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM fact_lab_request_orders \n",
    "    WHERE last_updated > '{last_timestamp}' \n",
    "    AND encounter_facility_id = 'ZW090A17'\n",
    "    ORDER BY last_updated ASC\n",
    "    \"\"\"\n",
    "    return execute_hive_query(conn, query)\n",
    "\n",
    "def close_hive_connection(conn):\n",
    "    \"\"\"Closes the connection to the Hive database.\"\"\"\n",
    "    try:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            logger.info(\"Hive connection closed\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error closing connection: {e}\")\n",
    "\n",
    "def check_if_snapshot_done():\n",
    "    \"\"\"Check the last processed date from the data mart.\"\"\"\n",
    "    try:\n",
    "        cur_mart.execute(\"\"\"\n",
    "            SELECT dm_date_created \n",
    "            FROM marts.dm_lab_request_orders \n",
    "            ORDER BY dm_date_created DESC \n",
    "            LIMIT 1\n",
    "        \"\"\") \n",
    "        result = cur_mart.fetchone()\n",
    "        if result:\n",
    "            logger.info(f\"Found last snapshot date: {result[0]}\")\n",
    "            return result[0]\n",
    "        else:\n",
    "            logger.info(\"No previous snapshot found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking snapshot status: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_patient_data_in_batches(conn, batch_size=50000):\n",
    "    \"\"\"Process data in batches with proper error handling.\"\"\"\n",
    "    batch_number = 1\n",
    "    total_processed = 0\n",
    "    snapshot_date = check_if_snapshot_done()\n",
    "\n",
    "    if snapshot_date:\n",
    "        last_processed_value = snapshot_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        logger.info(f\">>> Detected Snapshot done: {last_processed_value}\")\n",
    "    else:\n",
    "        last_processed_value = '2025-01-01 00:00:00'\n",
    "        logger.info(\">>> Initial snapshot\")\n",
    "\n",
    "    logger.info(\"=\" * 50)\n",
    "    \n",
    "    # DEBUG: Test Hive connection first\n",
    "    test_query = \"SELECT 1 as test_value\"\n",
    "    test_result = execute_hive_query(conn, test_query)\n",
    "    if test_result[0]:\n",
    "        logger.info(\"✓ Hive connection test successful\")\n",
    "    else:\n",
    "        logger.error(\"✗ Hive connection test failed\")\n",
    "        return last_processed_value\n",
    "    \n",
    "    # DEBUG: Test data mart connection\n",
    "    try:\n",
    "        cur_mart.execute(\"SELECT 1 as test_value\")\n",
    "        test_mart = cur_mart.fetchone()\n",
    "        if test_mart:\n",
    "            logger.info(\"✓ Data mart connection test successful\")\n",
    "        else:\n",
    "            logger.error(\"✗ Data mart connection test failed\")\n",
    "            return last_processed_value\n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Data mart connection error: {e}\")\n",
    "        return last_processed_value\n",
    "\n",
    "    # Get total count with proper facility filter\n",
    "    count_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) AS total_records, \n",
    "        COUNT(DISTINCT encounter_facility_id) AS total_facilities, \n",
    "        COUNT(DISTINCT patient_id) AS distinct_people, \n",
    "        COUNT(DISTINCT encounter_id) AS distinct_encounters, \n",
    "        MAX(last_updated) AS last_update_time\n",
    "    FROM fact_lab_request_orders \n",
    "    WHERE last_updated > '{last_processed_value}' \n",
    "    AND encounter_facility_id = 'ZW090A17'\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"DEBUG: Executing count query: {count_query}\")\n",
    "    result_data = execute_hive_query(conn, count_query)\n",
    "    \n",
    "    if not result_data[0]:\n",
    "        logger.error(\"✗ Failed to get count data from Hive\")\n",
    "        return last_processed_value\n",
    "\n",
    "    result = result_data[0][0]\n",
    "    total_records = result[0]\n",
    "    \n",
    "    logger.info(f\"✓ Count query successful\")\n",
    "    logger.info(f\"Total Number of Records to be fetched: {total_records}\")\n",
    "    logger.info(f\"Total Number of Facilities: {result[1]}\")\n",
    "    logger.info(f\"Number of Distinct People: {result[2]}\")\n",
    "    logger.info(f\"Number of Distinct Encounters: {result[3]}\")\n",
    "    logger.info(f\"Last time update: {result[4]}\")\n",
    "    logger.info(f\"Total Number of Batches: {math.ceil(int(total_records) / batch_size)}\")\n",
    "\n",
    "    if total_records == 0:\n",
    "        logger.info(\"⚠ No new records to process - this might be why no data is entering\")\n",
    "        # DEBUG: Let's check what data exists\n",
    "        debug_query = f\"\"\"\n",
    "        SELECT COUNT(*) as total_in_table,\n",
    "               MIN(last_updated) as earliest_date,\n",
    "               MAX(last_updated) as latest_date\n",
    "        FROM fact_lab_request_orders \n",
    "        WHERE encounter_facility_id = 'ZW090A17'\n",
    "        \"\"\"\n",
    "        logger.info(f\"DEBUG: Checking overall data availability: {debug_query}\")\n",
    "        debug_result = execute_hive_query(conn, debug_query)\n",
    "        if debug_result[0]:\n",
    "            debug_data = debug_result[0][0]\n",
    "            logger.info(f\"DEBUG: Total records in table for facility: {debug_data[0]}\")\n",
    "            logger.info(f\"DEBUG: Earliest date: {debug_data[1]}\")\n",
    "            logger.info(f\"DEBUG: Latest date: {debug_data[2]}\")\n",
    "            logger.info(f\"DEBUG: Current filter date: {last_processed_value}\")\n",
    "        return last_processed_value\n",
    "\n",
    "    while True:\n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(f\"Working on Batch {batch_number}\")\n",
    "        \n",
    "        # Fixed: Added facility filter to batch query\n",
    "        batch_query = f\"\"\"\n",
    "        SELECT * FROM fact_lab_request_orders \n",
    "        WHERE last_updated > '{last_processed_value}' \n",
    "        AND encounter_facility_id = 'ZW090A17'\n",
    "        ORDER BY last_updated ASC \n",
    "        LIMIT {batch_size}\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"DEBUG: Executing batch query: {batch_query[:200]}...\")\n",
    "        batch_data, columns = execute_hive_query(conn, batch_query)\n",
    "        \n",
    "        if not batch_data or len(batch_data) == 0:\n",
    "            logger.info(f\"All data fetched. Total batches processed: {batch_number - 1}\")\n",
    "            logger.info(f\"Total records processed: {total_processed}\")\n",
    "            break\n",
    "        \n",
    "        logger.info(f\"✓ Batch {batch_number}: Fetched {len(batch_data)} rows from Hive\")\n",
    "        logger.info(f\"DEBUG: First row sample: {batch_data[0][:5] if batch_data else 'No data'}\")\n",
    "        logger.info(f\"DEBUG: Columns: {columns[:10] if columns else 'No columns'}\")\n",
    "        \n",
    "        # Process the batch\n",
    "        try:\n",
    "            df = pd.DataFrame(batch_data, columns=columns)\n",
    "            logger.info(f\"✓ DataFrame created - Shape: {df.shape}\")\n",
    "            \n",
    "            # DEBUG: Check if required columns exist\n",
    "            required_cols = ['task_id', 'patient_id', 'last_updated']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                logger.error(f\"✗ Missing required columns: {missing_cols}\")\n",
    "                logger.info(f\"Available columns: {list(df.columns)}\")\n",
    "                break\n",
    "\n",
    "            # Data cleaning and transformation\n",
    "            df['test_type'] = df['test_type'].str.replace(')', '', regex=False)\n",
    "            \n",
    "            # DEBUG: Check columns before renaming\n",
    "            logger.info(f\"DEBUG: Columns before renaming: {list(df.columns)}\")\n",
    "            \n",
    "            # Fixed: Proper column renaming\n",
    "            rename_mapping = {\n",
    "                'patient_id': 'person_id', \n",
    "                'encounter_facility_id': 'facility_id_code',\n",
    "                'birth_date': 'birthdate',\n",
    "                'last_updated': 'date_created',\n",
    "                'result': 'test_results',\n",
    "                'task_authored_on': 'shr_date',\n",
    "                'task_execution_start_date': 'impilo_registration_date',\n",
    "                'task_status': 'lab_order_status',\n",
    "                'task_id': 'encounter_id'\n",
    "            }\n",
    "            \n",
    "            # Only rename columns that exist\n",
    "            existing_renames = {k: v for k, v in rename_mapping.items() if k in df.columns}\n",
    "            missing_renames = {k: v for k, v in rename_mapping.items() if k not in df.columns}\n",
    "            \n",
    "            if missing_renames:\n",
    "                logger.warning(f\"⚠ Missing columns for renaming: {missing_renames}\")\n",
    "            \n",
    "            df.rename(columns=existing_renames, inplace=True)\n",
    "            logger.info(f\"✓ Renamed columns: {existing_renames}\")\n",
    "            \n",
    "            # Check if encounter_id exists after renaming\n",
    "            if 'encounter_id' not in df.columns:\n",
    "                logger.error(\"✗ CRITICAL: encounter_id column missing after renaming!\")\n",
    "                logger.info(f\"Available columns after renaming: {list(df.columns)}\")\n",
    "                break\n",
    "            \n",
    "            df['event_date'] = df.get('shr_date')\n",
    "            logger.info(f\"✓ After transformation shape: {df.shape}\")\n",
    "            \n",
    "            # DEBUG: Sample the data\n",
    "            logger.info(f\"DEBUG: Sample encounter_id values: {df['encounter_id'].head(3).tolist()}\")\n",
    "            \n",
    "            # Process each row with proper error handling\n",
    "            successful_inserts = 0\n",
    "            successful_updates = 0\n",
    "            errors = 0\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                try:\n",
    "                    # Fix: Convert Series to individual values properly\n",
    "                    encounter_id = str(row['encounter_id']) if pd.notna(row['encounter_id']) else None\n",
    "                    \n",
    "                    if not encounter_id or encounter_id == 'nan' or encounter_id == '':\n",
    "                        logger.warning(f\"⚠ Skipping row {idx} - empty encounter_id\")\n",
    "                        continue\n",
    "                    \n",
    "                    # DEBUG: Log first few operations\n",
    "                    if idx < 3:\n",
    "                        logger.info(f\"DEBUG: Processing row {idx} with encounter_id: {encounter_id}\")\n",
    "                        logger.info(f\"DEBUG: Row type: {type(row)}, encounter_id type: {type(encounter_id)}\")\n",
    "                    \n",
    "                    # Check if record exists\n",
    "                    cur_mart.execute(\"\"\"\n",
    "                        SELECT 1 FROM marts.dm_lab_request_orders \n",
    "                        WHERE encounter_id = %s\n",
    "                    \"\"\", (encounter_id,))\n",
    "                    \n",
    "                    existing_record = cur_mart.fetchone()\n",
    "                    \n",
    "                    if existing_record:\n",
    "                        # Update existing record\n",
    "                        if idx < 3:  # Debug first few\n",
    "                            logger.info(f\"DEBUG: Updating existing record for encounter_id: {encounter_id}\")\n",
    "                        \n",
    "                        update_query = \"\"\"\n",
    "                        UPDATE marts.dm_lab_request_orders\n",
    "                        SET event_date = %s, dedupe_id = %s, lab_request_number = %s, \n",
    "                            birthdate = %s, gender = %s, shr_date = %s, \n",
    "                            impilo_registration_date = %s, date_sample_taken = %s, \n",
    "                            lab_order_status = %s, status_reason = %s, note = %s, \n",
    "                            sample_code = %s, sample_type = %s, test_type = %s, \n",
    "                            facility_id_code = %s, lab = %s, test_results = %s,\n",
    "                            dw_date_created = %s, dm_date_created = NOW(), person_id = %s\n",
    "                        WHERE encounter_id = %s\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        # Fix: Use helper function for safe conversion\n",
    "                        update_params = (\n",
    "                            safe_convert_value(row.get('event_date')),\n",
    "                            safe_convert_value(row.get('dedupe_id')),\n",
    "                            safe_convert_value(row.get('lab_request_number')),\n",
    "                            safe_convert_value(row.get('birthdate')),\n",
    "                            safe_convert_value(row.get('gender')),\n",
    "                            safe_convert_value(row.get('shr_date')),\n",
    "                            safe_convert_value(row.get('impilo_registration_date')),\n",
    "                            safe_convert_value(row.get('date_sample_taken')),\n",
    "                            safe_convert_value(row.get('lab_order_status')),\n",
    "                            safe_convert_value(row.get('status_reason')),\n",
    "                            safe_convert_value(row.get('note')),\n",
    "                            safe_convert_value(row.get('sample_code')),\n",
    "                            safe_convert_value(row.get('sample_type')),\n",
    "                            safe_convert_value(row.get('test_type')),\n",
    "                            safe_convert_value(row.get('facility_id_code')),\n",
    "                            safe_convert_value(row.get('lab')),\n",
    "                            safe_convert_value(row.get('test_results')),\n",
    "                            safe_convert_value(row.get('date_created')),\n",
    "                            safe_convert_value(row.get('person_id')),\n",
    "                            encounter_id\n",
    "                        )\n",
    "                        \n",
    "                        cur_mart.execute(update_query, update_params)\n",
    "                        successful_updates += 1\n",
    "                        \n",
    "                        if idx < 3:\n",
    "                            logger.info(f\"✓ Successfully updated encounter_id: {encounter_id}\")\n",
    "                            \n",
    "                    else:\n",
    "                        # Insert new record\n",
    "                        if idx < 3:  # Debug first few\n",
    "                            logger.info(f\"DEBUG: Inserting new record for encounter_id: {encounter_id}\")\n",
    "                        \n",
    "                        insert_query = \"\"\"\n",
    "                        INSERT INTO marts.dm_lab_request_orders (\n",
    "                            event_date, dedupe_id, lab_request_number, birthdate, gender, \n",
    "                            shr_date, impilo_registration_date, date_sample_taken,\n",
    "                            lab_order_status, status_reason, note, sample_code, \n",
    "                            sample_type, test_type, facility_id_code, lab, test_results,\n",
    "                            dw_date_created, dm_date_created, person_id, encounter_id\n",
    "                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, \n",
    "                                 %s, %s, %s, %s, %s, %s, NOW(), %s, %s)\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        # Fix: Use helper function for safe conversion\n",
    "                        insert_params = (\n",
    "                            safe_convert_value(row.get('event_date')),\n",
    "                            safe_convert_value(row.get('dedupe_id')),\n",
    "                            safe_convert_value(row.get('lab_request_number')),\n",
    "                            safe_convert_value(row.get('birthdate')),\n",
    "                            safe_convert_value(row.get('gender')),\n",
    "                            safe_convert_value(row.get('shr_date')),\n",
    "                            safe_convert_value(row.get('impilo_registration_date')),\n",
    "                            safe_convert_value(row.get('date_sample_taken')),\n",
    "                            safe_convert_value(row.get('lab_order_status')),\n",
    "                            safe_convert_value(row.get('status_reason')),\n",
    "                            safe_convert_value(row.get('note')),\n",
    "                            safe_convert_value(row.get('sample_code')),\n",
    "                            safe_convert_value(row.get('sample_type')),\n",
    "                            safe_convert_value(row.get('test_type')),\n",
    "                            safe_convert_value(row.get('facility_id_code')),\n",
    "                            safe_convert_value(row.get('lab')),\n",
    "                            safe_convert_value(row.get('test_results')),\n",
    "                            safe_convert_value(row.get('date_created')),\n",
    "                            safe_convert_value(row.get('person_id')),\n",
    "                            encounter_id\n",
    "                        )\n",
    "                        \n",
    "                        cur_mart.execute(insert_query, insert_params)\n",
    "                        successful_inserts += 1\n",
    "                        \n",
    "                        if idx < 3:\n",
    "                            logger.info(f\"✓ Successfully inserted encounter_id: {encounter_id}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    errors += 1\n",
    "                    logger.error(f\"✗ Error processing row {idx}: {e}\")\n",
    "                    if idx < 5:  # Show detailed error for first few rows\n",
    "                        logger.error(f\"Encounter ID type: {type(row.get('encounter_id'))}\")\n",
    "                        logger.error(f\"Row keys: {list(row.keys())}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"✓ Batch {batch_number} completed:\")\n",
    "            logger.info(f\"  - Successful inserts: {successful_inserts}\")\n",
    "            logger.info(f\"  - Successful updates: {successful_updates}\")\n",
    "            logger.info(f\"  - Errors: {errors}\")\n",
    "            logger.info(f\"  - Total processed in this batch: {successful_inserts + successful_updates}\")\n",
    "            \n",
    "            total_processed += successful_inserts + successful_updates\n",
    "            \n",
    "            # Verify data was actually inserted\n",
    "            if successful_inserts > 0 or successful_updates > 0:\n",
    "                try:\n",
    "                    cur_mart.execute(\"\"\"\n",
    "                        SELECT COUNT(*) FROM marts.dm_lab_request_orders \n",
    "                        WHERE dm_date_created::date = CURRENT_DATE\n",
    "                    \"\"\")\n",
    "                    today_count = cur_mart.fetchone()\n",
    "                    logger.info(f\"DEBUG: Total records in mart created today: {today_count[0] if today_count else 'Unknown'}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not verify today's inserts: {e}\")\n",
    "            \n",
    "            # Update the last_processed_value to the latest timestamp in the batch\n",
    "            if 'date_created' in df.columns and not df['date_created'].isna().all():\n",
    "                new_last_value = df['date_created'].max()\n",
    "                logger.info(f\"✓ Updated last_processed_value from {last_processed_value} to {new_last_value}\")\n",
    "                last_processed_value = new_last_value\n",
    "            else:\n",
    "                logger.warning(\"⚠ Could not update last_processed_value - date_created column issues\")\n",
    "            \n",
    "            batch_number += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Error processing batch {batch_number}: {e}\")\n",
    "            logger.error(f\"Exception details: {type(e).__name__}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    return last_processed_value\n",
    "\n",
    "def listen_for_changes(conn, last_timestamp, polling_interval=300):\n",
    "    \"\"\"Polls the Hive database for changes at a regular interval.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            logger.info(f\"Polling for changes after {last_timestamp}...\")\n",
    "            new_data_result = get_new_patient_data(conn, last_timestamp)\n",
    "            \n",
    "            if new_data_result[0] and len(new_data_result[0]) > 0:\n",
    "                logger.info(f\"New/Updated records found: {len(new_data_result[0])}\")\n",
    "                # Process the new data using the same batch processing logic\n",
    "                # For simplicity, we'll just update the timestamp here\n",
    "                # In practice, you'd want to process this data similarly\n",
    "                last_timestamp = max([row[-1] for row in new_data_result[0]])\n",
    "            else:\n",
    "                logger.info(\"No new records found.\")\n",
    "            \n",
    "            time.sleep(polling_interval)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during polling: {e}\")\n",
    "            time.sleep(polling_interval)  # Continue polling even after errors\n",
    "\n",
    "def main():\n",
    "    # Connection details\n",
    "    host = \"197.221.242.150\"\n",
    "    port = 17251\n",
    "    username = \"tnhema\"\n",
    "    password = \"ZFCG9ZSGksEMpSpA\"\n",
    "    database = \"default\"\n",
    "    auth_mode = \"LDAP\"\n",
    "    \n",
    "    start = \"on\"\n",
    "    retry_attempts = 0\n",
    "    max_retries = 5\n",
    "    retry_delay = 10\n",
    "    last_processed_value = '2025-01-01 00:00:00'\n",
    "    \n",
    "    while retry_attempts < max_retries:\n",
    "        conn = None\n",
    "        try:\n",
    "            logger.info(f\"Attempting to connect (Attempt {retry_attempts + 1})...\")\n",
    "            \n",
    "            conn = create_hive_connection(host, port, username, password, database, auth_mode)\n",
    "            \n",
    "            if conn:\n",
    "                if start == \"on\":\n",
    "                    logger.info(\"Initial load: Fetching all data from the fact_lab_request_orders table in batches of 50000.\")\n",
    "                    last_processed_value = get_all_patient_data_in_batches(conn, batch_size=50000)\n",
    "                \n",
    "                logger.info(\"Starting polling for changes...\")\n",
    "                listen_for_changes(conn, last_processed_value, polling_interval=300)\n",
    "            else:\n",
    "                raise Exception(\"Failed to establish Hive connection\")\n",
    "\n",
    "        except Exception as e:\n",
    "            retry_attempts += 1\n",
    "            logger.error(f\"An error occurred: {e}. Retry {retry_attempts}/{max_retries} in {retry_delay} seconds...\")\n",
    "            \n",
    "            if conn:\n",
    "                close_hive_connection(conn)\n",
    "            \n",
    "            if retry_attempts >= max_retries:\n",
    "                logger.error(f\"Max retries reached ({max_retries}). Exiting.\")\n",
    "                break\n",
    "                \n",
    "            time.sleep(retry_delay)\n",
    "        \n",
    "        finally:\n",
    "            if conn:\n",
    "                close_hive_connection(conn)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehr_lab_env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
